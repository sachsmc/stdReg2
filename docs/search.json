[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Affero General Public License","title":"GNU Affero General Public License","text":"Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU Affero General Public License","text":"GNU Affero General Public License free, copyleft license software kinds works, specifically designed ensure cooperation community case network server software. licenses software practical works designed take away freedom share change works. contrast, General Public Licenses intended guarantee freedom share change versions program–make sure remains free software users. speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. Developers use General Public Licenses protect rights two steps: (1) assert copyright software, (2) offer License gives legal permission copy, distribute /modify software. secondary benefit defending users’ freedom improvements made alternate versions program, receive widespread use, become available developers incorporate. Many developers free software heartened encouraged resulting cooperation. However, case software used network servers, result may fail come . GNU General Public License permits making modified version letting public access server without ever releasing source code public. GNU Affero General Public License designed specifically ensure , cases, modified source code becomes available community. requires operator network server provide source code modified version running users server. Therefore, public use modified version, publicly accessible server, gives public access source code modified version. older license, called Affero General Public License published Affero, designed accomplish similar goals. different license, version Affero GPL, Affero released new version Affero GPL permits relicensing license. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions.","title":"GNU Affero General Public License","text":"“License” refers version 3 GNU Affero General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code.","title":"GNU Affero General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions.","title":"GNU Affero General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law.","title":"GNU Affero General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies.","title":"GNU Affero General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions.","title":"GNU Affero General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: work must carry prominent notices stating modified , giving relevant date. work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms.","title":"GNU Affero General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms.","title":"GNU Affero General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: Disclaiming warranty limiting liability differently terms sections 15 16 License; Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; Limiting use publicity purposes names licensors authors material; Declining grant rights trademark law use trade names, trademarks, service marks; Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination.","title":"GNU Affero General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies.","title":"GNU Affero General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients.","title":"GNU Affero General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents.","title":"GNU Affero General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom.","title":"GNU Affero General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"/LICENSE.html","id":"id_13-remote-network-interaction-use-with-the-gnu-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Remote Network Interaction; Use with the GNU General Public License.","title":"GNU Affero General Public License","text":"Notwithstanding provision License, modify Program, modified version must prominently offer users interacting remotely computer network (version supports interaction) opportunity receive Corresponding Source version providing access Corresponding Source network server charge, standard customary means facilitating copying software. Corresponding Source shall include Corresponding Source work covered version 3 GNU General Public License incorporated pursuant following paragraph. Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU General Public License single combined work, convey resulting work. terms License continue apply part covered work, work combined remain governed version 3 GNU General Public License.","code":""},{"path":"/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License.","title":"GNU Affero General Public License","text":"Free Software Foundation may publish revised /new versions GNU Affero General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU Affero General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU Affero General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU Affero General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty.","title":"GNU Affero General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability.","title":"GNU Affero General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16.","title":"GNU Affero General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU Affero General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. software can interact users remotely computer network, also make sure provides way users get source. example, program web application, interface display “Source” link leads users archive code. many ways offer source, different solutions better different programs; see section 13 specific requirements. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU AGPL, see https://www.gnu.org/licenses/.","code":"<one line to give the program's name and a brief idea of what it does.>     Copyright (C) <year>  <name of author>      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU Affero General Public License as     published by the Free Software Foundation, either version 3 of the     License, or (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU Affero General Public License for more details.      You should have received a copy of the GNU Affero General Public License     along with this program.  If not, see <https://www.gnu.org/licenses/>."},{"path":"/articles/overview.html","id":"introduction-and-context","dir":"Articles","previous_headings":"","what":"Introduction and context","title":"Estimation of causal effects using stdReg2","text":"Suppose \\(X\\) denotes exposure interest takes values 0 1. represent two different medical treatments, environmental exposures, economic policies, genetic variants. often use biomedical examples biostatisticians. consider setting interest quantify effect intervening \\(X\\) outcome denote \\(Y\\). outcome represent numeric value, presence absence condition, time two events, time cancer diagnosis death. Let \\(Y(X = x)\\) denote potential outcome subjects population hypothetically exposed \\(X = x\\). quantify effect \\(X\\), must summarize distribution \\(Y(X = x)\\) statistic. \\(Y\\) dichotomous, natural use \\(p\\{Y(X = x) = 1\\}\\), called risk. \\(Y\\) continuous, mean natural summary statistic \\(E\\{Y(X = x)\\}\\). \\(Y\\) continuous time--event, probability exceeding particular value \\(t\\) reasonable statistic: \\(p\\{Y(X = x) > t\\}\\). general, denote summary statistic choice \\(T\\{Y(X = x)\\}\\). summary statistic can also applied conditional distributions, denote, e.g., \\(T\\{Y | X = x\\}\\). quantify effect \\(X\\), must also decide contrast measure causal effect. point contrast compare chosen summary statistic \\(X = 1\\) \\(X = 1\\). Typical choices difference \\(T\\{Y(X = 1)\\} - T\\{Y(X = 0)\\}\\) ratio \\(T\\{Y(X = 1)\\} / T\\{Y(X = 0)\\}\\). may also interest quantify report summary statistics within group \\((T\\{Y(X = 1)\\}, T\\{Y(X = 0)\\})\\). observational studies, relationship \\(X\\) \\(Y\\) likely confounded set variables \\(\\boldsymbol{Z}\\). means values outcome \\(Y\\) determined least subset \\(\\boldsymbol{Z}\\) values exposure \\(X\\) determined subset \\(\\boldsymbol{Z}\\). Naively estimating contrast lead biased estimates causal effect.","code":""},{"path":"/articles/overview.html","id":"regression-standardization","dir":"Articles","previous_headings":"","what":"Regression standardization","title":"Estimation of causal effects using stdReg2","text":"See Sjölander (2016) Sjölander (2018) details. Suppose covariates \\(\\boldsymbol{Z}\\) sufficient confounding control. information constitutes sufficient adjustment set, see Witte Didelez (2019). given summary statistic \\(T\\), \\[ T\\{Y(X = x)\\} = E_{\\boldsymbol{Z}}[T\\{Y | X = x, \\boldsymbol{Z}\\}], \\] expectation taken respect population distribution \\(\\boldsymbol{Z}\\). also known g-formula adjustment formula. order estimate quantity based independent identically distributed sample \\((X_1, Y_1, \\boldsymbol{Z}_1), \\ldots, (X_n, Y_n, \\boldsymbol{Z}_n)\\), proceed Specifying estimating regression model \\(Y\\) given \\(X\\) \\(\\boldsymbol{Z}\\). Use fitted model obtain estimates \\(T\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\}\\) \\(= 1, \\ldots, n\\). done creating copy observed dataset, replacing observed \\(X_i\\) \\(x\\) individual, using fitted model get predicted values copy observed data. Denote predicted values \\(\\hat{T}\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\}\\). Average empirical distribution \\(\\boldsymbol{Z}\\) obtain estimate \\[ \\hat{T}\\{Y(X = x)\\} = \\frac{1}{n}\\sum_{= 1}^n \\hat{T}\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\}. \\] One can level \\(X = 0, 1\\) compute desired contrast. assumptions 1) \\(\\boldsymbol{Z}\\) sufficient confounding control, 2) regression model step 1 correctly specified, estimator consistent asymptotically normal.","code":""},{"path":"/articles/overview.html","id":"improving-robustness-by-modeling-the-exposure","dir":"Articles","previous_headings":"","what":"Improving robustness by modeling the exposure","title":"Estimation of causal effects using stdReg2","text":"doubly-robust estimator estimator consistent given estimand one models used forming estimator correctly specified confounding. far, one model used estimator: outcome model. now introduce model exposure, see can combine . First, terminology: Misspecified model - true generating mechanism contained possible mechanisms possible selected model. Correctly Specified - model correctly specified misspecified. Correctly specified confounding - correctly specified model contains sufficient set confounders. can model \\(P(X=1|\\boldsymbol{Z})\\), correctly specified contains confounders, can use estimate probability individual \\(\\) received treatment \\(W_i = \\frac{X_i}{P(X_i=1|\\boldsymbol{Z}_i)} + \\frac{1-X_i}{1-P(X_i=1|\\boldsymbol{Z}_i)}\\). Let \\(\\hat{p}_i\\) denote estimated probability subject \\(\\) received treatment \\(1\\). Doubly robust estimator ATE Augmented inverse probability weighted AIPW estimator: \\[ \\frac{1}{n}\\sum_{= 1}^n\\frac{Y_i X_i}{\\hat{p}_i} - \\frac{\\hat{T}\\{Y_i | X_i = 1, \\boldsymbol{Z}_i\\}(X_i - \\hat{p}_i)}{\\hat{p}_i} - \\frac{Y_i (1 - X_i)}{1 - \\hat{p}_i} - \\frac{\\hat{T}\\{Y_i | X_i = 0, \\boldsymbol{Z}_i\\}(X_i - \\hat{p}_i)}{1 - \\hat{p}_i}.   \\] consistent estimation method can used outcome exposure models. long either outcome model propensity score model correctly specified confounding, doubly robust estimator consistent ATE. Recently, seems general misconception combining adjusted outcome model propensity score model always gives doubly robust estimator. true – matters combine !","code":""},{"path":"/articles/overview.html","id":"in-practice","dir":"Articles","previous_headings":"","what":"In practice","title":"Estimation of causal effects using stdReg2","text":"use regression standardization estimate average causal effect exposure (quitting smoking) variable qsmk weight gain outcome variable wt82\\_71 dhefs_dat dataset comes stdReg2 package. data collected part project National Center Health Statistics National Institute Aging collaboration agencies United States Public Health Service. designed investigate relationships clinical, nutritional, behavioral factors subsequent morbidity, mortality, hospital utilization changes risk factors, functional limitation, institutionalization. dataset includes 1629 individuals contains among others following variables: seqn: person id} wt82_71: weight gain kilograms 1971 1982 qsmk: quit smoking 1st questionnaire 1982, 1 = yes, 0 = sex: 0 = male, 1 = female race: 0 = white, 1 = black age: age years baseline education: level education 1971, 1 = 8th grade less, 2 = high school dropout, 3 = high school, 4 - dropout, 5 = college smokeintensity: number cigarettes smoked per day 1971 smokeyrs: number years smoked exercise: level physical activity 1971, 0 = much exercise, 1 = moderate exercise, 2 = little exercise active: level activity 1971, 0 = active, 1 = moderately active, 2 = inactive, 3 = missing wt71: weight kilograms 1971 ht: height centimeters 1971 assume set confounders \\(\\boldsymbol{Z}\\) includes sex, race, age, education, number cigarettes smoked per year, number years smoked, level physical activity, baseline weight 1971. equivalent assuming counterfactual weight gain independent exposure conditional variables. words, assuming following directed acyclic graph:  specific forms conditional expectations required outcome assume linear regression model linear quadratic forms continuous covariates. can fit peform regression standardization estimate causal effect use . must specify outcome regression model formula, provide data, describe values exposure wish estimate counterfactual means, specify contrasts want, specify reference level contrasts. following command estimates model obtain group-wise estimates, difference, ratio.   output model shows estimated potential outcome means exposure level, difference ratio thereof, associated standard error estimates, confidence intervals, p-values. Inference done using sandwich method variance calculation. assumptions outcome model correctly specified contains confounders, consistent estimates causal effects interest. can also get plots effects using plot function. obtain doubly robust inference use following command. Note now specify model exposure, propensity score model.","code":"summary(nhefs_dat) #>       seqn            qsmk            death            yrdth       #>  Min.   :  233   Min.   :0.0000   Min.   :0.0000   Min.   :83.00   #>  1st Qu.:10625   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:85.00   #>  Median :20702   Median :0.0000   Median :0.0000   Median :88.00   #>  Mean   :16639   Mean   :0.2573   Mean   :0.1858   Mean   :87.64   #>  3rd Qu.:22771   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:90.00   #>  Max.   :25061   Max.   :1.0000   Max.   :1.0000   Max.   :92.00   #>                                                    NA's   :1275    #>      modth            dadth           sbp             dbp         #>  Min.   : 1.000   Min.   : 1.0   Min.   : 87.0   Min.   : 47.00   #>  1st Qu.: 3.000   1st Qu.: 8.0   1st Qu.:115.0   1st Qu.: 70.00   #>  Median : 6.000   Median :16.0   Median :126.0   Median : 77.00   #>  Mean   : 6.383   Mean   :16.1   Mean   :128.6   Mean   : 77.74   #>  3rd Qu.:10.000   3rd Qu.:24.5   3rd Qu.:139.0   3rd Qu.: 85.00   #>  Max.   :12.000   Max.   :31.0   Max.   :229.0   Max.   :130.00   #>  NA's   :1271     NA's   :1271   NA's   :29      NA's   :33       #>       sex              age             race            income      #>  Min.   :0.0000   Min.   :25.00   Min.   :0.0000   Min.   :11.00   #>  1st Qu.:0.0000   1st Qu.:33.00   1st Qu.:0.0000   1st Qu.:17.00   #>  Median :1.0000   Median :43.00   Median :0.0000   Median :19.00   #>  Mean   :0.5134   Mean   :43.66   Mean   :0.1315   Mean   :17.99   #>  3rd Qu.:1.0000   3rd Qu.:53.00   3rd Qu.:0.0000   3rd Qu.:20.00   #>  Max.   :1.0000   Max.   :74.00   Max.   :1.0000   Max.   :22.00   #>                                                    NA's   :59      #>     marital          school        education           ht        #>  Min.   :2.000   Min.   : 0.00   Min.   :1.000   Min.   :142.9   #>  1st Qu.:2.000   1st Qu.:10.00   1st Qu.:2.000   1st Qu.:161.8   #>  Median :2.000   Median :12.00   Median :3.000   Median :168.2   #>  Mean   :2.496   Mean   :11.17   Mean   :2.715   Mean   :168.7   #>  3rd Qu.:2.000   3rd Qu.:12.00   3rd Qu.:3.000   3rd Qu.:175.3   #>  Max.   :8.000   Max.   :17.00   Max.   :5.000   Max.   :198.1   #>                                                                  #>       wt71             wt82           wt82_71          birthplace    #>  Min.   : 39.58   Min.   : 35.38   Min.   :-41.280   Min.   : 1.00   #>  1st Qu.: 59.53   1st Qu.: 61.69   1st Qu.: -1.478   1st Qu.:22.00   #>  Median : 69.23   Median : 72.12   Median :  2.604   Median :34.00   #>  Mean   : 70.83   Mean   : 73.47   Mean   :  2.638   Mean   :31.67   #>  3rd Qu.: 79.80   3rd Qu.: 83.46   3rd Qu.:  6.690   3rd Qu.:42.00   #>  Max.   :151.73   Max.   :136.53   Max.   : 48.538   Max.   :56.00   #>                                                      NA's   :90      #>  smokeintensity  smkintensity82_71    smokeyrs         asthma        #>  Min.   : 1.00   Min.   :-80.000   Min.   : 1.00   Min.   :0.00000   #>  1st Qu.:10.00   1st Qu.:-10.000   1st Qu.:15.00   1st Qu.:0.00000   #>  Median :20.00   Median : -1.000   Median :24.00   Median :0.00000   #>  Mean   :20.53   Mean   : -4.633   Mean   :24.59   Mean   :0.04853   #>  3rd Qu.:30.00   3rd Qu.:  1.000   3rd Qu.:33.00   3rd Qu.:0.00000   #>  Max.   :80.00   Max.   : 50.000   Max.   :64.00   Max.   :1.00000   #>                                                                      #>      bronch              tb                hf                hbp        #>  Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   :0.000   #>  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.000   #>  Median :0.00000   Median :0.00000   Median :0.000000   Median :1.000   #>  Mean   :0.08365   Mean   :0.01341   Mean   :0.005109   Mean   :1.059   #>  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:2.000   #>  Max.   :1.00000   Max.   :1.00000   Max.   :1.000000   Max.   :2.000   #>                                                                         #>   pepticulcer        colitis          hepatitis        chroniccough     #>  Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   #>  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   #>  Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000   #>  Mean   :0.1015   Mean   :0.03448   Mean   :0.01788   Mean   :0.05109   #>  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   #>  Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   #>                                                                         #>     hayfever          diabetes          polio             tumor         #>  Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   #>  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   #>  Median :0.00000   Median :0.0000   Median :0.00000   Median :0.00000   #>  Mean   :0.08621   Mean   :0.9898   Mean   :0.01405   Mean   :0.02363   #>  3rd Qu.:0.00000   3rd Qu.:2.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   #>  Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   Max.   :1.00000   #>                                                                         #>   nervousbreak       alcoholpy       alcoholfreq     alcoholtype    #>  Min.   :0.00000   Min.   :0.0000   Min.   :0.000   Min.   :1.000   #>  1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:1.000   #>  Median :0.00000   Median :1.0000   Median :2.000   Median :3.000   #>  Mean   :0.02746   Mean   :0.8787   Mean   :1.913   Mean   :2.466   #>  3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:4.000   #>  Max.   :1.00000   Max.   :2.0000   Max.   :5.000   Max.   :4.000   #>                                                                     #>  alcoholhowmuch        pica          headache        otherpain      #>  Min.   : 1.000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000   #>  1st Qu.: 2.000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000   #>  Median : 2.000   Median :0.000   Median :1.0000   Median :0.0000   #>  Mean   : 3.293   Mean   :0.986   Mean   :0.6328   Mean   :0.2433   #>  3rd Qu.: 4.000   3rd Qu.:2.000   3rd Qu.:1.0000   3rd Qu.:0.0000   #>  Max.   :48.000   Max.   :2.000   Max.   :1.0000   Max.   :1.0000   #>  NA's   :397                                                        #>    weakheart         allergies           nerves         lackpep        #>  Min.   :0.00000   Min.   :0.00000   Min.   :0.000   Min.   :0.00000   #>  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000   #>  Median :0.00000   Median :0.00000   Median :0.000   Median :0.00000   #>  Mean   :0.02235   Mean   :0.06322   Mean   :0.145   Mean   :0.05045   #>  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000   3rd Qu.:0.00000   #>  Max.   :1.00000   Max.   :1.00000   Max.   :1.000   Max.   :1.00000   #>                                                                        #>      hbpmed       boweltrouble       wtloss          infection     #>  Min.   :0.000   Min.   :0.000   Min.   :0.00000   Min.   :0.000   #>  1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.000   #>  Median :1.000   Median :1.000   Median :0.00000   Median :0.000   #>  Mean   :1.015   Mean   :1.046   Mean   :0.02618   Mean   :0.145   #>  3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.000   #>  Max.   :2.000   Max.   :2.000   Max.   :1.00000   Max.   :1.000   #>                                                                    #>      active          exercise      birthcontrol    pregnancies     #>  Min.   :0.0000   Min.   :0.000   Min.   :0.000   Min.   : 1.000   #>  1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.000   1st Qu.: 2.000   #>  Median :1.0000   Median :1.000   Median :1.000   Median : 3.000   #>  Mean   :0.6469   Mean   :1.195   Mean   :1.081   Mean   : 3.694   #>  3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 5.000   #>  Max.   :2.0000   Max.   :2.000   Max.   :2.000   Max.   :15.000   #>                                                   NA's   :866      #>   cholesterol      hightax82         price71         price82      #>  Min.   : 78.0   Min.   :0.0000   Min.   :1.507   Min.   :1.452   #>  1st Qu.:189.0   1st Qu.:0.0000   1st Qu.:2.037   1st Qu.:1.740   #>  Median :216.0   Median :0.0000   Median :2.168   Median :1.815   #>  Mean   :219.9   Mean   :0.1653   Mean   :2.138   Mean   :1.806   #>  3rd Qu.:245.0   3rd Qu.:0.0000   3rd Qu.:2.242   3rd Qu.:1.868   #>  Max.   :416.0   Max.   :1.0000   Max.   :2.693   Max.   :2.103   #>  NA's   :16      NA's   :90       NA's   :90      NA's   :90      #>      tax71            tax82          price71_82         tax71_82      #>  Min.   :0.5249   Min.   :0.2200   Min.   :-0.2027   Min.   :0.0360   #>  1st Qu.:0.9449   1st Qu.:0.4399   1st Qu.: 0.2010   1st Qu.:0.4610   #>  Median :1.0498   Median :0.5060   Median : 0.3360   Median :0.5440   #>  Mean   :1.0580   Mean   :0.5058   Mean   : 0.3324   Mean   :0.5522   #>  3rd Qu.:1.1548   3rd Qu.:0.5719   3rd Qu.: 0.4438   3rd Qu.:0.6220   #>  Max.   :1.5225   Max.   :0.7479   Max.   : 0.6121   Max.   :0.8844   #>  NA's   :90       NA's   :90       NA's   :90        NA's   :90       #>        w                sw         #>  Min.   : 1.054   Min.   :0.3312   #>  1st Qu.: 1.230   1st Qu.:0.8665   #>  Median : 1.373   Median :0.9503   #>  Mean   : 1.996   Mean   :0.9988   #>  3rd Qu.: 1.990   3rd Qu.:1.0793   #>  Max.   :16.700   Max.   :4.2977   #> m <- glm(wt82_71 ~ qsmk + sex + race + age + I(age^2) +          as.factor(education) + smokeintensity + I(smokeintensity^2) +          smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +           wt71 + I(wt71^2),          data = nhefs_dat) summary(m) #>  #> Call: #> glm(formula = wt82_71 ~ qsmk + sex + race + age + I(age^2) +  #>     as.factor(education) + smokeintensity + I(smokeintensity^2) +  #>     smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +  #>     wt71 + I(wt71^2), data = nhefs_dat) #>  #> Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)           -1.6586176  4.3137734  -0.384 0.700666     #> qsmk                   3.4626218  0.4384543   7.897 5.36e-15 *** #> sex                   -1.4650496  0.4683410  -3.128 0.001792 **  #> race                   0.5864117  0.5816949   1.008 0.313560     #> age                    0.3626624  0.1633431   2.220 0.026546 *   #> I(age^2)              -0.0061377  0.0017263  -3.555 0.000389 *** #> as.factor(education)2  0.8185263  0.6067815   1.349 0.177546     #> as.factor(education)3  0.5715004  0.5561211   1.028 0.304273     #> as.factor(education)4  1.5085173  0.8323778   1.812 0.070134 .   #> as.factor(education)5 -0.1708264  0.7413289  -0.230 0.817786     #> smokeintensity         0.0651533  0.0503115   1.295 0.195514     #> I(smokeintensity^2)   -0.0010468  0.0009373  -1.117 0.264261     #> smokeyrs               0.1333931  0.0917319   1.454 0.146104     #> I(smokeyrs^2)         -0.0018270  0.0015438  -1.183 0.236818     #> as.factor(exercise)1   0.3206824  0.5349616   0.599 0.548961     #> as.factor(exercise)2   0.3628786  0.5589557   0.649 0.516300     #> as.factor(active)1    -0.9429574  0.4100208  -2.300 0.021593 *   #> as.factor(active)2    -0.2580374  0.6847219  -0.377 0.706337     #> wt71                   0.0373642  0.0831658   0.449 0.653297     #> I(wt71^2)             -0.0009158  0.0005235  -1.749 0.080426 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for gaussian family taken to be 53.59474) #>  #>     Null deviance: 97176  on 1565  degrees of freedom #> Residual deviance: 82857  on 1546  degrees of freedom #> AIC: 10701 #>  #> Number of Fisher Scoring iterations: 2 m2 <- standardize_glm(wt82_71 ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              values = list(qsmk = c(0,1)),             contrasts = c(\"difference\", \"ratio\"),             references = \"0\")  m2 #> Outcome formula: wt82_71 ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: gaussian  #> Outcome link function: identity  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std. Error lower 0.95 upper 0.95 #> 1    0     1.75      0.217       1.32       2.17 #> 2    1     5.21      0.420       4.39       6.03 #>  #> Reference level:  = 0  #> Contrast:  difference  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0     0.00      0.000       0.00       0.00 #> 2        1     3.46      0.466       2.55       4.38 #>  #> Reference level:  = 0  #> Contrast:  ratio  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0     1.00      0.000       1.00       1.00 #> 2        1     2.98      0.435       2.13       3.83  plot(m2) plot(m2, contrast = \"difference\", reference = 0) m2_dr <- standardize_glm_dr(formula_outcome = wt82_71 ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),                 formula_exposure = qsmk ~ sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                  smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                  wt71 + I(wt71^2),             data = nhefs_dat,              values = list(qsmk = c(0,1)),             reference = 0)  #> Warning in format_result_standardize(res, contrasts, references, transforms, : #> Reference level or contrast not specified. Defaulting to NULL.  m2_dr #> Doubly robust estimator with:  #>  #> Exposure formula: qsmk ~ sex + race + age + I(age^2) + as.factor(education) + smokeintensity +  #>     I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + as.factor(exercise) +  #>     as.factor(active) + wt71 + I(wt71^2) #> Exposure link function: logit  #> Outcome formula: wt82_71 ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: gaussian  #> Outcome link function: identity  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std. Error lower 0.95 upper 0.95 #> 1    0     1.76      0.218       1.34       2.19 #> 2    1     5.19      0.438       4.33       6.04"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael C Sachs. Author, maintainer. Arvid Sjölander. Author. Erin E Gabriel. Author. Johan Sebastian Ohlendorff. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sachs M, Sjölander , Gabriel E, Ohlendorff J (2023). stdReg2: Regression Standardization Causal Inference. R package version 0.2.0.","code":"@Manual{,   title = {stdReg2: Regression Standardization for Causal Inference},   author = {Michael C Sachs and Arvid Sjölander and Erin E Gabriel and Johan Sebastian Ohlendorff},   year = {2023},   note = {R package version 0.2.0}, }"},{"path":"/index.html","id":"stdreg2-regression-standardization-for-causal-inference","dir":"","previous_headings":"","what":"Regression Standardization for Causal Inference","title":"Regression Standardization for Causal Inference","text":"Goals: create unified interface regression standardization obtain estimates causal effects average treatment effect, relative treatment effect. easy use applied practitioners, .e., easy running glm coxph. want implement modern, theoretically grounded, doubly-robust estimators, associated variance estimators. want extensible statistical researchers, .e., possible implement new estimators get models used within interface. Robust clear documentation lots examples explanation necessary assumptions. Demonstrate stability package simulations unit tests.","code":""},{"path":"/index.html","id":"difference-between-stdreg2-and-stdreg","dir":"","previous_headings":"","what":"Difference between stdReg2 and stdReg","title":"Regression Standardization for Causal Inference","text":"stdReg2 next generation stdReg. happy using stdReg, can continue using nothing change near future. stdReg2 aim solve similar problems nicer output, available methods, possibility include new methods, mainly make maintenance updating easier.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Regression Standardization for Causal Inference","text":"can install development version stdReg2 GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"sachsmc/stdReg2\")"},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Regression Standardization for Causal Inference","text":"basic example shows use regression standardization logistic regression model obtain estimates causal risk difference causal risk ratio:","code":"library(stdReg2)  # basic example # need to correctly specify the outcome model and no unmeasured confounders # (+ standard causal assumptions) set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm(  formula = Y ~ X * Z,  family = \"binomial\",  data = dd,  values = list(X = 0:1),  contrasts = c(\"difference\", \"ratio\"),  reference = 0 ) x #> Outcome formula: Y ~ X * Z #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X  #>  #> Tables:  #>   X Estimate Std. Error lower 0.95 upper 0.95 #> 1 0    0.519     0.0615      0.399      0.640 #> 2 1    0.391     0.0882      0.218      0.563 #>  #> Reference level:  = 0  #> Contrast:  difference  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0    0.000     0.0000      0.000    0.00000 #> 2        1   -0.129     0.0638     -0.254   -0.00353 #>  #> Reference level:  = 0  #> Contrast:  ratio  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0    1.000      0.000      1.000      1.000 #> 2        1    0.752      0.126      0.505      0.999"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Regression Standardization for Causal Inference","text":"","code":"citation(\"stdReg2\") #> To cite package 'stdReg2' in publications use: #>  #>   Sachs M, Sjölander A, Gabriel E, Ohlendorff J (2023). _stdReg2: #>   Regression Standardization for Causal Inference_. R package version #>   0.2.0. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Manual{, #>     title = {stdReg2: Regression Standardization for Causal Inference}, #>     author = {Michael C Sachs and Arvid Sjölander and Erin E Gabriel and Johan Sebastian Ohlendorff}, #>     year = {2023}, #>     note = {R package version 0.2.0}, #>   }"},{"path":"/reference/nhefs_dat.html","id":null,"dir":"Reference","previous_headings":"","what":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"data collected part project National Center Health Statistics National Institute Aging collaboration agencies United States Public Health Service. designed investigate relationships clinical, nutritional, behavioral factors subsequent morbidity, mortality, hospital utilization changes risk factors, functional limitation, institutionalization. dataset includes 1629 individuals contains among others following variables:","code":""},{"path":"/reference/nhefs_dat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"","code":"nhefs_dat"},{"path":"/reference/nhefs_dat.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"data frame 1566 rows 66 columns: seqn person id wt82\\_71 weight gain kilograms 1971 1982 qsmk quit smoking 1st questionnaire 1982, 1 = yes, 0 = sex 0 = male, 1 = female race 0 = white, 1 = black age age years baseline education level education 1971, 1 = 8th grade less, 2 = high school dropout, 3 = high school, 4 = college dropout, 5 = college smokeintensity number cigarettes smoked per day 1971 smokeyrs number years smoked exercise level physical activity 1971, 0 = much exercise, 1 = moderate exercise, 2 = little exercise active level activity 1971, 0 = active, 1 = moderately active, 2 = inactive, 3 = missing information wt71 weight kilograms 1971 ht height centimeters 1971","code":""},{"path":[]},{"path":"/reference/parfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits shared frailty gamma-Weibull models — parfrailty","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"parfrailty fits shared frailty gamma-Weibull models. specifically designed work function standardize_parfrailty, performs regression standardization shared frailty gamma-Weibull models.","code":""},{"path":"/reference/parfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"","code":"parfrailty(formula, data, clusterid, init)"},{"path":"/reference/parfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"formula object class \"formula\", format accepted coxph function survival package. data data frame containing variables model. clusterid string containing name cluster identification variable. init optional vector initial values model parameters.","code":""},{"path":"/reference/parfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"object class \"parfrailty\" list containing: est ML estimates \\(\\{log(\\hat{\\alpha}),log(\\hat{\\eta}), log(\\hat{\\phi}),\\hat{\\beta}\\}\\). vcov variance-covariance vector ML estimates. score matrix containing cluster-specific contributions ML score equations.","code":""},{"path":"/reference/parfrailty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"parfrailty fits shared frailty gamma-Weibull model $$\\lambda(t_{ij}|C_{ij})=\\lambda(t_{ij};\\alpha,\\eta)U_iexp\\{h(C_{ij};\\beta)\\},$$ \\(t_{ij}\\) \\(C_{ij}\\) survival time covariate vector subject \\(j\\) cluster \\(\\), respectively. \\(\\lambda(t;\\alpha,\\eta)\\) Weibull baseline hazard function $$\\eta t^{\\eta-1}\\alpha^{-\\eta},$$ \\(\\eta\\) shape parameter \\(\\alpha\\) scale parameter. \\(U_i\\) unobserved frailty term cluster \\(\\), assumed gamma distribution scale = 1/shape = \\(\\phi\\). \\(h(X;\\beta)\\) regression function specified formula argument, parametrized vector \\(\\beta\\). ML estimates \\(\\{log(\\hat{\\alpha}),log(\\hat{\\eta}),log(\\hat{\\phi}),\\hat{\\beta}\\}\\) obtained maximizing marginal (\\(U\\)) likelihood.","code":""},{"path":"/reference/parfrailty.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"left truncation present, assumed strong left truncation.  means , even truncation time may subject-specific, whole cluster unobserved least one subject cluster dies /truncation time. subjects cluster survive beyond subject-specific truncation times, whole cluster observed (Van den Berg Drepper, 2016).","code":""},{"path":"/reference/parfrailty.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"Dahlqwist E., Pawitan Y., Sjolander . (2019). Regression standardization attributable fraction estimation -within frailty models clustered survival data. Statistical Methods Medical Research 28(2), 462-485. Van den Berg G.J., Drepper B. (2016). Inference shared frailty survival models left-truncated data. Econometric Reviews, 35(6), 1075-1098.","code":""},{"path":"/reference/parfrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"Arvid Sjolander Elisabeth Dahlqwist.","code":""},{"path":"/reference/parfrailty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"","code":"require(survival) #> Loading required package: survival  # simulate data set.seed(5) n <- 200 m <- 3 alpha <- 1.5 eta <- 1 phi <- 0.5 beta <- 1 id <- rep(1:n, each = m) U <- rep(rgamma(n, shape = 1 / phi, scale = phi), each = m) X <- rnorm(n * m) # reparametrize scale as in rweibull function weibull.scale <- alpha / (U * exp(beta * X))^(1 / eta) T <- rweibull(n * m, shape = eta, scale = weibull.scale)  # right censoring C <- runif(n * m, 0, 10) D <- as.numeric(T < C) T <- pmin(T, C)  # strong left-truncation L <- runif(n * m, 0, 2) incl <- T > L incl <- ave(x = incl, id, FUN = sum) == m dd <- data.frame(L, T, D, X, id) dd <- dd[incl, ]  fit <- parfrailty(formula = Surv(L, T, D) ~ X, data = dd, clusterid = \"id\") print(fit) #> $formula #> Surv(L, T, D) ~ X #> <environment: 0x5581c82429d0> #>  #> $data #>              L          T D            X  id #> 4   1.78722376 3.03496785 1  0.634371955   2 #> 5   0.87593944 3.43397538 0 -0.231192902   2 #> 6   1.36879064 1.62048311 0 -1.368194204   2 #> 13  1.93077359 3.88018888 1 -0.869256130   5 #> 14  1.77243226 7.36242205 1 -1.332288341   5 #> 15  1.56056491 2.34032225 0  0.070562870   5 #> 19  1.22215738 9.84817895 0 -2.334691775   7 #> 20  1.21702364 5.95522302 0 -1.730891047   7 #> 21  1.49315787 2.32677008 1  0.825009586   7 #> 34  0.05522163 0.07623505 0  1.806258708  12 #> 35  0.46919068 5.65551975 0 -1.912519898  12 #> 36  0.60892023 0.64452417 0  0.199282075  12 #> 64  0.63330686 1.58382458 1  0.744600427  22 #> 65  1.19372559 8.92000775 0 -0.690190218  22 #> 66  1.57970257 2.76014646 1 -0.791411761  22 #> 67  1.81702192 3.16249173 1 -0.262081191  23 #> 68  0.48221653 7.21706324 0 -0.407917108  23 #> 69  0.18189881 0.28269222 1  0.201311056  23 #> 94  1.87380979 2.72313373 1 -0.454146942  32 #> 95  1.11637152 3.55521551 1  0.194874583  32 #> 96  0.24535298 3.02099790 1  0.031498073  32 #> 109 0.90027766 4.55355698 0 -1.515531972  37 #> 110 0.76864173 1.05942131 1  0.266538188  37 #> 111 0.05512638 2.86769961 1  0.291532260  37 #> 118 1.42239958 1.89974406 1 -0.394103521  40 #> 119 0.47835233 8.89930612 0 -1.510240230  40 #> 120 1.12377144 1.83634274 1  0.005989837  40 #> 127 0.69530126 8.08615804 0 -0.017375770  43 #> 128 1.82338834 1.94208484 0  0.541082912  43 #> 129 0.08598422 5.68351861 0 -0.540041112  43 #> 130 0.51413870 1.38826341 0 -0.080849396  44 #> 131 1.67780447 2.02832169 1 -0.029633958  44 #> 132 0.17971137 3.31347489 0 -0.145250484  44 #> 136 0.51457090 8.44363529 0 -1.154205453  46 #> 137 1.93562494 9.85561167 0  0.422597786  46 #> 138 1.77824194 7.14841753 0 -0.386989726  46 #> 139 1.87523751 5.23743428 0 -1.240525985  47 #> 140 1.81141152 3.71216593 0  0.456962145  47 #> 141 1.03691251 6.76273142 0 -1.410793353  47 #> 166 0.56609081 1.35006822 0  0.757933789  56 #> 167 0.33184205 0.37859500 1  0.359560263  56 #> 168 0.61275686 2.24092552 1 -0.829235675  56 #> 169 0.06145601 0.94865387 1 -0.445752122  57 #> 170 0.53187267 7.37740149 0 -0.670912047  57 #> 171 1.11085148 7.33086430 0 -0.800916902  57 #> 175 0.33909702 0.41699230 1  1.289449757  59 #> 176 1.97795961 5.06034775 1 -0.143912736  59 #> 177 0.17176757 0.67997250 0 -2.288008838  59 #> 250 0.10938821 3.22209051 0 -0.680978405  84 #> 251 0.50699914 6.26629895 0 -2.794959603  84 #> 252 1.25613561 3.98106755 0 -1.084586126  84 #> 259 1.52728552 6.91797197 0 -1.017287856  87 #> 260 1.25885725 1.91230933 1  1.014988922  87 #> 261 1.50238017 7.78931048 0 -0.651507685  87 #> 262 0.58556676 4.63435814 1  0.038215849  88 #> 263 0.96045975 2.80293191 1  0.184078667  88 #> 264 0.15020993 1.11106961 1  0.451722586  88 #> 274 0.18629962 0.90293922 0 -0.335845291  92 #> 275 1.92073994 2.92865786 1 -0.010529493  92 #> 276 0.32608739 1.05008773 1 -0.139135003  92 #> 286 0.73120809 2.17042107 1  0.126737533  96 #> 287 0.62003899 1.88088515 1 -0.485950257  96 #> 288 0.39135693 8.31694088 0 -1.003831691  96 #> 289 0.73954127 8.36045880 0  1.784203198  97 #> 290 0.70089152 1.28215405 1 -0.867144559  97 #> 291 0.54075855 8.57589681 0 -0.970574785  97 #> 298 0.27611811 0.82661051 1 -0.873557167 100 #> 299 0.15071073 0.49972060 1  0.111954035 100 #> 300 0.27641701 0.67741073 1  1.514231380 100 #> 316 0.92415468 5.11869150 0  0.082773252 106 #> 317 1.18738138 4.50767761 0 -1.585977289 106 #> 318 0.11445318 0.93428980 0 -1.529555425 106 #> 358 1.90755391 2.36375300 0 -0.628017639 120 #> 359 1.85712558 5.53994832 1  0.253333621 120 #> 360 1.67931147 2.57420883 0 -0.025188786 120 #> 394 0.71279878 1.32882429 1  0.460090646 132 #> 395 0.42105843 0.85011831 1 -0.218147958 132 #> 396 1.33021640 2.55882491 1  1.323011066 132 #> 403 1.20056311 5.05213969 0 -0.127865246 135 #> 404 1.10080783 8.38274394 0 -0.890580307 135 #> 405 0.26320789 1.16477906 1  0.896985420 135 #> 406 0.13875032 0.33482656 1  1.350929367 136 #> 407 1.88179018 6.89945541 0 -1.032052945 136 #> 408 0.57307729 1.69205188 1  0.513654664 136 #> 424 0.38881144 3.35369691 0 -0.966578842 142 #> 425 0.27245622 0.63939907 1  0.375867015 142 #> 426 1.36902177 6.59952559 0 -2.020810170 142 #> 427 0.19699762 1.46133286 1  0.244792596 143 #> 428 0.14960346 1.54345792 1  0.024357435 143 #> 429 0.43622702 6.56075123 0 -0.937410840 143 #> 445 0.57758966 4.79877953 0 -1.654511931 149 #> 446 1.86685356 8.30339929 0 -1.984728500 149 #> 447 0.47436808 6.92532206 1 -0.514325863 149 #> 448 1.77473551 3.51203199 0 -1.963281412 150 #> 449 1.85746704 4.18200700 0  0.303169082 150 #> 450 0.64277312 0.86599516 0  0.613763724 150 #> 469 0.16195196 0.44779638 1  0.354026642 157 #> 470 1.21458744 3.67393086 1 -0.764599960 157 #> 471 1.12302965 1.65112343 1 -0.509099170 157 #> 529 0.48063096 0.80881503 1  1.249536805 177 #> 530 1.28349557 3.27313504 0  0.496885618 177 #> 531 1.33671206 2.47686403 1 -0.309076753 177 #> 571 0.38581986 6.00508764 0 -0.913394120 191 #> 572 0.82877988 0.93946968 0  1.225612645 191 #> 573 1.85757261 4.85840462 0 -1.153224135 191 #> 589 0.70452328 1.60237702 0 -1.441137171 197 #> 590 0.22542309 0.72176056 1  1.638578200 197 #> 591 1.73939423 3.62888657 0 -1.923365406 197 #> 598 0.62471978 1.17165533 0 -0.453984276 200 #> 599 1.52603893 2.83751867 0 -0.097612023 200 #> 600 0.21336639 2.14748748 1 -0.337073721 200 #>  #> $clusterid #> [1] \"id\" #>  #> $ncluster #> [1] 37 #>  #> $n #> [1] 111 #>  #> $X #>                X #> 4    0.634371955 #> 5   -0.231192902 #> 6   -1.368194204 #> 13  -0.869256130 #> 14  -1.332288341 #> 15   0.070562870 #> 19  -2.334691775 #> 20  -1.730891047 #> 21   0.825009586 #> 34   1.806258708 #> 35  -1.912519898 #> 36   0.199282075 #> 64   0.744600427 #> 65  -0.690190218 #> 66  -0.791411761 #> 67  -0.262081191 #> 68  -0.407917108 #> 69   0.201311056 #> 94  -0.454146942 #> 95   0.194874583 #> 96   0.031498073 #> 109 -1.515531972 #> 110  0.266538188 #> 111  0.291532260 #> 118 -0.394103521 #> 119 -1.510240230 #> 120  0.005989837 #> 127 -0.017375770 #> 128  0.541082912 #> 129 -0.540041112 #> 130 -0.080849396 #> 131 -0.029633958 #> 132 -0.145250484 #> 136 -1.154205453 #> 137  0.422597786 #> 138 -0.386989726 #> 139 -1.240525985 #> 140  0.456962145 #> 141 -1.410793353 #> 166  0.757933789 #> 167  0.359560263 #> 168 -0.829235675 #> 169 -0.445752122 #> 170 -0.670912047 #> 171 -0.800916902 #> 175  1.289449757 #> 176 -0.143912736 #> 177 -2.288008838 #> 250 -0.680978405 #> 251 -2.794959603 #> 252 -1.084586126 #> 259 -1.017287856 #> 260  1.014988922 #> 261 -0.651507685 #> 262  0.038215849 #> 263  0.184078667 #> 264  0.451722586 #> 274 -0.335845291 #> 275 -0.010529493 #> 276 -0.139135003 #> 286  0.126737533 #> 287 -0.485950257 #> 288 -1.003831691 #> 289  1.784203198 #> 290 -0.867144559 #> 291 -0.970574785 #> 298 -0.873557167 #> 299  0.111954035 #> 300  1.514231380 #> 316  0.082773252 #> 317 -1.585977289 #> 318 -1.529555425 #> 358 -0.628017639 #> 359  0.253333621 #> 360 -0.025188786 #> 394  0.460090646 #> 395 -0.218147958 #> 396  1.323011066 #> 403 -0.127865246 #> 404 -0.890580307 #> 405  0.896985420 #> 406  1.350929367 #> 407 -1.032052945 #> 408  0.513654664 #> 424 -0.966578842 #> 425  0.375867015 #> 426 -2.020810170 #> 427  0.244792596 #> 428  0.024357435 #> 429 -0.937410840 #> 445 -1.654511931 #> 446 -1.984728500 #> 447 -0.514325863 #> 448 -1.963281412 #> 449  0.303169082 #> 450  0.613763724 #> 469  0.354026642 #> 470 -0.764599960 #> 471 -0.509099170 #> 529  1.249536805 #> 530  0.496885618 #> 531 -0.309076753 #> 571 -0.913394120 #> 572  1.225612645 #> 573 -1.153224135 #> 589 -1.441137171 #> 590  1.638578200 #> 591 -1.923365406 #> 598 -0.453984276 #> 599 -0.097612023 #> 600 -0.337073721 #>  #> $fit #> $fit$par #> [1]  0.3746027 -0.2732453 -1.0648873  1.2779566 #>  #> $fit$value #> [1] -111.5306 #>  #> $fit$counts #> function gradient  #>       45       16  #>  #> $fit$convergence #> [1] 0 #>  #> $fit$message #> NULL #>  #>  #> $est #>     log(α)     log(η)     log(ϕ)          X  #>  0.3746027 -0.2732453 -1.0648873  1.2779566  #>  #> $score #>             log(α)      log(η)      log(ϕ)           X #>  [1,]  0.013953486  0.20289224 -0.11965534  0.37373130 #>  [2,] -0.497884032  2.52001507 -0.46772296 -1.42384130 #>  [3,] -0.069007646  0.27713823 -0.23097814  0.66557460 #>  [4,]  0.137859989 -0.14641165  0.11013129  0.08105783 #>  [5,] -0.093520015  0.33405218 -0.30480286 -0.05516350 #>  [6,] -0.146292463 -1.03889812 -0.30025772  0.56123686 #>  [7,] -0.290499183  2.16086109 -0.47596378 -0.38636190 #>  [8,]  0.011099791  0.79398549 -0.21691275  0.29200148 #>  [9,] -0.535543940  1.18487038 -0.42506246  0.44181409 #> [10,]  0.472335268 -1.88851477  1.03243696  0.46707458 #> [11,]  0.095094051  0.20316653 -0.06270845  0.11113571 #> [12,]  0.608980969 -2.91764013  1.55679132 -0.03715771 #> [13,]  0.347705224 -1.22974595  0.54900599  0.16346613 #> [14,] -0.367475159  0.77234663 -0.45571856 -1.15674320 #> [15,]  0.425117806 -1.67705538  0.05311604  0.62175200 #> [16,] -0.398640488  1.06649391 -0.57673102  0.65703311 #> [17,]  0.511627932 -0.79298325  0.18290053  0.62054930 #> [18,]  0.053590571 -0.51220966 -0.03847510  1.12121813 #> [19,] -0.079267003  1.46932616 -0.33441943  0.13383099 #> [20,] -0.486755926  1.62765783 -0.44031649  0.01347658 #> [21,] -0.137988012  0.44503587 -0.27985220  0.45230331 #> [22,]  0.497364950 -4.91200778  2.74404636 -3.73290563 #> [23,] -0.495455064  1.69041058 -0.59856851 -2.12530681 #> [24,]  0.644289075 -1.65400943  0.56131700  0.08090285 #> [25,]  0.062392090  0.16838772 -0.04211771  0.04566587 #> [26,] -0.282676261  1.14737959 -0.90297310 -1.35697090 #> [27,]  0.387083643 -1.36082538  0.33269226  0.72348819 #> [28,] -0.139397789  0.18403258 -0.34557360  0.97437992 #> [29,] -0.077711350 -0.26019634 -0.16042625  0.90063440 #> [30,]  0.111303122  0.43070182 -0.18646515  0.65027345 #> [31,]  0.297062945 -0.01523354 -0.05276062  0.60089034 #> [32,]  0.258589968 -0.99229482  0.45701684 -0.06569007 #> [33,] -1.050040656  2.42648364 -0.34545708 -0.40334180 #> [34,] -0.155333955  0.63706623 -0.49576788 -0.24783674 #> [35,]  0.180210992 -0.77083636  0.34050852  0.53482735 #> [36,]  0.191151851  0.01906961  0.08256861 -0.21955355 #> [37,] -0.003373936  0.40771967 -0.14288170 -0.07758479 #>  #> $vcov #>             log(α)     log(η)      log(ϕ)          X #> log(α)  0.38182395 0.09552250 -0.02123094 0.01369009 #> log(η)  0.09552250 0.08284338  0.11211763 0.04021393 #> log(ϕ) -0.02123094 0.11211763  0.32410342 0.07514008 #> X       0.01369009 0.04021393  0.07514008 0.07210706 #>  #> $call #> parfrailty(formula = Surv(L, T, D) ~ X, data = dd, clusterid = \"id\") #>  #> attr(,\"class\") #> [1] \"parfrailty\""},{"path":"/reference/plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots regression standardization fit — plot.std_surv","title":"Plots regression standardization fit — plot.std_surv","text":"plot method class \"std_glm\".","code":""},{"path":"/reference/plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots regression standardization fit — plot.std_surv","text":"","code":"# S3 method for std_surv plot(   x,   plot_ci = TRUE,   ci_type = \"plain\",   ci_level = 0.95,   transform = NULL,   contrast = NULL,   reference = NULL,   legendpos = \"bottomleft\",   summary_fun = \"summary_std_coxph\",   ... )  # S3 method for std_glm plot(   x,   plot_ci = TRUE,   ci_type = \"plain\",   ci_level = 0.95,   transform = NULL,   contrast = NULL,   reference = NULL,   summary_fun = \"summary_std_glm\",   ... )  # S3 method for std_custom plot(   x,   plot_ci = TRUE,   ci_level = 0.95,   transform = NULL,   contrast = NULL,   reference = NULL,   ... )"},{"path":"/reference/plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots regression standardization fit — plot.std_surv","text":"x object class \"std_glm\". plot_ci TRUE, add confidence intervals plot. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. ci_level Coverage probability confidence intervals. transform set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. left unspecified, \\(\\psi(x)=\\theta(x)\\). contrast set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL, doubly robust estimator standardized estimator used. reference contrast specified, desired reference level. legendpos position legend; see help legend. summary_fun internal use . change. ... Unused.","code":""},{"path":"/reference/plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots regression standardization fit — plot.std_surv","text":"","code":"# see standardize_glm"},{"path":"/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Prints summary of regression standardization fit — print.std_surv","title":"Prints summary of regression standardization fit — print.std_surv","text":"Prints summary regression standardization fit","code":""},{"path":"/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prints summary of regression standardization fit — print.std_surv","text":"","code":"# S3 method for std_surv print(x, ...)  # S3 method for std_glm print(x, ...)  # S3 method for std_custom print(x, ...)"},{"path":"/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prints summary of regression standardization fit — print.std_surv","text":"x object class \"std_glm\", \"std_surv\" \"std_custom\". ... unused","code":""},{"path":"/reference/sandwich.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the sandwich variance components from a model fit — sandwich","title":"Compute the sandwich variance components from a model fit — sandwich","text":"Compute sandwich variance components model fit","code":""},{"path":"/reference/sandwich.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the sandwich variance components from a model fit — sandwich","text":"","code":"sandwich(fit, data, weights, t, fit.detail)"},{"path":"/reference/sandwich.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the sandwich variance components from a model fit — sandwich","text":"fit fitted model object class glm, coxph, ah, survfit data data used fit model weights Optional weights t Optional fixed time point survival objects fit.detail Additional information survival objects, see Details","code":""},{"path":"/reference/sandwich.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the sandwich variance components from a model fit — sandwich","text":"list consisting Fisher information matrix () Score equations (U)","code":""},{"path":"/reference/standardize.html","id":null,"dir":"Reference","previous_headings":"","what":"Get standardized estimates using the g-formula with a custom model — standardize","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Get standardized estimates using g-formula custom model","code":""},{"path":"/reference/standardize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"","code":"standardize(   arguments,   data,   fitter,   predict_fun,   values,   B = NULL,   ci_level = 0.95,   contrasts = NULL,   references = NULL,   seed = NULL,   times = NULL,   transforms = NULL )"},{"path":"/reference/standardize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"arguments arguments used fitter function list. data data. fitter function call fit data. predict_fun function used predict means/probabilities new data set response level. survival data, matrix column time, row data. values named list data.frame specifying variables values marginal means outcome estimated. B Number nonparametric bootstrap resamples. Default NULL (bootstrap). ci_level Coverage probability confidence intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. references vector references following format: contrasts NULL, desired reference level(s). seed seed use nonparametric bootstrap. times use survival data. Set NULL otherwise. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"object class std_custom. basically list components estimates fit outcome model.","code":""},{"path":"/reference/standardize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. standardize uses model estimate standardized mean \\(\\theta(x)=E\\{E(Y|X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\(Z\\). survival data, \\(Y=(T > t)\\), vector different time points times (\\(t\\)) can given, \\(T\\) uncensored survival time.","code":""},{"path":"/reference/standardize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams \\& Wilkins. Sjolander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjolander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"/reference/standardize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"","code":"set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) prob_predict.glm <- function(...) predict.glm(..., type = \"response\")  x <- standardize(   arguments = list(     formula = Y ~ X * Z,     family = \"binomial\"   ),   predict_fun = prob_predict.glm,   fitter = \"glm\",   data = dd,   values = list(X = seq(-1, 1, 0.1)),   B = 100,   references = 0,   contrasts = \"difference\" ) #> Bootstrapping... This may take some time...  #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% x #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #>       X Estimate lower 0.95 upper 0.95 #> 1  -1.0    0.689      0.537      0.882 #> 2  -0.9    0.671      0.526      0.865 #> 3  -0.8    0.653      0.521      0.845 #> 4  -0.7    0.635      0.515      0.822 #> 5  -0.6    0.617      0.508      0.797 #> 6  -0.5    0.600      0.493      0.770 #> 7  -0.4    0.583      0.476      0.745 #> 8  -0.3    0.566      0.466      0.720 #> 9  -0.2    0.550      0.450      0.699 #> 10 -0.1    0.534      0.429      0.679 #> 11  0.0    0.519      0.409      0.658 #> 12  0.1    0.504      0.389      0.638 #> 13  0.2    0.490      0.369      0.621 #> 14  0.3    0.476      0.348      0.608 #> 15  0.4    0.462      0.322      0.595 #> 16  0.5    0.449      0.298      0.587 #> 17  0.6    0.437      0.275      0.583 #> 18  0.7    0.425      0.254      0.577 #> 19  0.8    0.413      0.235      0.571 #> 20  0.9    0.401      0.222      0.561 #> 21  1.0    0.391      0.207      0.551 #>  #> Reference level:  = 0  #> Contrast:  difference  #>    Exposure Estimate lower 0.95 upper 0.95 #> 1      -1.0   0.1697    0.04037    0.30879 #> 2      -0.9   0.1516    0.03562    0.28505 #> 3      -0.8   0.1335    0.03104    0.25348 #> 4      -0.7   0.1156    0.02663    0.22134 #> 5      -0.6   0.0980    0.02238    0.19201 #> 6      -0.5   0.0807    0.01829    0.16211 #> 7      -0.4   0.0637    0.01435    0.13054 #> 8      -0.3   0.0472    0.01056    0.09810 #> 9      -0.2   0.0310    0.00691    0.06525 #> 10     -0.1   0.0153    0.00339    0.03243 #> 11      0.0   0.0000    0.00000    0.00000 #> 12      0.1  -0.0148   -0.03177   -0.00326 #> 13      0.2  -0.0292   -0.06286   -0.00641 #> 14      0.3  -0.0432   -0.09365   -0.00944 #> 15      0.4  -0.0566   -0.12370   -0.01237 #> 16      0.5  -0.0697   -0.15192   -0.01519 #> 17      0.6  -0.0823   -0.17663   -0.01791 #> 18      0.7  -0.0945   -0.19954   -0.02054 #> 19      0.8  -0.1062   -0.22074   -0.02308 #> 20      0.9  -0.1176   -0.24033   -0.02553 #> 21      1.0  -0.1285   -0.25891   -0.02790 #>  plot(x)  plot(x, reference = 0, contrast = \"difference\")  plot(x, reference = 0, contrast = \"difference\", plot_ci = FALSE)   require(survival) prob_predict.coxph <- function(object, newdata, times) {   fit.detail <- suppressWarnings(basehaz(object))   cum.haz <- fit.detail$hazard[sapply(times, function(x) max(which(fit.detail$time <= x)))]   predX <- predict(object = object, newdata = newdata, type = \"risk\")   res <- matrix(NA, ncol = length(times), nrow = length(predX))   for (ti in seq_len(length(times))) {     res[, ti] <- exp(-predX * cum.haz[ti])   }   res } set.seed(68) n <- 500 Z <- rnorm(n) X <- rnorm(n, mean = Z) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, X, U, D) x <- standardize(   arguments = list(     formula = Surv(U, D) ~ X + Z + X * Z,     method = \"breslow\",     x = TRUE,     y = TRUE   ),   fitter = \"coxph\",   data = dd,   times = 1:5,   predict_fun = prob_predict.coxph,   values = list(X = c(-1, 0, 1)),   B = 100,   references = 0,   contrasts = \"difference\" ) #> Bootstrapping... This may take some time...  #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% x #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #> Time:  1  #>    X  estimate     lower     upper #> 1 -1 0.6691328 0.6078557 0.7171407 #> 2  0 0.3632410 0.3011138 0.4060458 #> 3  1 0.2466151 0.1878811 0.2902875 #>  #> Time:  2  #>    X  estimate     lower     upper #> 1 -1 0.4676722 0.3847559 0.5519054 #> 2  0 0.2135942 0.1659279 0.2611208 #> 3  1 0.1656470 0.1168408 0.2079257 #>  #> Time:  3  #>    X  estimate      lower     upper #> 1 -1 0.3505275 0.26905828 0.4459009 #> 2  0 0.1520739 0.11902194 0.2044295 #> 3  1 0.1315275 0.09380255 0.1761028 #>  #> Time:  4  #>    X  estimate      lower     upper #> 1 -1 0.2546911 0.12386946 0.3508293 #> 2  0 0.1101689 0.06701637 0.1505347 #> 3  1 0.1069771 0.06241746 0.1430167 #>  #> Time:  5  #>    X   estimate      lower     upper #> 1 -1 0.16309819 0.07709911 0.2613375 #> 2  0 0.07480425 0.03816440 0.1127717 #> 3  1 0.08456621 0.04640860 0.1207358 #>  #>  #> Reference level:  = 0  #> Contrast:  difference  #> Time:  1  #>   Exposure   estimate      lower       upper #> 1       -1  0.3058918  0.2515877  0.35267464 #> 2        0  0.0000000  0.0000000  0.00000000 #> 3        1 -0.1166259 -0.1534746 -0.08540127 #>  #> Time:  2  #>   Exposure    estimate       lower       upper #> 1       -1  0.25407801  0.18502799  0.33076959 #> 2        0  0.00000000  0.00000000  0.00000000 #> 3        1 -0.04794719 -0.07480939 -0.02350433 #>  #> Time:  3  #>   Exposure    estimate       lower       upper #> 1       -1  0.19845364  0.12283854 0.284032243 #> 2        0  0.00000000  0.00000000 0.000000000 #> 3        1 -0.02054638 -0.04719901 0.001297897 #>  #> Time:  4  #>   Exposure     estimate       lower      upper #> 1       -1  0.144522281  0.04976681 0.22983546 #> 2        0  0.000000000  0.00000000 0.00000000 #> 3        1 -0.003191768 -0.02345454 0.01911037 #>  #> Time:  5  #>   Exposure    estimate        lower      upper #> 1       -1 0.088293936  0.018981386 0.17857030 #> 2        0 0.000000000  0.000000000 0.00000000 #> 3        1 0.009761965 -0.009490497 0.02585953 #>  #>  plot(x)  plot(x, reference = 0, contrast = \"difference\")  plot(x, reference = 0, contrast = \"difference\", plot_ci = FALSE)"},{"path":"/reference/standardize_coxph.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in Cox proportional hazards models — standardize_coxph","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"standardize_coxph performs regression standardization Cox proportional hazards models, specified values exposure, sample covariate distribution. Let \\(T\\), \\(X\\), \\(Z\\) survival outcome, exposure, vector covariates, respectively. standardize_coxph uses fitted Cox proportional hazards model estimate standardized survival function \\(\\theta(t,x)=E\\{S(t|X=x,Z)\\}\\), \\(t\\) specific value \\(T\\), \\(x\\) specific value \\(X\\), expectation marginal distribution \\(Z\\).","code":""},{"path":"/reference/standardize_coxph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"","code":"standardize_coxph(   formula,   data,   values,   times,   clusterid,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   references = NULL,   transforms = NULL )"},{"path":"/reference/standardize_coxph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"formula formula used fit glm model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. times vector containing specific values \\(T\\) estimate standardized survival function. clusterid optional string containing name cluster identification variable data clustered. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. references vector references following format: contrasts NULL, desired reference level(s). transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_coxph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"object class std_surv. basically list components estimates covariance estimates res Results transformations, contrasts, references stored res_contrasts.  output contains estimates contrasts confidence intervals combinations transforms, references","code":""},{"path":"/reference/standardize_coxph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"standardize_coxph assumes Cox proportional hazards model $$\\lambda(t|X,Z)=\\lambda_0(t)exp\\{h(X,Z;\\beta)\\}$$ fitted. Breslow's estimator cumulative baseline hazard \\(\\Lambda_0(t)=\\int_0^t\\lambda_0(u)du\\) used together partial likelihood estimate \\(\\beta\\) obtain estimates survival function \\(S(t|X=x,Z)\\): $$\\hat{S}(t|X=x,Z)=exp[-\\hat{\\Lambda}_0(t)exp\\{h(X=x,Z;\\hat{\\beta})\\}].$$ \\(t\\) t argument \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\)) produce estimates $$\\hat{\\theta}(t,x)=\\sum_{=1}^n \\hat{S}(t|X=x,Z_i)/n,$$ \\(Z_i\\) value \\(Z\\) subject \\(\\), \\(=1,...,n\\).  variance \\(\\hat{\\theta}(t,x)\\) obtained sandwich formula.","code":""},{"path":"/reference/standardize_coxph.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Standardized survival functions sometimes referred (direct) adjusted survival functions literature. standardize_coxph/standardize_parfrailty currently handle time-varying exposures covariates. standardize_coxph/standardize_parfrailty internally loops values t argument. Therefore, function usually considerably faster length(t) small. variance calculation performed standardize_coxph condition observed covariates \\(\\bar{Z}=(Z_1,...,Z_n)\\). see matters, note $$var\\{\\hat{\\theta}(t,x)\\}=E[var\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) Cox proportional hazards model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(t,x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\). variance calculation Gail Byar (1986) ignores term, thus effectively conditions \\(\\bar{Z}\\).","code":""},{"path":"/reference/standardize_coxph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Chang .M., Gelman G., Pagano M. (1982). Corrected group prognostic curves summary statistics. Journal Chronic Diseases 35, 669-674. Gail M.H. Byar D.P. (1986). Variance calculations direct adjusted survival curves, applications testing treatment effect. Biometrical Journal 28(5), 587-599. Makuch R.W. (1982). Adjusted survival curve estimation using covariates. Journal Chronic Diseases 35, 437-443. Sjolander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjolander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"/reference/standardize_coxph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Arvid Sjolander","code":""},{"path":"/reference/standardize_coxph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"","code":"require(survival) set.seed(7) n <- 300 Z <- rnorm(n) X <- rnorm(n, mean = Z) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, X, U, D) fit.std <- standardize_coxph(   formula = Surv(U, D) ~ X + Z + X * Z,   data = dd,   values = list(X = seq(-1, 1, 0.5)),   times = 1:5 ) print(fit.std) #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:   #> Survival functions evaluated at t = 1  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.708     0.0479      0.614      0.802 #> [2,]    0.563     0.0514      0.462      0.663 #> [3,]    0.426     0.0427      0.343      0.510 #> [4,]    0.340     0.0391      0.264      0.417 #> [5,]    0.291     0.0389      0.214      0.367 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:   #> Survival functions evaluated at t = 2  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.469     0.0679      0.336      0.602 #> [2,]    0.320     0.0493      0.223      0.416 #> [3,]    0.238     0.0366      0.166      0.309 #> [4,]    0.201     0.0342      0.134      0.268 #> [5,]    0.183     0.0339      0.117      0.250 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:   #> Survival functions evaluated at t = 3  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.372     0.0694     0.2357      0.508 #> [2,]    0.241     0.0448     0.1531      0.329 #> [3,]    0.184     0.0342     0.1168      0.251 #> [4,]    0.162     0.0323     0.0988      0.225 #> [5,]    0.153     0.0322     0.0898      0.216 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:   #> Survival functions evaluated at t = 4  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.253     0.0671     0.1216      0.385 #> [2,]    0.157     0.0399     0.0791      0.235 #> [3,]    0.128     0.0320     0.0655      0.191 #> [4,]    0.121     0.0305     0.0611      0.181 #> [5,]    0.120     0.0305     0.0602      0.180 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:   #> Survival functions evaluated at t = 5  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]   0.0867     0.0439    0.00071      0.173 #> [2,]   0.0572     0.0274    0.00350      0.111 #> [3,]   0.0588     0.0255    0.00871      0.109 #> [4,]   0.0658     0.0261    0.01468      0.117 #> [5,]   0.0735     0.0270    0.02055      0.126 #>  plot(fit.std)"},{"path":"/reference/standardize_gee.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in conditional generalized estimating equations — standardize_gee","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"standardize_gee performs regression standardization linear log-linear fixed effects models, specified values exposure, sample covariate distribution. Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. assumed data clustered cluster indicator \\(\\). standardize_gee uses fitted fixed effects model, cluster-specific intercept \\(a_i\\) (see details), estimate standardized mean \\(\\theta(x)=E\\{E(Y|,X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\((a_i,Z)\\).","code":""},{"path":"/reference/standardize_gee.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"","code":"standardize_gee(   formula,   link = \"identity\",   data,   values,   clusterid,   case_control = FALSE,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   references = NULL,   transforms = NULL )"},{"path":"/reference/standardize_gee.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"formula formula used \"gee\" drgee package. link link function used \"gee\". data data. values named list data.frame specifying variables values marginal means outcome estimated. clusterid optional string containing name cluster identification variable data clustered. case_control Whether data comes case-control study. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. references vector references following format: contrasts NULL, desired reference level(s). transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_gee.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"object class std_glm. basically list components estimates covariance estimates res Results transformations, contrasts, references stored res_contrasts.","code":""},{"path":"/reference/standardize_gee.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"standardize_gee assumes fixed effects model $$\\eta\\{E(Y|,X,Z)\\}=a_i+h(X,Z;\\beta)$$ fitted. link function \\(\\eta\\) assumed identity link log link. conditional generalized estimating equation (CGGE) estimate \\(\\beta\\) used obtain estimates cluster-specific means: $$\\hat{}_i=\\sum_{j=1}^{n_i}r_{ij}/n_i,$$ $$r_{ij}=Y_{ij}-h(X_{ij},Z_{ij};\\hat{\\beta})$$ \\(\\eta\\) identity link, $$r_{ij}=Y_{ij}exp\\{-h(X_{ij},Z_{ij};\\hat{\\beta})\\}$$ \\(\\eta\\) log link, \\((X_{ij},Z_{ij})\\) value \\((X,Z)\\) subject \\(j\\) cluster \\(\\), \\(j=1,...,n_i\\), \\(=1,...,n\\). CGEE estimate \\(\\beta\\) estimate \\(a_i\\) used estimate mean \\(E(Y|,X=x,Z)\\): $$\\hat{E}(Y|,X=x,Z)=\\eta^{-1}\\{\\hat{}_i+h(X=x,Z;\\hat{\\beta})\\}.$$ \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\) estimated values \\(a_i\\)) produce estimates $$\\hat{\\theta}(x)=\\sum_{=1}^n \\sum_{j=1}^{n_i} \\hat{E}(Y|,X=x,Z_i)/N,$$ \\(N=\\sum_{=1}^n n_i\\). variance \\(\\hat{\\theta}(x)\\) obtained sandwich formula.","code":""},{"path":"/reference/standardize_gee.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"variance calculation performed standardize_gee condition observed covariates \\(\\bar{Z}=(Z_{11},...,Z_{nn_i})\\). see matters, note $$var\\{\\hat{\\theta}(x)\\}=E[var\\{\\hat{\\theta}(x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) generalized linear model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\).","code":""},{"path":"/reference/standardize_gee.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"Goetgeluk S. Vansteelandt S. (2008). Conditional generalized estimating equations analysis clustered longitudinal data. Biometrics 64(3), 772-780. Martin R.S. (2017). Estimation average marginal effects multiplicative unobserved effects panel models. Economics Letters 160, 16-19. Sjolander . (2019). Estimation marginal causal effects presence confounding cluster. Biostatistics doi: 10.1093/biostatistics/kxz054","code":""},{"path":"/reference/standardize_gee.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"Arvid Sjolander.","code":""},{"path":"/reference/standardize_gee.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"","code":"require(drgee) #> Loading required package: drgee  set.seed(4) n <- 300 ni <- 2 id <- rep(1:n, each = ni) ai <- rep(rnorm(n), each = ni) Z <- rnorm(n * ni) X <- rnorm(n * ni, mean = ai + Z) Y <- rnorm(n * ni, mean = ai + X + Z + 0.1 * X^2) dd <- data.frame(id, Z, X, Y) fit.std <- standardize_gee(   formula = Y ~ X + Z + I(X^2),   link = \"identity\",   data = dd,   values = list(X = seq(-3, 3, 0.5)),   clusterid = \"id\" ) print(fit.std) #> Outcome formula: Y ~ X + Z + I(X^2) #> <environment: 0x5581cad6d658> #> Outcome family:  #> Outcome link function:  #> Exposure:  X  #>  #> Tables:  #>       X Estimate Std. Error lower 0.95 upper 0.95 #> 1  -3.0  -2.3117     0.1895     -2.683     -1.940 #> 2  -2.5  -2.0248     0.1531     -2.325     -1.725 #> 3  -2.0  -1.6962     0.1246     -1.940     -1.452 #> 4  -1.5  -1.3258     0.1045     -1.531     -1.121 #> 5  -1.0  -0.9137     0.0930     -1.096     -0.731 #> 6  -0.5  -0.4598     0.0894     -0.635     -0.285 #> 7   0.0   0.0358     0.0920     -0.144      0.216 #> 8   0.5   0.5732     0.0991      0.379      0.767 #> 9   1.0   1.1523     0.1101      0.936      1.368 #> 10  1.5   1.7731     0.1249      1.528      2.018 #> 11  2.0   2.4357     0.1442      2.153      2.718 #> 12  2.5   3.1401     0.1686      2.810      3.471 #> 13  3.0   3.8862     0.1989      3.496      4.276 #>  plot(fit.std)"},{"path":"/reference/standardize_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Get regression standardized estimates from a glm — standardize_glm","title":"Get regression standardized estimates from a glm — standardize_glm","text":"Get regression standardized estimates glm","code":""},{"path":"/reference/standardize_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get regression standardized estimates from a glm — standardize_glm","text":"","code":"standardize_glm(   formula,   data,   values,   clusterid,   matched_density_cases,   matched_density_controls,   matching_variable,   p_population,   case_control = FALSE,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   references = NULL,   transforms = NULL )"},{"path":"/reference/standardize_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get regression standardized estimates from a glm — standardize_glm","text":"formula formula used fit glm model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. clusterid optional string containing name cluster identification variable data clustered. matched_density_cases function matching variable. probability (density) matched variable among cases. matched_density_controls function matching variable. probability (density) matched variable among controls. matching_variable matching variable extracted data set. p_population Specifies incidence population case_control=TRUE. case_control Whether data comes case-control study. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. references vector references following format: contrasts NULL, desired reference level(s). transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get regression standardized estimates from a glm — standardize_glm","text":"object class std_glm. basically list components estimates covariance estimates res Results transformations, contrasts, references stored res_contrasts.","code":""},{"path":"/reference/standardize_glm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get regression standardized estimates from a glm — standardize_glm","text":"standardize_glm performs regression standardization generalized linear models, specified values exposure, sample covariate distribution. Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. standardize_glm uses fitted generalized linear model estimate standardized mean \\(\\theta(x)=E\\{E(Y|X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\(Z\\).","code":""},{"path":"/reference/standardize_glm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get regression standardized estimates from a glm — standardize_glm","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams \\& Wilkins. Sjolander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjolander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"/reference/standardize_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get regression standardized estimates from a glm — standardize_glm","text":"","code":"# basic example # needs to correctly specify the outcome model and no unmeasered confounders # (+ standard causal assunmptions) set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm(   formula = Y ~ X * Z,   family = \"binomial\",   data = dd,   values = list(X = 0:1),   contrasts = c(\"difference\", \"ratio\"),   reference = 0 ) x #> Outcome formula: Y ~ X * Z #> <environment: 0x5581cfe062b0> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X  #>  #> Tables:  #>   X Estimate Std. Error lower 0.95 upper 0.95 #> 1 0    0.519     0.0615      0.399      0.640 #> 2 1    0.391     0.0882      0.218      0.563 #>  #> Reference level:  = 0  #> Contrast:  difference  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0    0.000     0.0000      0.000    0.00000 #> 2        1   -0.129     0.0638     -0.254   -0.00353 #>  #> Reference level:  = 0  #> Contrast:  ratio  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1        0    1.000      0.000      1.000      1.000 #> 2        1    0.752      0.126      0.505      0.999 #>  # different transformations of causal effects  # example from Sjolander (2016) with case-control data # here the matching variable needs to be passed as an argument singapore <- AF::singapore #> Registered S3 method overwritten by 'stdReg': #>   method             from    #>   summary.parfrailty stdReg2 Mi <- singapore$Age m <- mean(Mi) s <- sd(Mi) d <- 5 standardize_glm(   formula = Oesophagealcancer ~ (Everhotbev + Age + Dial + Samsu + Cigs)^2,   family = binomial, data = singapore,   values = list(Everhotbev = 0:1), clusterid = \"Set\",   case_control = TRUE,   matched_density_cases = function(x) dnorm(x, m, s),   matched_density_controls = function(x) dnorm(x, m - d, s),   matching_variable = Mi,   p_population = 19.3 / 100000 ) #> Warning: case_control = TRUE may not give reasonable results for the variance with clustering #> Outcome formula: Oesophagealcancer ~ (Everhotbev + Age + Dial + Samsu + Cigs)^2 #> <environment: 0x5581cfe062b0> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  Everhotbev  #>  #> Tables:  #>   Everhotbev Estimate Std. Error lower 0.95 upper 0.95 #> 1          0 0.000128   1.69e-05   9.48e-05   0.000161 #> 2          1 0.000570   2.19e-04   1.41e-04   0.000999 #>   # multiple exposures set.seed(7) n <- 100 Z <- rnorm(n) X1 <- rnorm(n, mean = Z) X2 <- rnorm(n) Y <- rbinom(n, 1, prob = (1 + exp(X1 + X2 + Z))^(-1)) dd <- data.frame(Z, X1, X2, Y) x <- standardize_glm(   formula = Y ~ X1 + X2 + Z,   family = \"binomial\",   data = dd, values = list(X1 = 0:1, X2 = 0:1),   contrasts = c(\"difference\", \"ratio\"),   references = \"0, 0\" ) x #> Outcome formula: Y ~ X1 + X2 + Z #> <environment: 0x5581cfe062b0> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X1, X2  #>  #> Tables:  #>   X1 X2 Estimate Std. Error lower 0.95 upper 0.95 #> 1  0  0    0.419     0.0576     0.3058      0.532 #> 2  1  0    0.252     0.0740     0.1074      0.397 #> 3  0  1    0.273     0.0690     0.1382      0.409 #> 4  1  1    0.146     0.0631     0.0221      0.269 #>  #> Reference level:  = 0,0  #> Contrast:  difference  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1      0,0    0.000     0.0000      0.000     0.0000 #> 2      1,0   -0.166     0.0590     -0.282    -0.0509 #> 3      0,1   -0.145     0.0458     -0.235    -0.0557 #> 4      1,1   -0.273     0.0581     -0.387    -0.1593 #>  #> Reference level:  = 0,0  #> Contrast:  ratio  #>   Exposure Estimate Std. Error lower 0.95 upper 0.95 #> 1      0,0    1.000      0.000      1.000      1.000 #> 2      1,0    0.603      0.141      0.327      0.878 #> 3      0,1    0.653      0.114      0.430      0.876 #> 4      1,1    0.348      0.131      0.091      0.605 #>   # continuous exposure set.seed(2) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rnorm(n, mean = X + Z + 0.1 * X^2) dd <- data.frame(Z, X, Y) x <- standardize_glm(   formula = Y ~ X * Z,   family = \"gaussian\",   data = dd,   values = list(X = seq(-1, 1, 0.1)) )  # plot standardized mean as a function of x plot(x)  # plot standardized mean - standardized mean at x = 0 as a function of x plot(x, contrast = \"difference\", reference = 0)"},{"path":"/reference/standardize_glm_dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"Get regression standardized doubly-robust estimates glm","code":""},{"path":"/reference/standardize_glm_dr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"","code":"standardize_glm_dr(   formula_exposure,   formula_outcome,   data,   values,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family_outcome = \"gaussian\",   family_exposure = \"binomial\",   references = NULL,   transforms = NULL )"},{"path":"/reference/standardize_glm_dr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"formula_exposure formula used fit glm model exposure. NULL, doubly robust estimator standardized estimator used. formula_outcome formula used fit glm model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family_outcome family argument used fit glm model outcome. family_exposure family argument used fit glm model exposure. references vector references following format: contrasts NULL, desired reference level(s). transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_glm_dr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"object class std_glm. basically list components estimates covariance estimates res Results transformations, contrasts, references stored res_contrasts.","code":""},{"path":"/reference/standardize_glm_dr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"standardize_glm_dr performs regression standardization generalized linear models, see e.g., documentation standardize_glm_dr. Specifically, version uses doubly robust estimator standardization, meaning inference valid either outcome regression exposure model correctly specified unmeasured confounding.","code":""},{"path":"/reference/standardize_glm_dr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"Gabriel E.E., Sachs, M.C., Martinussen T., Waernbaum ., Goetghebeur E., Vansteelandt S., Sjolander . (????), Inverse probability treatment weighting generalized linear outcome models doubly robust estimation. ????","code":""},{"path":"/reference/standardize_glm_dr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"","code":"# doubly robust estimator # needs to correctly specify either the outcome model or the exposure model # for confounding # NOTE: only works with binary exposures data <- AF::clslowbwt x <- standardize_glm_dr(   formula_outcome = bwt ~ smoker * (race + age + lwt) + I(age^2) + I(lwt^2),   formula_exposure = smoker ~ race * age * lwt + I(age^2) + I(lwt^2),   family_outcome = \"gaussian\",   family_exposure = \"binomial\",   data = data,   values = list(smoker = c(0, 1)), contrasts = \"difference\", reference = 0 )  set.seed(6) n <- 100 Z <- rnorm(n) X <- rbinom(n, 1, prob = (1 + exp(Z))^(-1)) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm_dr(   formula_outcome = Y ~ X * Z, formula_exposure = X ~ Z,   family_outcome = \"binomial\",   data = dd,   values = list(X = 0:1), references = c(0, 1), contrasts = c(\"difference\"), transforms = c(\"odds\") )"},{"path":"/reference/standardize_level.html","id":null,"dir":"Reference","previous_headings":"","what":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"Get standardized estimates using g-formula separate models exposure level data","code":""},{"path":"/reference/standardize_level.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"","code":"standardize_level(   arguments,   data,   fitter_list,   predict_fun_list,   values,   B = NULL,   ci_level = 0.95,   contrasts = NULL,   references = NULL,   seed = NULL,   times = NULL,   transforms = NULL )"},{"path":"/reference/standardize_level.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"arguments arguments used fitter function list. data data. fitter_list function call fit data (list). predict_fun_list function used predict means/probabilities new data set response level. survival data, matrix column time, row data (list). values named list data.frame specifying variables values marginal means outcome estimated. B Number nonparametric bootstrap resamples. Default NULL (bootstrap). ci_level Coverage probability confidence intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. references vector references following format: contrasts NULL, desired reference level(s). seed seed use nonparametric bootstrap. times use survival data. Set NULL otherwise. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_level.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"object class std_custom. basically list components estimates fit outcome model.","code":""},{"path":"/reference/standardize_level.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"See standardize. difference different models can fitted value x values.","code":""},{"path":"/reference/standardize_level.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams \\& Wilkins. Sjolander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjolander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"/reference/standardize_level.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"","code":"require(survival) prob_predict.coxph <- function(object, newdata, times) {   fit.detail <- suppressWarnings(basehaz(object))   cum.haz <- fit.detail$hazard[sapply(times, function(x) max(which(fit.detail$time <= x)))]   predX <- predict(object = object, newdata = newdata, type = \"risk\")   res <- matrix(NA, ncol = length(times), nrow = length(predX))   for (ti in seq_len(length(times))) {     res[, ti] <- exp(-predX * cum.haz[ti])   }   res }  set.seed(68) n <- 500 Z <- rnorm(n) X <- rbinom(n, 1, prob = 0.5) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, X, U, D) x <- standardize_level(   arguments = list(     list(       formula = Surv(U, D) ~ X + Z + X * Z,       method = \"breslow\",       x = TRUE,       y = TRUE     ),     list(       formula = Surv(U, D) ~ X,       method = \"breslow\",       x = TRUE,       y = TRUE     )   ),   fitter_list = list(\"coxph\", \"coxph\"),   data = dd,   times = seq(1, 5, 0.1),   predict_fun_list = list(prob_predict.coxph, prob_predict.coxph),   values = list(X = c(0, 1)),   B = 100,   references = 0,   contrasts = \"difference\" ) #> Bootstrapping... This may take some time...  #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% print(x) #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #> Time:  1  #>   X  estimate     lower     upper #> 1 0 0.3426359 0.2685875 0.4126004 #> 2 1 0.3306066 0.2719930 0.3968803 #>  #> Time:  1.1  #>   X  estimate     lower     upper #> 1 0 0.3193937 0.2478319 0.3800825 #> 2 1 0.3140225 0.2554192 0.3785019 #>  #> Time:  1.2  #>   X  estimate     lower     upper #> 1 0 0.2910216 0.2229411 0.3537408 #> 2 1 0.2929892 0.2427978 0.3563471 #>  #> Time:  1.3  #>   X  estimate     lower     upper #> 1 0 0.2784148 0.2174233 0.3452300 #> 2 1 0.2844500 0.2347156 0.3465131 #>  #> Time:  1.4  #>   X  estimate     lower     upper #> 1 0 0.2722029 0.2119961 0.3390574 #> 2 1 0.2801398 0.2333808 0.3414379 #>  #> Time:  1.5  #>   X  estimate     lower     upper #> 1 0 0.2537637 0.1902978 0.3208143 #> 2 1 0.2670585 0.2136579 0.3222557 #>  #> Time:  1.6  #>   X  estimate     lower     upper #> 1 0 0.2473441 0.1836796 0.3202737 #> 2 1 0.2625318 0.2112083 0.3187268 #>  #> Time:  1.7  #>   X  estimate     lower     upper #> 1 0 0.2284202 0.1650471 0.2935825 #> 2 1 0.2487069 0.1926763 0.3090800 #>  #> Time:  1.8  #>   X  estimate     lower     upper #> 1 0 0.2284202 0.1650471 0.2935825 #> 2 1 0.2487069 0.1926763 0.3090800 #>  #> Time:  1.9  #>   X  estimate     lower     upper #> 1 0 0.2155678 0.1545570 0.2795913 #> 2 1 0.2385031 0.1879035 0.2981300 #>  #> Time:  2  #>   X  estimate     lower     upper #> 1 0 0.2083187 0.1540708 0.2697798 #> 2 1 0.2328579 0.1835953 0.2894313 #>  #> Time:  2.1  #>   X  estimate     lower     upper #> 1 0 0.2010004 0.1422739 0.2664994 #> 2 1 0.2271435 0.1820213 0.2827236 #>  #> Time:  2.2  #>   X  estimate     lower     upper #> 1 0 0.2010004 0.1422739 0.2664994 #> 2 1 0.2271435 0.1820213 0.2827236 #>  #> Time:  2.3  #>   X  estimate     lower     upper #> 1 0 0.1931929 0.1335466 0.2582027 #> 2 1 0.2210044 0.1767496 0.2766781 #>  #> Time:  2.4  #>   X  estimate     lower     upper #> 1 0 0.1931929 0.1335466 0.2582027 #> 2 1 0.2210044 0.1767496 0.2766781 #>  #> Time:  2.5  #>   X  estimate     lower     upper #> 1 0 0.1648695 0.1077254 0.2294345 #> 2 1 0.1993885 0.1465611 0.2613009 #>  #> Time:  2.6  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.7  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.8  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.9  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  3  #>   X  estimate      lower     upper #> 1 0 0.1455376 0.09859477 0.2099133 #> 2 1 0.1838344 0.13550757 0.2463232 #>  #> Time:  3.1  #>   X  estimate      lower     upper #> 1 0 0.1344841 0.08736479 0.1989949 #> 2 1 0.1752849 0.13259212 0.2324468 #>  #> Time:  3.2  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.3  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.6  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.7  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.8  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.9  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.1  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.2  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.3  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.6  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.7  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.8  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.9  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #>  #> Reference level:  = 0  #> Contrast:  difference  #> Time:  1  #>   Exposure    estimate       lower    upper #> 1        0  0.00000000  0.00000000 0.000000 #> 2        1 -0.01202936 -0.08167224 0.063079 #>  #> Time:  1.1  #>   Exposure     estimate       lower      upper #> 1        0  0.000000000  0.00000000 0.00000000 #> 2        1 -0.005371227 -0.07540159 0.07050428 #>  #> Time:  1.2  #>   Exposure    estimate       lower      upper #> 1        0 0.000000000  0.00000000 0.00000000 #> 2        1 0.001967654 -0.06990685 0.07668053 #>  #> Time:  1.3  #>   Exposure    estimate      lower     upper #> 1        0 0.000000000  0.0000000 0.0000000 #> 2        1 0.006035151 -0.0652183 0.0817649 #>  #> Time:  1.4  #>   Exposure    estimate       lower      upper #> 1        0 0.000000000  0.00000000 0.00000000 #> 2        1 0.007936927 -0.06470686 0.08250201 #>  #> Time:  1.5  #>   Exposure   estimate       lower    upper #> 1        0 0.00000000  0.00000000 0.000000 #> 2        1 0.01329481 -0.05696887 0.086919 #>  #> Time:  1.6  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.01518772 -0.05696887 0.08771999 #>  #> Time:  1.7  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.02028668 -0.04978553 0.09089055 #>  #> Time:  1.8  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.02028668 -0.04978553 0.09089055 #>  #> Time:  1.9  #>   Exposure   estimate       lower     upper #> 1        0 0.00000000  0.00000000 0.0000000 #> 2        1 0.02293534 -0.04767694 0.0918496 #>  #> Time:  2  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.02453919 -0.04613972 0.09298423 #>  #> Time:  2.1  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.02614307 -0.04472088 0.09315902 #>  #> Time:  2.2  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.02614307 -0.04472088 0.09315902 #>  #> Time:  2.3  #>   Exposure  estimate       lower      upper #> 1        0 0.0000000  0.00000000 0.00000000 #> 2        1 0.0278115 -0.04234635 0.09415061 #>  #> Time:  2.4  #>   Exposure  estimate       lower      upper #> 1        0 0.0000000  0.00000000 0.00000000 #> 2        1 0.0278115 -0.04234635 0.09415061 #>  #> Time:  2.5  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03451898 -0.03573643 0.09685651 #>  #> Time:  2.6  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.7  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.8  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.9  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  3  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.03829678 -0.03307009 0.09632176 #>  #> Time:  3.1  #>   Exposure  estimate       lower      upper #> 1        0 0.0000000  0.00000000 0.00000000 #> 2        1 0.0408008 -0.02680682 0.09743773 #>  #> Time:  3.2  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.3  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.4  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.5  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.6  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.7  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.8  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.9  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.1  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.2  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.3  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.4  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.5  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.6  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.7  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.8  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.9  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  5  #>   Exposure   estimate       lower      upper #> 1        0 0.00000000  0.00000000 0.00000000 #> 2        1 0.04235446 -0.02413252 0.09619845 #>  #>  plot(x)  plot(x, reference = 0, contrast = \"difference\")"},{"path":"/reference/standardize_parfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"standardize_parfrailty performs regression standardization shared frailty gamma-Weibull models, specified values exposure, sample covariate distribution. Let \\(T\\), \\(X\\), \\(Z\\) survival outcome, exposure, vector covariates, respectively. standardize_parfrailty uses fitted Cox proportional hazards model estimate standardized survival function \\(\\theta(t,x)=E\\{S(t|X=x,Z)\\}\\), \\(t\\) specific value \\(T\\), \\(x\\) specific value \\(X\\), expectation marginal distribution \\(Z\\).","code":""},{"path":"/reference/standardize_parfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"","code":"standardize_parfrailty(   formula,   data,   values,   times,   clusterid,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   references = NULL,   transforms = NULL )"},{"path":"/reference/standardize_parfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"formula formula used fit glm model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. times vector containing specific values \\(T\\) estimate standardized survival function. clusterid optional string containing name cluster identification variable data clustered. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. references vector references following format: contrasts NULL, desired reference level(s). transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=log\\{\\theta(x)\\}\\), \\(\\psi(x)=log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"/reference/standardize_parfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"object class std_surv. basically list components estimates covariance estimates res Results transformations, contrasts, references stored res_contrasts.  output contains estimates contrasts confidence intervals combinations transforms, references","code":""},{"path":"/reference/standardize_parfrailty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"standardize_parfrailty assumes shared frailty gamma-Weibull model $$\\lambda(t_{ij}|X_{ij},Z_{ij})=\\lambda(t_{ij};\\alpha,\\eta)U_iexp\\{h(X_{ij},Z_{ij};\\beta)\\}$$ fitted, parametrization descibed help section parfrailty. Integrating gamma frailty gives survival function $$S(t|X,Z)=[1+\\phi\\Lambda_0(t;\\alpha,\\eta)exp\\{h(X,Z;\\beta)\\}]^{-1/\\phi},$$ \\(\\Lambda_0(t;\\alpha,\\eta)\\) cumulative baseline hazard $$(t/\\alpha)^{\\eta}.$$ ML estimates \\((\\alpha,\\eta,\\phi,\\beta)\\) used obtain estimates survival function \\(S(t|X=x,Z)\\): $$\\hat{S}(t|X=x,Z)=[1+\\hat{\\phi}\\Lambda_0(t;\\hat{\\alpha},\\hat{\\eta})exp\\{h(X,Z;\\hat{\\beta})\\}]^{-1/\\hat{\\phi}}.$$ \\(t\\) t argument \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\)) produce estimates $$\\hat{\\theta}(t,x)=\\sum_{=1}^n \\hat{S}(t|X=x,Z_i)/n.$$ variance \\(\\hat{\\theta}(t,x)\\) obtained sandwich formula.","code":""},{"path":"/reference/standardize_parfrailty.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Standardized survival functions sometimes referred (direct) adjusted survival functions literature. standardize_coxph/standardize_parfrailty currently handle time-varying exposures covariates. standardize_coxph/standardize_parfrailty internally loops values t argument. Therefore, function usually considerably faster length(t) small. variance calculation performed standardize_coxph condition observed covariates \\(\\bar{Z}=(Z_1,...,Z_n)\\). see matters, note $$var\\{\\hat{\\theta}(t,x)\\}=E[var\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) Cox proportional hazards model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(t,x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\). variance calculation Gail Byar (1986) ignores term, thus effectively conditions \\(\\bar{Z}\\).","code":""},{"path":"/reference/standardize_parfrailty.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Chang .M., Gelman G., Pagano M. (1982). Corrected group prognostic curves summary statistics. Journal Chronic Diseases 35, 669-674. Dahlqwist E., Pawitan Y., Sjolander . (2019). Regression standardization attributable fraction estimation -within frailty models clustered survival data. Statistical Methods Medical Research 28(2), 462-485. Gail M.H. Byar D.P. (1986). Variance calculations direct adjusted survival curves, applications testing treatement effect. Biometrical Journal 28(5), 587-599. Makuch R.W. (1982). Adjusted survival curve estimation using covariates. Journal Chronic Diseases 35, 437-443.","code":""},{"path":"/reference/standardize_parfrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Arvid Sjolander","code":""},{"path":"/reference/standardize_parfrailty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"","code":"require(survival)  # simulate data set.seed(6) n <- 300 m <- 3 alpha <- 1.5 eta <- 1 phi <- 0.5 beta <- 1 id <- rep(1:n, each = m) U <- rep(rgamma(n, shape = 1 / phi, scale = phi), each = m) X <- rnorm(n * m) # reparametrize scale as in rweibull function weibull.scale <- alpha / (U * exp(beta * X))^(1 / eta) T <- rweibull(n * m, shape = eta, scale = weibull.scale)  # right censoring C <- runif(n * m, 0, 10) D <- as.numeric(T < C) T <- pmin(T, C)  # strong left-truncation L <- runif(n * m, 0, 2) incl <- T > L incl <- ave(x = incl, id, FUN = sum) == m dd <- data.frame(L, T, D, X, id) dd <- dd[incl, ]  fit.std <- standardize_parfrailty(   formula = Surv(L, T, D) ~ X,   data = dd,   values = list(X = seq(-1, 1, 0.5)),   times = 1:5,   clusterid = \"id\" ) print(fit.std) #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 1  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.799     0.0565      0.688      0.909 #> [2,]    0.719     0.0701      0.582      0.857 #> [3,]    0.619     0.0858      0.451      0.787 #> [4,]    0.503     0.1011      0.304      0.701 #> [5,]    0.378     0.1109      0.161      0.595 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 2  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.667     0.0672      0.535      0.799 #> [2,]    0.557     0.0745      0.411      0.703 #> [3,]    0.435     0.0805      0.277      0.592 #> [4,]    0.311     0.0822      0.150      0.473 #> [5,]    0.202     0.0762      0.053      0.352 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 3  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]    0.568     0.0693     0.4324      0.704 #> [2,]    0.446     0.0700     0.3092      0.584 #> [3,]    0.323     0.0686     0.1883      0.457 #> [4,]    0.212     0.0632     0.0878      0.336 #> [5,]    0.125     0.0526     0.0218      0.228 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 4  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]   0.4909     0.0682    0.35727      0.625 #> [2,]   0.3663     0.0637    0.24151      0.491 #> [3,]   0.2491     0.0576    0.13620      0.362 #> [4,]   0.1527     0.0492    0.05629      0.249 #> [5,]   0.0841     0.0380    0.00958      0.159 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 5  #>  #>      Estimate Std. Error lower 0.95 upper 0.95 #> [1,]   0.4289     0.0658     0.2998      0.558 #> [2,]   0.3061     0.0574     0.1936      0.419 #> [3,]   0.1978     0.0486     0.1026      0.293 #> [4,]   0.1150     0.0391     0.0382      0.192 #> [5,]   0.0601     0.0287     0.0039      0.116 #>  plot(fit.std)"},{"path":"/reference/stdReg2-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","title":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","text":"Contains modern tools causal inference using regression standardization. Four general classes models allowed; generalized linear models, conditional generalized estimating equation models, Cox proportional hazards models shared frailty gamma-Weibull models. Sjolander, . (2016) doi:10.1007/s10654-016-0157-3 . Also includes functionality doubly robust estimation model classes special cases.","code":""},{"path":"/reference/stdReg2-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","text":"Maintainer: Michael C Sachs sachsmc@gmail.com Authors: Arvid Sjölander Erin E Gabriel Johan Sebastian Ohlendorff","code":""},{"path":"/reference/summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizes parfrailty fit — summary.parfrailty","title":"Summarizes parfrailty fit — summary.parfrailty","text":"summary method class \"parfrailty\".","code":""},{"path":"/reference/summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizes parfrailty fit — summary.parfrailty","text":"","code":"# S3 method for parfrailty summary(   object,   ci_type = \"plain\",   ci_level = 0.95,   digits = max(3L, getOption(\"digits\") - 3L),   ... )"},{"path":"/reference/summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizes parfrailty fit — summary.parfrailty","text":"object object class \"parfrailty\". ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. ci_level desired coverage probability confidence intervals, decimal form. digits number significant digits use printing.. ... used.","code":""},{"path":[]},{"path":"/reference/summary.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summarizes parfrailty fit — summary.parfrailty","text":"Arvid Sjolander Elisabeth Dahlqwist.","code":""},{"path":"/reference/summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizes parfrailty fit — summary.parfrailty","text":"","code":"## See documentation for parfrailty"}]

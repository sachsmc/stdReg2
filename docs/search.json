[{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Affero General Public License","title":"GNU Affero General Public License","text":"Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU Affero General Public License","text":"GNU Affero General Public License free, copyleft license software kinds works, specifically designed ensure cooperation community case network server software. licenses software practical works designed take away freedom share change works. contrast, General Public Licenses intended guarantee freedom share change versions program–make sure remains free software users. speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. Developers use General Public Licenses protect rights two steps: (1) assert copyright software, (2) offer License gives legal permission copy, distribute /modify software. secondary benefit defending users’ freedom improvements made alternate versions program, receive widespread use, become available developers incorporate. Many developers free software heartened encouraged resulting cooperation. However, case software used network servers, result may fail come . GNU General Public License permits making modified version letting public access server without ever releasing source code public. GNU Affero General Public License designed specifically ensure , cases, modified source code becomes available community. requires operator network server provide source code modified version running users server. Therefore, public use modified version, publicly accessible server, gives public access source code modified version. older license, called Affero General Public License published Affero, designed accomplish similar goals. different license, version Affero GPL, Affero released new version Affero GPL permits relicensing license. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions.","title":"GNU Affero General Public License","text":"“License” refers version 3 GNU Affero General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code.","title":"GNU Affero General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions.","title":"GNU Affero General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law.","title":"GNU Affero General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies.","title":"GNU Affero General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions.","title":"GNU Affero General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: work must carry prominent notices stating modified , giving relevant date. work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms.","title":"GNU Affero General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms.","title":"GNU Affero General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: Disclaiming warranty limiting liability differently terms sections 15 16 License; Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; Limiting use publicity purposes names licensors authors material; Declining grant rights trademark law use trade names, trademarks, service marks; Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination.","title":"GNU Affero General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies.","title":"GNU Affero General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients.","title":"GNU Affero General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents.","title":"GNU Affero General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom.","title":"GNU Affero General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_13-remote-network-interaction-use-with-the-gnu-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Remote Network Interaction; Use with the GNU General Public License.","title":"GNU Affero General Public License","text":"Notwithstanding provision License, modify Program, modified version must prominently offer users interacting remotely computer network (version supports interaction) opportunity receive Corresponding Source version providing access Corresponding Source network server charge, standard customary means facilitating copying software. Corresponding Source shall include Corresponding Source work covered version 3 GNU General Public License incorporated pursuant following paragraph. Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU General Public License single combined work, convey resulting work. terms License continue apply part covered work, work combined remain governed version 3 GNU General Public License.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License.","title":"GNU Affero General Public License","text":"Free Software Foundation may publish revised /new versions GNU Affero General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU Affero General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU Affero General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU Affero General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty.","title":"GNU Affero General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability.","title":"GNU Affero General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16.","title":"GNU Affero General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://sachsmc.github.io/stdReg2/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU Affero General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. software can interact users remotely computer network, also make sure provides way users get source. example, program web application, interface display “Source” link leads users archive code. many ways offer source, different solutions better different programs; see section 13 specific requirements. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU AGPL, see https://www.gnu.org/licenses/.","code":"<one line to give the program's name and a brief idea of what it does.>     Copyright (C) <year>  <name of author>      This program is free software: you can redistribute it and/or modify     it under the terms of the GNU Affero General Public License as     published by the Free Software Foundation, either version 3 of the     License, or (at your option) any later version.      This program is distributed in the hope that it will be useful,     but WITHOUT ANY WARRANTY; without even the implied warranty of     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     GNU Affero General Public License for more details.      You should have received a copy of the GNU Affero General Public License     along with this program.  If not, see <https://www.gnu.org/licenses/>."},{"path":"https://sachsmc.github.io/stdReg2/articles/custom.html","id":"general-implementation","dir":"Articles","previous_headings":"","what":"General implementation","title":"Implementing custom and new methods for standardization","text":"stdReg2 package provides functionality implementing regression standardization arbitrary model. long user can provide function estimate model, another function produce predictions desired summary statistic given set confounders model, regression standardization can applied produce estimates causal effect. demonstrate implementing polytomous logistic regression model outcome activity nhefs data, takes three levels: (0) active, (1) moderately active, (2) inactive. Inference can done using nonparametric bootstrap percentile confidence intervals (Tibshirani Efron 1993) setting argument B number replicates, omit brevity. Note method applies point exposures; methods needed produce valid effect estimates time-varying exposures, see Robins (1986) gfoRmula package (McGrath et al. 2020). Also note nonparametric bootstrap may provide valid inference outcome model data-adaptive, e.g., based machine learning algorithms. situations alternative inference methods may required (Naimi, Mishler, Kennedy 2021; Zivich Breskin 2021). instruct standardize fit target outcome model, tell name function use fitter parameter (\"multinom\" case) provide arguments fitter function named list arguments. required arguments data, values causal effects desired, predict_function. predict function takes input object returned fitter, outputs vector predicted summary statistics row data. case predict probability inactive, third column result predict multinomial regression fit. output result given . find estimated probability inactive 9% quit smoking, 11% quit smoking. also possible fit different models level exposure, using standardize_level function, following example: provide list fitters, arguments, predict functions, one desired levels exposure.","code":"nhefs_dat <- causaldata::nhefs_complete # the target outcome model # mfit <- multinom(active ~ qsmk + sex + race + age + I(age^2) +  #               as.factor(education) + smokeintensity, data = nhefs_dat)  ## here we predict the probability of being inactive (level 3) predict_multinom <- function(...) {   predict(..., type = \"probs\")[, 3] }  std_custom <- standardize(fitter = \"multinom\",                            arguments = list(formula = active ~ qsmk + sex +                                               race + age + I(age^2) +                 as.factor(education) + smokeintensity, trace = FALSE),                 predict_fun = predict_multinom,                 data = nhefs_dat,                 values = list(qsmk = 0:1)) std_custom #> Exposure:  qsmk  #> Tables:  #>   #>   qsmk Estimate #> 1    0     0.09 #> 2    1     0.11 std_custom2 <- standardize_level(fitter_list = list(\"multinom\", \"glm\"),                           arguments = list(list(formula = active ~ qsmk + sex +                                               race + age + I(age^2) +                 as.factor(education) + smokeintensity, trace = FALSE),                 list(formula = I(active == 2) ~ qsmk + sex +                                               race + age  +                 as.factor(education) + smokeintensity, family = binomial)),                predict_fun_list = list(predict_multinom,                                         \\(...) predict.glm(..., type = \"response\")),                data = nhefs_dat,                 values = list(qsmk = 0:1)) std_custom2 #> Exposure:  qsmk  #> Tables:  #>   #>   qsmk Estimate #> 1    0   0.0894 #> 2    1   0.1117"},{"path":"https://sachsmc.github.io/stdReg2/articles/overview.html","id":"introduction-and-context","dir":"Articles","previous_headings":"","what":"Introduction and context","title":"Estimation of causal effects using stdReg2","text":"Suppose XX denotes exposure interest takes values 0 1. represent two different medical treatments, environmental exposures, economic policies, genetic variants. often use biomedical examples biostatisticians. consider setting interest quantify effect intervening XX outcome denote YY. outcome represent numeric value, presence absence condition, time two events, time cancer diagnosis death. Let Y(X=x)Y(X = x) denote potential outcome subjects population hypothetically exposed X=xX = x. quantify effect XX, must summarize distribution Y(X=x)Y(X = x) statistic. YY dichotomous, natural use p{Y(X=x)=1}p\\{Y(X = x) = 1\\}, called risk. YY continuous, mean natural summary statistic E{Y(X=x)}E\\{Y(X = x)\\}. YY continuous time--event, probability exceeding particular value tt reasonable statistic: p{Y(X=x)>t}p\\{Y(X = x) > t\\}. general, denote summary statistic choice T{Y(X=x)}T\\{Y(X = x)\\}. summary statistic can also applied conditional distributions, denote, e.g., T{Y|X=x}T\\{Y | X = x\\}. quantify effect XX, must also decide contrast measure causal effect. point contrast compare chosen summary statistic X=1X = 1 X=0X = 0 interventions. Typical choices difference T{Y(X=1)}−T{Y(X=0)}T\\{Y(X = 1)\\} - T\\{Y(X = 0)\\} ratio T{Y(X=1)}/T{Y(X=0)}T\\{Y(X = 1)\\} / T\\{Y(X = 0)\\}. may also interest quantify report summary statistics within group (T{Y(X=1)},T{Y(X=0)})(T\\{Y(X = 1)\\}, T\\{Y(X = 0)\\}). observational studies, relationship XX YY likely confounded set variables 𝐙\\boldsymbol{Z}. means values outcome YY determined least subset 𝐙\\boldsymbol{Z} values exposure XX determined subset 𝐙\\boldsymbol{Z}. Naively estimating contrast lead biased estimates causal effect.","code":""},{"path":"https://sachsmc.github.io/stdReg2/articles/overview.html","id":"regression-standardization","dir":"Articles","previous_headings":"","what":"Regression standardization","title":"Estimation of causal effects using stdReg2","text":"See Sjölander (2016) Sjölander (2018) details. Suppose covariates 𝐙\\boldsymbol{Z} sufficient confounding control. information constitutes sufficient adjustment set, see Witte Didelez (2019). given summary statistic TT, T{Y(X=x)}=E𝐙[T{Y|X=x,𝐙}], T\\{Y(X = x)\\} = E_{\\boldsymbol{Z}}[T\\{Y | X = x, \\boldsymbol{Z}\\}],   expectation taken respect population distribution 𝐙\\boldsymbol{Z}. also known g-formula adjustment formula. order estimate quantity based independent identically distributed sample (X1,Y1,𝐙1),…,(Xn,Yn,𝐙n)(X_1, Y_1, \\boldsymbol{Z}_1), \\ldots, (X_n, Y_n, \\boldsymbol{Z}_n), proceed Specifying estimating regression model YY given XX 𝐙\\boldsymbol{Z}. Use fitted model obtain estimates T{Yi|Xi=x,𝐙i}T\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\} =1,…,ni = 1, \\ldots, n. done creating copy observed dataset, replacing observed XiX_i xx individual, using fitted model get predicted values copy observed data. Denote predicted values T̂{Yi|Xi=x,𝐙i}\\hat{T}\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\}. Average empirical distribution 𝐙\\boldsymbol{Z} obtain estimate T̂{Y(X=x)}=1n∑=1nT̂{Yi|Xi=x,𝐙i}. \\hat{T}\\{Y(X = x)\\} = \\frac{1}{n}\\sum_{= 1}^n \\hat{T}\\{Y_i | X_i = x, \\boldsymbol{Z}_i\\}. One can level X=0,1X = 0, 1 compute desired contrast. assumptions 1) 𝐙\\boldsymbol{Z} sufficient confounding control, 2) regression model step 1 correctly specified, estimator consistent asymptotically normal.","code":""},{"path":"https://sachsmc.github.io/stdReg2/articles/overview.html","id":"improving-robustness-by-modeling-the-exposure","dir":"Articles","previous_headings":"","what":"Improving robustness by modeling the exposure","title":"Estimation of causal effects using stdReg2","text":"doubly-robust estimator estimator consistent given estimand one models used forming estimator correctly specified confounding. far, one model used estimator: outcome model. now introduce model exposure, see can combine . First, terminology: Misspecified model - true generating mechanism contained possible mechanisms possible selected model. Correctly Specified - model correctly specified misspecified. Correctly specified confounding - correctly specified model contains sufficient set confounders. can model P(X=1|𝐙)P(X=1|\\boldsymbol{Z}), correctly specified contains confounders, can use estimate probability individual ii received treatment Wi=XiP(Xi=1|𝐙i)+1−Xi1−P(Xi=1|𝐙i)W_i = \\frac{X_i}{P(X_i=1|\\boldsymbol{Z}_i)} + \\frac{1-X_i}{1-P(X_i=1|\\boldsymbol{Z}_i)}. Let p̂\\hat{p}_i denote estimated probability subject ii received treatment 11. consistent estimation method can used outcome exposure models. long either outcome model propensity score model correctly specified confounding, doubly robust estimator consistent ATE. package, generalized linear models, use estimator described Gabriel et al. (2024). Recently, seems general misconception combining adjusted outcome model propensity score model always gives doubly robust estimator. true – matters combine !","code":""},{"path":"https://sachsmc.github.io/stdReg2/articles/overview.html","id":"in-practice","dir":"Articles","previous_headings":"","what":"In practice","title":"Estimation of causal effects using stdReg2","text":"use regression standardization estimate average causal effect exposure (quitting smoking) variable qsmk weight gain outcome variable wt82_71 nhefs_complete dataset comes causaldata package. data collected part project National Center Health Statistics National Institute Aging collaboration agencies United States Public Health Service. designed investigate relationships clinical, nutritional, behavioral factors subsequent morbidity, mortality, hospital utilization changes risk factors, functional limitation, institutionalization. dataset includes 1566 individuals contains among others following variables: seqn: person id} wt82_71: weight gain kilograms 1971 1982 qsmk: quit smoking 1st questionnaire 1982, 1 = yes, 0 = sex: 0 = male, 1 = female race: 0 = white, 1 = black age: age years baseline education: level education 1971, 1 = 8th grade less, 2 = high school dropout, 3 = high school, 4 - dropout, 5 = college smokeintensity: number cigarettes smoked per day 1971 smokeyrs: number years smoked exercise: level physical activity 1971, 0 = much exercise, 1 = moderate exercise, 2 = little exercise active: level activity 1971, 0 = active, 1 = moderately active, 2 = inactive, 3 = missing wt71: weight kilograms 1971 ht: height centimeters 1971 assume set confounders 𝐙\\boldsymbol{Z} includes sex, race, age, education, number cigarettes smoked per year, number years smoked, level physical activity, baseline weight 1971. equivalent assuming counterfactual weight gain independent exposure conditional variables. words, assuming following directed acyclic graph:  specific forms conditional expectations required outcome assume linear regression model linear quadratic forms continuous covariates. can fit perform regression standardization estimate causal effect use standardize_glm. must specify outcome regression model formula, provide data, describe values exposure wish estimate counterfactual means, specify contrasts want, specify reference level contrasts. following command estimates model obtain group-wise estimates, difference, ratio.   output model shows estimated potential outcome means exposure level, difference ratio thereof, associated standard error estimates, confidence intervals, p-values. Inference done using sandwich method variance calculation. assumptions outcome model correctly specified contains confounders, consistent estimates causal effects interest. can also get plots effects using plot function. obtain estimates inference different contrasts tidy data frame, suitable saving data table using downstream analyses reports, provide tidy function: obtain doubly robust inference use following command. Note now specify model exposure, propensity score model. Based results, can report estimated effect smoking average weight change measured difference potential outcome means 3.46 (2.55 4.38) measured ratio potential outcome means 2.98 (2.13 3.83). Using doubly-robust estimation method, obtain similarly 3.42 (2.48 4.37) measured ratio potential outcome means 2.94 (2.10 3.79). particular, believe necessary assumptions valid, can conclude smoking causes increase weight numeric summaries indicate smoking increases weight change 11 years 3.5 pounds (additive scale) smoking increases weight change 11 years multiplicative factor 3. doubly-robust method requires specification exposure model addition specification outcome model. estimated models can accessed inspected looking fit_outcome fit_exposure elements return object: useful want inspect distribution propensity scores exposure status. important thing check practice, looking practical violations positivity assumption.","code":"nhefs_dat <- causaldata::nhefs_complete summary(nhefs_dat) #>       seqn            qsmk            death            yrdth       #>  Min.   :  233   Min.   :0.0000   Min.   :0.0000   Min.   :83.00   #>  1st Qu.:10625   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:85.00   #>  Median :20702   Median :0.0000   Median :0.0000   Median :88.00   #>  Mean   :16639   Mean   :0.2573   Mean   :0.1858   Mean   :87.64   #>  3rd Qu.:22771   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:90.00   #>  Max.   :25061   Max.   :1.0000   Max.   :1.0000   Max.   :92.00   #>                                                    NA's   :1275    #>      modth            dadth           sbp             dbp         sex     #>  Min.   : 1.000   Min.   : 1.0   Min.   : 87.0   Min.   : 47.00   0:762   #>  1st Qu.: 3.000   1st Qu.: 8.0   1st Qu.:115.0   1st Qu.: 70.00   1:804   #>  Median : 6.000   Median :16.0   Median :126.0   Median : 77.00           #>  Mean   : 6.383   Mean   :16.1   Mean   :128.6   Mean   : 77.74           #>  3rd Qu.:10.000   3rd Qu.:24.5   3rd Qu.:139.0   3rd Qu.: 85.00           #>  Max.   :12.000   Max.   :31.0   Max.   :229.0   Max.   :130.00           #>  NA's   :1271     NA's   :1271   NA's   :29      NA's   :33               #>       age        race         income         marital          school      #>  Min.   :25.00   0:1360   Min.   :11.00   Min.   :2.000   Min.   : 0.00   #>  1st Qu.:33.00   1: 206   1st Qu.:17.00   1st Qu.:2.000   1st Qu.:10.00   #>  Median :43.00            Median :19.00   Median :2.000   Median :12.00   #>  Mean   :43.66            Mean   :17.99   Mean   :2.496   Mean   :11.17   #>  3rd Qu.:53.00            3rd Qu.:20.00   3rd Qu.:2.000   3rd Qu.:12.00   #>  Max.   :74.00            Max.   :22.00   Max.   :8.000   Max.   :17.00   #>                           NA's   :59                                      #>  education       ht             wt71             wt82           wt82_71        #>  1:291     Min.   :142.9   Min.   : 39.58   Min.   : 35.38   Min.   :-41.280   #>  2:340     1st Qu.:161.8   1st Qu.: 59.53   1st Qu.: 61.69   1st Qu.: -1.478   #>  3:637     Median :168.2   Median : 69.23   Median : 72.12   Median :  2.604   #>  4:121     Mean   :168.7   Mean   : 70.83   Mean   : 73.47   Mean   :  2.638   #>  5:177     3rd Qu.:175.3   3rd Qu.: 79.80   3rd Qu.: 83.46   3rd Qu.:  6.690   #>            Max.   :198.1   Max.   :151.73   Max.   :136.53   Max.   : 48.538   #>                                                                                #>    birthplace    smokeintensity  smkintensity82_71    smokeyrs     #>  Min.   : 1.00   Min.   : 1.00   Min.   :-80.000   Min.   : 1.00   #>  1st Qu.:22.00   1st Qu.:10.00   1st Qu.:-10.000   1st Qu.:15.00   #>  Median :34.00   Median :20.00   Median : -1.000   Median :24.00   #>  Mean   :31.67   Mean   :20.53   Mean   : -4.633   Mean   :24.59   #>  3rd Qu.:42.00   3rd Qu.:30.00   3rd Qu.:  1.000   3rd Qu.:33.00   #>  Max.   :56.00   Max.   :80.00   Max.   : 50.000   Max.   :64.00   #>  NA's   :90                                                        #>      asthma            bronch              tb                hf           #>  Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   #>  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000   #>  Median :0.00000   Median :0.00000   Median :0.00000   Median :0.000000   #>  Mean   :0.04853   Mean   :0.08365   Mean   :0.01341   Mean   :0.005109   #>  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000   #>  Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000000   #>                                                                           #>       hbp         pepticulcer        colitis          hepatitis       #>  Min.   :0.000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   #>  1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   #>  Median :1.000   Median :0.0000   Median :0.00000   Median :0.00000   #>  Mean   :1.059   Mean   :0.1015   Mean   :0.03448   Mean   :0.01788   #>  3rd Qu.:2.000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   #>  Max.   :2.000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   #>                                                                       #>   chroniccough        hayfever          diabetes          polio         #>  Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000   #>  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000   #>  Median :0.00000   Median :0.00000   Median :0.0000   Median :0.00000   #>  Mean   :0.05109   Mean   :0.08621   Mean   :0.9898   Mean   :0.01405   #>  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:2.0000   3rd Qu.:0.00000   #>  Max.   :1.00000   Max.   :1.00000   Max.   :2.0000   Max.   :1.00000   #>                                                                         #>      tumor          nervousbreak       alcoholpy       alcoholfreq    #>  Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000   #>  1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:1.000   #>  Median :0.00000   Median :0.00000   Median :1.0000   Median :2.000   #>  Mean   :0.02363   Mean   :0.02746   Mean   :0.8787   Mean   :1.913   #>  3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:3.000   #>  Max.   :1.00000   Max.   :1.00000   Max.   :2.0000   Max.   :5.000   #>                                                                       #>   alcoholtype    alcoholhowmuch        pica          headache      #>  Min.   :1.000   Min.   : 1.000   Min.   :0.000   Min.   :0.0000   #>  1st Qu.:1.000   1st Qu.: 2.000   1st Qu.:0.000   1st Qu.:0.0000   #>  Median :3.000   Median : 2.000   Median :0.000   Median :1.0000   #>  Mean   :2.466   Mean   : 3.293   Mean   :0.986   Mean   :0.6328   #>  3rd Qu.:4.000   3rd Qu.: 4.000   3rd Qu.:2.000   3rd Qu.:1.0000   #>  Max.   :4.000   Max.   :48.000   Max.   :2.000   Max.   :1.0000   #>                  NA's   :397                                       #>    otherpain        weakheart         allergies           nerves      #>  Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000   #>  1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000   #>  Median :0.0000   Median :0.00000   Median :0.00000   Median :0.000   #>  Mean   :0.2433   Mean   :0.02235   Mean   :0.06322   Mean   :0.145   #>  3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000   #>  Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.000   #>                                                                       #>     lackpep            hbpmed       boweltrouble       wtloss        #>  Min.   :0.00000   Min.   :0.000   Min.   :0.000   Min.   :0.00000   #>  1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.00000   #>  Median :0.00000   Median :1.000   Median :1.000   Median :0.00000   #>  Mean   :0.05045   Mean   :1.015   Mean   :1.046   Mean   :0.02618   #>  3rd Qu.:0.00000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:0.00000   #>  Max.   :1.00000   Max.   :2.000   Max.   :2.000   Max.   :1.00000   #>                                                                      #>    infection     active  exercise  birthcontrol    pregnancies     #>  Min.   :0.000   0:702   0:300    Min.   :0.000   Min.   : 1.000   #>  1st Qu.:0.000   1:715   1:661    1st Qu.:0.000   1st Qu.: 2.000   #>  Median :0.000   2:149   2:605    Median :1.000   Median : 3.000   #>  Mean   :0.145                    Mean   :1.081   Mean   : 3.694   #>  3rd Qu.:0.000                    3rd Qu.:2.000   3rd Qu.: 5.000   #>  Max.   :1.000                    Max.   :2.000   Max.   :15.000   #>                                                   NA's   :866      #>   cholesterol      hightax82         price71         price82      #>  Min.   : 78.0   Min.   :0.0000   Min.   :1.507   Min.   :1.452   #>  1st Qu.:189.0   1st Qu.:0.0000   1st Qu.:2.037   1st Qu.:1.740   #>  Median :216.0   Median :0.0000   Median :2.168   Median :1.815   #>  Mean   :219.9   Mean   :0.1653   Mean   :2.138   Mean   :1.806   #>  3rd Qu.:245.0   3rd Qu.:0.0000   3rd Qu.:2.242   3rd Qu.:1.868   #>  Max.   :416.0   Max.   :1.0000   Max.   :2.693   Max.   :2.103   #>  NA's   :16      NA's   :90       NA's   :90      NA's   :90      #>      tax71            tax82          price71_82         tax71_82      #>  Min.   :0.5249   Min.   :0.2200   Min.   :-0.2027   Min.   :0.0360   #>  1st Qu.:0.9449   1st Qu.:0.4399   1st Qu.: 0.2010   1st Qu.:0.4610   #>  Median :1.0498   Median :0.5060   Median : 0.3360   Median :0.5440   #>  Mean   :1.0580   Mean   :0.5058   Mean   : 0.3324   Mean   :0.5522   #>  3rd Qu.:1.1548   3rd Qu.:0.5719   3rd Qu.: 0.4438   3rd Qu.:0.6220   #>  Max.   :1.5225   Max.   :0.7479   Max.   : 0.6121   Max.   :0.8844   #>  NA's   :90       NA's   :90       NA's   :90        NA's   :90       #>        id            censored     older        #>  Min.   :   1.0   Min.   :0   Min.   :0.0000   #>  1st Qu.: 414.2   1st Qu.:0   1st Qu.:0.0000   #>  Median : 824.5   Median :0   Median :0.0000   #>  Mean   : 821.0   Mean   :0   Mean   :0.2989   #>  3rd Qu.:1228.8   3rd Qu.:0   3rd Qu.:1.0000   #>  Max.   :1629.0   Max.   :0   Max.   :1.0000   #> m <- glm(wt82_71 ~ qsmk + sex + race + age + I(age^2) +          as.factor(education) + smokeintensity + I(smokeintensity^2) +          smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +           wt71 + I(wt71^2),          data = nhefs_dat) summary(m) #>  #> Call: #> glm(formula = wt82_71 ~ qsmk + sex + race + age + I(age^2) +  #>     as.factor(education) + smokeintensity + I(smokeintensity^2) +  #>     smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +  #>     wt71 + I(wt71^2), data = nhefs_dat) #>  #> Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)           -1.6586176  4.3137734  -0.384 0.700666     #> qsmk                   3.4626218  0.4384543   7.897 5.36e-15 *** #> sex1                  -1.4650496  0.4683410  -3.128 0.001792 **  #> race1                  0.5864117  0.5816949   1.008 0.313560     #> age                    0.3626624  0.1633431   2.220 0.026546 *   #> I(age^2)              -0.0061377  0.0017263  -3.555 0.000389 *** #> as.factor(education)2  0.8185263  0.6067815   1.349 0.177546     #> as.factor(education)3  0.5715004  0.5561211   1.028 0.304273     #> as.factor(education)4  1.5085173  0.8323778   1.812 0.070134 .   #> as.factor(education)5 -0.1708264  0.7413289  -0.230 0.817786     #> smokeintensity         0.0651533  0.0503115   1.295 0.195514     #> I(smokeintensity^2)   -0.0010468  0.0009373  -1.117 0.264261     #> smokeyrs               0.1333931  0.0917319   1.454 0.146104     #> I(smokeyrs^2)         -0.0018270  0.0015438  -1.183 0.236818     #> as.factor(exercise)1   0.3206824  0.5349616   0.599 0.548961     #> as.factor(exercise)2   0.3628786  0.5589557   0.649 0.516300     #> as.factor(active)1    -0.9429574  0.4100208  -2.300 0.021593 *   #> as.factor(active)2    -0.2580374  0.6847219  -0.377 0.706337     #> wt71                   0.0373642  0.0831658   0.449 0.653297     #> I(wt71^2)             -0.0009158  0.0005235  -1.749 0.080426 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for gaussian family taken to be 53.59474) #>  #>     Null deviance: 97176  on 1565  degrees of freedom #> Residual deviance: 82857  on 1546  degrees of freedom #> AIC: 10701 #>  #> Number of Fisher Scoring iterations: 2 m2 <- standardize_glm(wt82_71 ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              values = list(qsmk = c(0,1)),             contrasts = c(\"difference\", \"ratio\"),             reference = 0)  m2 #> Outcome formula: wt82_71 ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: gaussian  #> Outcome link function: identity  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.75     0.217       1.32       2.17 #> 2    1     5.21     0.420       4.39       6.03 #>  #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     0.00     0.000       0.00       0.00 #> 2    1     3.46     0.466       2.55       4.38 #>  #> Reference level:  qsmk = 0  #> Contrast:  ratio  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.00     0.000       1.00       1.00 #> 2    1     2.98     0.435       2.13       3.83  plot(m2) plot(m2, contrast = \"difference\", reference = 0) (tidy(m2) -> tidy_m2) |> print() #>   qsmk Estimate Std.Error lower.0.95 upper.0.95   contrast transform #> 1    0 1.747216 0.2174047   1.321111   2.173322       none  identity #> 2    1 5.209838 0.4199393   4.386772   6.032904       none  identity #> 3    0 0.000000 0.0000000   0.000000   0.000000 difference  identity #> 4    1 3.462622 0.4660848   2.549112   4.376131 difference  identity #> 5    0 1.000000 0.0000000   1.000000   1.000000      ratio  identity #> 6    1 2.981793 0.4349643   2.129279   3.834308      ratio  identity m2_dr <- standardize_glm_dr(formula_outcome = wt82_71 ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),                 formula_exposure = qsmk ~ sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                  smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                  wt71 + I(wt71^2),             data = nhefs_dat,              values = list(qsmk = c(0,1)),             contrast = c(\"difference\", \"ratio\"),             reference = 0)   m2_dr #> Doubly robust estimator with:  #>  #> Exposure formula: qsmk ~ sex + race + age + I(age^2) + as.factor(education) + smokeintensity +  #>     I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + as.factor(exercise) +  #>     as.factor(active) + wt71 + I(wt71^2) #> Exposure link function: logit  #> Outcome formula: wt82_71 ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: gaussian  #> Outcome link function: identity  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.76     0.218       1.34       2.19 #> 2    1     5.19     0.438       4.33       6.04 #>  #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     0.00      0.00       0.00       0.00 #> 2    1     3.42      0.48       2.48       4.37 #>  #> Reference level:  qsmk = 0  #> Contrast:  ratio  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.00     0.000        1.0       1.00 #> 2    1     2.94     0.431        2.1       3.79  (tidy(m2_dr) -> tidy_m2_dr) |> print() #>   qsmk Estimate Std.Error lower.0.95 upper.0.95   contrast transform #> 1    0 1.762981 0.2176810   1.336334   2.189628       none  identity #> 2    1 5.186772 0.4378466   4.328609   6.044936       none  identity #> 3    0 0.000000 0.0000000   0.000000   0.000000 difference  identity #> 4    1 3.423792 0.4804026   2.482220   4.365363 difference  identity #> 5    0 1.000000 0.0000000   1.000000   1.000000      ratio  identity #> 6    1 2.942047 0.4310190   2.097265   3.786829      ratio  identity m2_dr$res$fit_outcome #>  #> Call:  fit_glm(formula = formula_outcome, family = family_outcome, data = data,  #>     response = \"outcome\", weights = weights) #>  #> Coefficients: #>           (Intercept)                   qsmk                   sex1   #>            -1.931e+00              3.424e+00             -1.989e+00   #>                 race1                    age               I(age^2)   #>             4.707e-01              6.207e-01             -8.678e-03   #> as.factor(education)2  as.factor(education)3  as.factor(education)4   #>             1.935e+00              8.366e-01              2.564e+00   #> as.factor(education)5         smokeintensity    I(smokeintensity^2)   #>             8.339e-01              1.130e-01             -2.240e-03   #>              smokeyrs          I(smokeyrs^2)   as.factor(exercise)1   #>             1.034e-01             -1.473e-03              7.865e-01   #>  as.factor(exercise)2     as.factor(active)1     as.factor(active)2   #>             1.231e+00             -8.251e-01             -4.576e-01   #>                  wt71              I(wt71^2)   #>            -1.221e-01              8.616e-06   #>  #> Degrees of Freedom: 1565 Total (i.e. Null);  1546 Residual #> Null Deviance:       212700  #> Residual Deviance: 175100    AIC: 11030 m2_dr$res$fit_exposure #>  #> Call:  fit_glm(formula = formula_exposure, family = family_exposure,  #>     data = data, response = \"exposure\", weights = weights) #>  #> Coefficients: #>           (Intercept)                   sex1                  race1   #>            -2.2425191             -0.5274782             -0.8392636   #>                   age               I(age^2)  as.factor(education)2   #>             0.1212052             -0.0008246             -0.0287755   #> as.factor(education)3  as.factor(education)4  as.factor(education)5   #>             0.0864318              0.0636010              0.4759606   #>        smokeintensity    I(smokeintensity^2)               smokeyrs   #>            -0.0772704              0.0010451             -0.0735966   #>         I(smokeyrs^2)   as.factor(exercise)1   as.factor(exercise)2   #>             0.0008441              0.3548405              0.3957040   #>    as.factor(active)1     as.factor(active)2                   wt71   #>             0.0319445              0.1767840             -0.0152357   #>             I(wt71^2)   #>             0.0001352   #>  #> Degrees of Freedom: 1565 Total (i.e. Null);  1547 Residual #> Null Deviance:       1786  #> Residual Deviance: 1677  AIC: 1715 hist(m2_dr$res$fit_exposure$fitted[nhefs_dat$qsmk == 0],      xlim = c(0, 1), main = \"qsmk = 0\", xlab = \"estimated propensity\") hist(m2_dr$res$fit_exposure$fitted[nhefs_dat$qsmk == 1],      xlim = c(0, 1), main = \"qsmk = 1\", xlab = \"estimated propensity\")"},{"path":"https://sachsmc.github.io/stdReg2/articles/overview.html","id":"other-outcome-types","dir":"Articles","previous_headings":"In practice","what":"Other outcome types","title":"Estimation of causal effects using stdReg2","text":"function standardize_glm doubly-robust counterpart standardize_glm_dr support outcome variable type can used glm. includes binary, count, . Just like glm, specify model like use outcome specifying family argument standardize_glm family_outcome argument standardize_glm_dr. default \"gaussian\" corresponds linear regression. binary outcome might like use \"binomial\" corresponds logistic regression. Let us see works using binary outcome create dichotomizing weight change 0. interpret estimates terms probabilities, risk gaining weight 11 years. particular, smoking causes 13% additive increase risk gaining weight, 1.2 fold multiplicative increase risk gaining weight. binary outcomes, can use transformations get different contrasts, also improve performance ratio contrast. using log transformation combined difference contrast, estimates become log risk ratios. can exponential get risk ratios, often better construct confidence intervals ratios log scale back transform. estimates reported table log risk ratios, now back transform exponentiating compare ratios estimated previously. case, estimates inference using transformation nearly identical without transformation. available transformations include odds, logit (log odds). can used estimate causal odds ratios, untransformed transformed, respectively. case estimate identical, confidence interval slightly different symmetric odds ratio scale first case, symmetric log odds ratio scale transformed case.","code":"nhefs_dat$gained_weight <- 1.0 * (nhefs_dat$wt82_71 > 0) m3 <- standardize_glm(gained_weight ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              family = \"binomial\",             values = list(qsmk = c(0,1)),             contrasts = c(\"difference\", \"ratio\"),             reference = 0)  m3 #> Outcome formula: gained_weight ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.639    0.0140      0.611      0.666 #> 2    1    0.772    0.0195      0.733      0.810 #>  #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.000    0.0000     0.0000      0.000 #> 2    1    0.133    0.0236     0.0866      0.179 #>  #> Reference level:  qsmk = 0  #> Contrast:  ratio  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.00    0.0000       1.00       1.00 #> 2    1     1.21    0.0398       1.13       1.29 m3_log <- standardize_glm(gained_weight ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              family = \"binomial\",             values = list(qsmk = c(0,1)),             contrasts = c(\"difference\"),             transforms = c(\"log\"),             reference = 0) m3_log #> Outcome formula: gained_weight ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  qsmk  #>  #> Tables:  #> Transform:   #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0   -0.448    0.0220     -0.492     -0.405 #> 2    1   -0.259    0.0253     -0.309     -0.210 #>  #> Transform:   #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.000    0.0000      0.000      0.000 #> 2    1    0.189    0.0329      0.125      0.254 tm3_log <- tidy(m3_log) tm3_log$rr <- exp(tm3_log$Estimate) tm3_log$rr.lower <- exp(tm3_log$lower.0.95) tm3_log$rr.upper <- exp(tm3_log$upper.0.95)  subset(tm3_log, contrast == \"difference\" & qsmk == 1) #>   qsmk  Estimate  Std.Error lower.0.95 upper.0.95   contrast transform       rr #> 4    1 0.1890909 0.03292713  0.1245549  0.2536269 difference       log 1.208151 #>   rr.lower rr.upper #> 4 1.132644 1.288691 m3 #> Outcome formula: gained_weight ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  qsmk  #>  #> Tables:  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.639    0.0140      0.611      0.666 #> 2    1    0.772    0.0195      0.733      0.810 #>  #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.000    0.0000     0.0000      0.000 #> 2    1    0.133    0.0236     0.0866      0.179 #>  #> Reference level:  qsmk = 0  #> Contrast:  ratio  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.00    0.0000       1.00       1.00 #> 2    1     1.21    0.0398       1.13       1.29 m3_odds <- standardize_glm(gained_weight ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              family = \"binomial\",             values = list(qsmk = c(0,1)),             contrasts = c(\"ratio\"),             transforms = c(\"odds\"),             reference = 0) m3_odds #> Outcome formula: gained_weight ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  qsmk  #>  #> Tables:  #> Transform:   #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.77     0.108       1.56       1.98 #> 2    1     3.38     0.374       2.64       4.11 #>  #> Transform:   #> Reference level:  qsmk = 0  #> Contrast:  ratio  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0     1.00     0.000       1.00       1.00 #> 2    1     1.91     0.238       1.44       2.38  m3_logit <- standardize_glm(gained_weight ~ qsmk + sex + race + age + I(age^2) +                 as.factor(education) + smokeintensity + I(smokeintensity^2) +                 smokeyrs + I(smokeyrs^2) + as.factor(exercise) + as.factor(active) +                wt71 + I(wt71^2),              data = nhefs_dat,              family = \"binomial\",             values = list(qsmk = c(0,1)),             contrasts = c(\"difference\"),             transforms = c(\"logit\"),             reference = 0) m3_logit #> Outcome formula: gained_weight ~ qsmk + sex + race + age + I(age^2) + as.factor(education) +  #>     smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +  #>     as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2) #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  qsmk  #>  #> Tables:  #> Transform:   #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.569    0.0609       0.45      0.689 #> 2    1    1.217    0.1108       1.00      1.434 #>  #> Transform:   #> Reference level:  qsmk = 0  #> Contrast:  difference  #>   qsmk Estimate Std.Error lower.0.95 upper.0.95 #> 1    0    0.000     0.000      0.000      0.000 #> 2    1    0.648     0.124      0.404      0.892  m3_logOR <- tidy(m3_logit) |>    subset(contrast == \"difference\" & qsmk == 1)     sprintf(\"%.2f (%.2f to %.2f)\", exp(m3_logOR$Estimate),          exp(m3_logOR$lower.0.95), exp(m3_logOR$upper.0.95)) #> [1] \"1.91 (1.50 to 2.44)\""},{"path":"https://sachsmc.github.io/stdReg2/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael C Sachs. Author, maintainer. Arvid Sjölander. Author. Erin E Gabriel. Author. Johan Sebastian Ohlendorff. Author. Adam Brand. Author.","code":""},{"path":"https://sachsmc.github.io/stdReg2/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sachs M, Sjölander , Gabriel E, Ohlendorff J, Brand (2025). stdReg2: Regression Standardization Causal Inference. R package version 1.0.4, https://sachsmc.github.io/stdReg2/.","code":"@Manual{,   title = {stdReg2: Regression Standardization for Causal Inference},   author = {Michael C Sachs and Arvid Sjölander and Erin E Gabriel and Johan Sebastian Ohlendorff and Adam Brand},   year = {2025},   note = {R package version 1.0.4},   url = {https://sachsmc.github.io/stdReg2/}, }"},{"path":"https://sachsmc.github.io/stdReg2/index.html","id":"stdreg2-regression-standardization-for-causal-inference","dir":"","previous_headings":"","what":"Regression Standardization for Causal Inference","title":"Regression Standardization for Causal Inference","text":"Goals: create unified interface regression standardization obtain estimates causal effects average treatment effect, relative treatment effect. easy use applied practitioners, .e., easy running glm coxph. want implement modern, theoretically grounded, doubly-robust estimators, associated variance estimators. want extensible statistical researchers, .e., possible implement new estimators get models used within interface. Robust clear documentation lots examples explanation necessary assumptions.","code":""},{"path":"https://sachsmc.github.io/stdReg2/index.html","id":"difference-between-stdreg2-and-stdreg","dir":"","previous_headings":"","what":"Difference between stdReg2 and stdReg","title":"Regression Standardization for Causal Inference","text":"stdReg2 next generation stdReg. happy using stdReg, can continue using nothing change near future. stdReg2 aim solve similar problems nicer output, available methods, possibility include new methods, mainly make maintenance updating easier.","code":""},{"path":"https://sachsmc.github.io/stdReg2/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Regression Standardization for Causal Inference","text":"stdReg2 available CRAN can installed : can install development version stdReg2 GitHub :","code":"install.packages(\"stdReg2\") # install.packages(\"remotes\") remotes::install_github(\"sachsmc/stdReg2\")"},{"path":"https://sachsmc.github.io/stdReg2/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Regression Standardization for Causal Inference","text":"basic example shows use regression standardization logistic regression model obtain estimates causal risk difference causal risk ratio:  detailed examples, see vignette “Estimation causal effects using stdReg2”.","code":"library(stdReg2)  # basic example # need to correctly specify the outcome model and no unmeasured confounders # (+ standard causal assumptions) set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm(  formula = Y ~ X * Z,  family = \"binomial\",  data = dd,  values = list(X = 0:1),  contrasts = c(\"difference\", \"ratio\"),  reference = 0 ) x #> Outcome formula: Y ~ X * Z #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X  #>  #> Tables:  #>   X Estimate Std.Error lower.0.95 upper.0.95 #> 1 0    0.519    0.0615      0.399      0.640 #> 2 1    0.391    0.0882      0.218      0.563 #>  #> Reference level:  X = 0  #> Contrast:  difference  #>   X Estimate Std.Error lower.0.95 upper.0.95 #> 1 0    0.000    0.0000      0.000    0.00000 #> 2 1   -0.129    0.0638     -0.254   -0.00353 #>  #> Reference level:  X = 0  #> Contrast:  ratio  #>   X Estimate Std.Error lower.0.95 upper.0.95 #> 1 0    1.000     0.000      1.000      1.000 #> 2 1    0.752     0.126      0.505      0.999 plot(x) tidy(x) #>   X   Estimate  Std.Error lower.0.95   upper.0.95   contrast transform #> 1 0  0.5190639 0.06149960  0.3985269  0.639600881       none  identity #> 2 1  0.3905311 0.08816362  0.2177336  0.563328623       none  identity #> 3 0  0.0000000 0.00000000  0.0000000  0.000000000 difference  identity #> 4 1 -0.1285328 0.06377604 -0.2535315 -0.003534039 difference  identity #> 5 0  1.0000000 0.00000000  1.0000000  1.000000000      ratio  identity #> 6 1  0.7523758 0.12604216  0.5053377  0.999413910      ratio  identity"},{"path":"https://sachsmc.github.io/stdReg2/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Regression Standardization for Causal Inference","text":"","code":"citation(\"stdReg2\") #> To cite package 'stdReg2' in publications use: #>  #>   Sachs M, Sjölander A, Gabriel E, Ohlendorff J, Brand A (2025). #>   _stdReg2: Regression Standardization for Causal Inference_. R package #>   version 1.0.3, <https://sachsmc.github.io/stdReg2/>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Manual{, #>     title = {stdReg2: Regression Standardization for Causal Inference}, #>     author = {Michael C Sachs and Arvid Sjölander and Erin E Gabriel and Johan Sebastian Ohlendorff and Adam Brand}, #>     year = {2025}, #>     note = {R package version 1.0.3}, #>     url = {https://sachsmc.github.io/stdReg2/}, #>   }"},{"path":"https://sachsmc.github.io/stdReg2/reference/nhefs_dat.html","id":null,"dir":"Reference","previous_headings":"","what":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"data collected part project National Center Health Statistics National Institute Aging collaboration agencies United States Public Health Service. designed investigate relationships clinical, nutritional, behavioral factors subsequent morbidity, mortality, hospital utilization changes risk factors, functional limitation, institutionalization. dataset includes 1629 individuals contains among others following variables:","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/nhefs_dat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"","code":"nhefs_dat"},{"path":"https://sachsmc.github.io/stdReg2/reference/nhefs_dat.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). — nhefs_dat","text":"data frame 1566 rows 66 columns: seqn person id wt82\\_71 weight gain kilograms 1971 1982 qsmk quit smoking 1st questionnaire 1982, 1 = yes, 0 = sex 0 = male, 1 = female race 0 = white, 1 = black age age years baseline education level education 1971, 1 = 8th grade less, 2 = high school dropout, 3 = high school, 4 = college dropout, 5 = college smokeintensity number cigarettes smoked per day 1971 smokeyrs number years smoked exercise level physical activity 1971, 0 = much exercise, 1 = moderate exercise, 2 = little exercise active level activity 1971, 0 = active, 1 = moderately active, 2 = inactive, 3 = missing information wt71 weight kilograms 1971 ht height centimeters 1971","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Fits shared frailty gamma-Weibull models — parfrailty","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"parfrailty fits shared frailty gamma-Weibull models. specifically designed work function standardize_parfrailty, performs regression standardization shared frailty gamma-Weibull models.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"","code":"parfrailty(formula, data, clusterid, init)"},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"formula object class \"formula\", format accepted coxph function. data data frame containing variables model. clusterid string containing name cluster identification variable. init optional vector initial values model parameters.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"object class \"parfrailty\" list containing: est Maximum Likelihood (ML) estimates \\(\\{\\log(\\hat{\\alpha}),\\log(\\hat{\\eta}), \\log(\\hat{\\phi}),\\hat{\\beta}\\}\\). vcov variance-covariance vector ML estimates. score matrix containing cluster-specific contributions ML score equations.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"parfrailty fits shared frailty gamma-Weibull model $$\\lambda(t_{ij}|C_{ij})=\\lambda(t_{ij};\\alpha,\\eta)U_i\\exp\\{h(C_{ij};\\beta)\\},$$ \\(t_{ij}\\) \\(C_{ij}\\) survival time covariate vector subject \\(j\\) cluster \\(\\), respectively. \\(\\lambda(t;\\alpha,\\eta)\\) Weibull baseline hazard function $$\\eta t^{\\eta-1}\\alpha^{-\\eta},$$ \\(\\eta\\) shape parameter \\(\\alpha\\) scale parameter. \\(U_i\\) unobserved frailty term cluster \\(\\), assumed gamma distribution scale = 1/shape = \\(\\phi\\). \\(h(X;\\beta)\\) regression function specified formula argument, parameterized vector \\(\\beta\\). ML estimates \\(\\{\\log(\\hat{\\alpha}),\\log(\\hat{\\eta}),\\log(\\hat{\\phi}),\\hat{\\beta}\\}\\) obtained maximizing marginal (\\(U\\)) likelihood.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"left truncation present, assumed strong left truncation.  means even truncation time may subject-specific, whole cluster unobserved least one subject cluster dies /truncation time. subjects cluster survive beyond subject-specific truncation times, whole cluster observed (Van den Berg Drepper, 2016).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"Dahlqwist E., Pawitan Y., Sjölander . (2019). Regression standardization attributable fraction estimation -within frailty models clustered survival data. Statistical Methods Medical Research 28(2), 462-485. Van den Berg G.J., Drepper B. (2016). Inference shared frailty survival models left-truncated data. Econometric Reviews, 35(6), 1075-1098.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"Arvid Sjölander Elisabeth Dahlqwist.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/parfrailty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fits shared frailty gamma-Weibull models — parfrailty","text":"","code":"require(survival) #> Loading required package: survival  # simulate data set.seed(5) n <- 200 m <- 3 alpha <- 1.5 eta <- 1 phi <- 0.5 beta <- 1 id <- rep(1:n, each = m) U <- rep(rgamma(n, shape = 1 / phi, scale = phi), each = m) X <- rnorm(n * m) # reparameterize scale as in rweibull function weibull.scale <- alpha / (U * exp(beta * X))^(1 / eta) T <- rweibull(n * m, shape = eta, scale = weibull.scale)  # right censoring C <- runif(n * m, 0, 10) D <- as.numeric(T < C) T <- pmin(T, C)  # strong left-truncation L <- runif(n * m, 0, 2) incl <- T > L incl <- ave(x = incl, id, FUN = sum) == m dd <- data.frame(L, T, D, X, id) dd <- dd[incl, ]  fit <- parfrailty(formula = Surv(L, T, D) ~ X, data = dd, clusterid = \"id\") print(fit) #> $formula #> Surv(L, T, D) ~ X #> <environment: 0x557031fff308> #>  #> $data #>              L          T D            X  id #> 4   1.78722376 3.03496785 1  0.634371955   2 #> 5   0.87593944 3.43397538 0 -0.231192902   2 #> 6   1.36879064 1.62048311 0 -1.368194204   2 #> 13  1.93077359 3.88018888 1 -0.869256130   5 #> 14  1.77243226 7.36242205 1 -1.332288341   5 #> 15  1.56056491 2.34032225 0  0.070562870   5 #> 19  1.22215738 9.84817895 0 -2.334691775   7 #> 20  1.21702364 5.95522302 0 -1.730891047   7 #> 21  1.49315787 2.32677008 1  0.825009586   7 #> 34  0.05522163 0.07623505 0  1.806258708  12 #> 35  0.46919068 5.65551975 0 -1.912519898  12 #> 36  0.60892023 0.64452417 0  0.199282075  12 #> 64  0.63330686 1.58382458 1  0.744600427  22 #> 65  1.19372559 8.92000775 0 -0.690190218  22 #> 66  1.57970257 2.76014646 1 -0.791411761  22 #> 67  1.81702192 3.16249173 1 -0.262081191  23 #> 68  0.48221653 7.21706324 0 -0.407917108  23 #> 69  0.18189881 0.28269222 1  0.201311056  23 #> 94  1.87380979 2.72313373 1 -0.454146942  32 #> 95  1.11637152 3.55521551 1  0.194874583  32 #> 96  0.24535298 3.02099790 1  0.031498073  32 #> 109 0.90027766 4.55355698 0 -1.515531972  37 #> 110 0.76864173 1.05942131 1  0.266538188  37 #> 111 0.05512638 2.86769961 1  0.291532260  37 #> 118 1.42239958 1.89974406 1 -0.394103521  40 #> 119 0.47835233 8.89930612 0 -1.510240230  40 #> 120 1.12377144 1.83634274 1  0.005989837  40 #> 127 0.69530126 8.08615804 0 -0.017375770  43 #> 128 1.82338834 1.94208484 0  0.541082912  43 #> 129 0.08598422 5.68351861 0 -0.540041112  43 #> 130 0.51413870 1.38826341 0 -0.080849396  44 #> 131 1.67780447 2.02832169 1 -0.029633958  44 #> 132 0.17971137 3.31347489 0 -0.145250484  44 #> 136 0.51457090 8.44363529 0 -1.154205453  46 #> 137 1.93562494 9.85561167 0  0.422597786  46 #> 138 1.77824194 7.14841753 0 -0.386989726  46 #> 139 1.87523751 5.23743428 0 -1.240525985  47 #> 140 1.81141152 3.71216593 0  0.456962145  47 #> 141 1.03691251 6.76273142 0 -1.410793353  47 #> 166 0.56609081 1.35006822 0  0.757933789  56 #> 167 0.33184205 0.37859500 1  0.359560263  56 #> 168 0.61275686 2.24092552 1 -0.829235675  56 #> 169 0.06145601 0.94865387 1 -0.445752122  57 #> 170 0.53187267 7.37740149 0 -0.670912047  57 #> 171 1.11085148 7.33086430 0 -0.800916902  57 #> 175 0.33909702 0.41699230 1  1.289449757  59 #> 176 1.97795961 5.06034775 1 -0.143912736  59 #> 177 0.17176757 0.67997250 0 -2.288008838  59 #> 250 0.10938821 3.22209051 0 -0.680978405  84 #> 251 0.50699914 6.26629895 0 -2.794959603  84 #> 252 1.25613561 3.98106755 0 -1.084586126  84 #> 259 1.52728552 6.91797197 0 -1.017287856  87 #> 260 1.25885725 1.91230933 1  1.014988922  87 #> 261 1.50238017 7.78931048 0 -0.651507685  87 #> 262 0.58556676 4.63435814 1  0.038215849  88 #> 263 0.96045975 2.80293191 1  0.184078667  88 #> 264 0.15020993 1.11106961 1  0.451722586  88 #> 274 0.18629962 0.90293922 0 -0.335845291  92 #> 275 1.92073994 2.92865786 1 -0.010529493  92 #> 276 0.32608739 1.05008773 1 -0.139135003  92 #> 286 0.73120809 2.17042107 1  0.126737533  96 #> 287 0.62003899 1.88088515 1 -0.485950257  96 #> 288 0.39135693 8.31694088 0 -1.003831691  96 #> 289 0.73954127 8.36045880 0  1.784203198  97 #> 290 0.70089152 1.28215405 1 -0.867144559  97 #> 291 0.54075855 8.57589681 0 -0.970574785  97 #> 298 0.27611811 0.82661051 1 -0.873557167 100 #> 299 0.15071073 0.49972060 1  0.111954035 100 #> 300 0.27641701 0.67741073 1  1.514231380 100 #> 316 0.92415468 5.11869150 0  0.082773252 106 #> 317 1.18738138 4.50767761 0 -1.585977289 106 #> 318 0.11445318 0.93428980 0 -1.529555425 106 #> 358 1.90755391 2.36375300 0 -0.628017639 120 #> 359 1.85712558 5.53994832 1  0.253333621 120 #> 360 1.67931147 2.57420883 0 -0.025188786 120 #> 394 0.71279878 1.32882429 1  0.460090646 132 #> 395 0.42105843 0.85011831 1 -0.218147958 132 #> 396 1.33021640 2.55882491 1  1.323011066 132 #> 403 1.20056311 5.05213969 0 -0.127865246 135 #> 404 1.10080783 8.38274394 0 -0.890580307 135 #> 405 0.26320789 1.16477906 1  0.896985420 135 #> 406 0.13875032 0.33482656 1  1.350929367 136 #> 407 1.88179018 6.89945541 0 -1.032052945 136 #> 408 0.57307729 1.69205188 1  0.513654664 136 #> 424 0.38881144 3.35369691 0 -0.966578842 142 #> 425 0.27245622 0.63939907 1  0.375867015 142 #> 426 1.36902177 6.59952559 0 -2.020810170 142 #> 427 0.19699762 1.46133286 1  0.244792596 143 #> 428 0.14960346 1.54345792 1  0.024357435 143 #> 429 0.43622702 6.56075123 0 -0.937410840 143 #> 445 0.57758966 4.79877953 0 -1.654511931 149 #> 446 1.86685356 8.30339929 0 -1.984728500 149 #> 447 0.47436808 6.92532206 1 -0.514325863 149 #> 448 1.77473551 3.51203199 0 -1.963281412 150 #> 449 1.85746704 4.18200700 0  0.303169082 150 #> 450 0.64277312 0.86599516 0  0.613763724 150 #> 469 0.16195196 0.44779638 1  0.354026642 157 #> 470 1.21458744 3.67393086 1 -0.764599960 157 #> 471 1.12302965 1.65112343 1 -0.509099170 157 #> 529 0.48063096 0.80881503 1  1.249536805 177 #> 530 1.28349557 3.27313504 0  0.496885618 177 #> 531 1.33671206 2.47686403 1 -0.309076753 177 #> 571 0.38581986 6.00508764 0 -0.913394120 191 #> 572 0.82877988 0.93946968 0  1.225612645 191 #> 573 1.85757261 4.85840462 0 -1.153224135 191 #> 589 0.70452328 1.60237702 0 -1.441137171 197 #> 590 0.22542309 0.72176056 1  1.638578200 197 #> 591 1.73939423 3.62888657 0 -1.923365406 197 #> 598 0.62471978 1.17165533 0 -0.453984276 200 #> 599 1.52603893 2.83751867 0 -0.097612023 200 #> 600 0.21336639 2.14748748 1 -0.337073721 200 #>  #> $clusterid #> [1] \"id\" #>  #> $ncluster #> [1] 37 #>  #> $n #> [1] 111 #>  #> $X #>                X #> 4    0.634371955 #> 5   -0.231192902 #> 6   -1.368194204 #> 13  -0.869256130 #> 14  -1.332288341 #> 15   0.070562870 #> 19  -2.334691775 #> 20  -1.730891047 #> 21   0.825009586 #> 34   1.806258708 #> 35  -1.912519898 #> 36   0.199282075 #> 64   0.744600427 #> 65  -0.690190218 #> 66  -0.791411761 #> 67  -0.262081191 #> 68  -0.407917108 #> 69   0.201311056 #> 94  -0.454146942 #> 95   0.194874583 #> 96   0.031498073 #> 109 -1.515531972 #> 110  0.266538188 #> 111  0.291532260 #> 118 -0.394103521 #> 119 -1.510240230 #> 120  0.005989837 #> 127 -0.017375770 #> 128  0.541082912 #> 129 -0.540041112 #> 130 -0.080849396 #> 131 -0.029633958 #> 132 -0.145250484 #> 136 -1.154205453 #> 137  0.422597786 #> 138 -0.386989726 #> 139 -1.240525985 #> 140  0.456962145 #> 141 -1.410793353 #> 166  0.757933789 #> 167  0.359560263 #> 168 -0.829235675 #> 169 -0.445752122 #> 170 -0.670912047 #> 171 -0.800916902 #> 175  1.289449757 #> 176 -0.143912736 #> 177 -2.288008838 #> 250 -0.680978405 #> 251 -2.794959603 #> 252 -1.084586126 #> 259 -1.017287856 #> 260  1.014988922 #> 261 -0.651507685 #> 262  0.038215849 #> 263  0.184078667 #> 264  0.451722586 #> 274 -0.335845291 #> 275 -0.010529493 #> 276 -0.139135003 #> 286  0.126737533 #> 287 -0.485950257 #> 288 -1.003831691 #> 289  1.784203198 #> 290 -0.867144559 #> 291 -0.970574785 #> 298 -0.873557167 #> 299  0.111954035 #> 300  1.514231380 #> 316  0.082773252 #> 317 -1.585977289 #> 318 -1.529555425 #> 358 -0.628017639 #> 359  0.253333621 #> 360 -0.025188786 #> 394  0.460090646 #> 395 -0.218147958 #> 396  1.323011066 #> 403 -0.127865246 #> 404 -0.890580307 #> 405  0.896985420 #> 406  1.350929367 #> 407 -1.032052945 #> 408  0.513654664 #> 424 -0.966578842 #> 425  0.375867015 #> 426 -2.020810170 #> 427  0.244792596 #> 428  0.024357435 #> 429 -0.937410840 #> 445 -1.654511931 #> 446 -1.984728500 #> 447 -0.514325863 #> 448 -1.963281412 #> 449  0.303169082 #> 450  0.613763724 #> 469  0.354026642 #> 470 -0.764599960 #> 471 -0.509099170 #> 529  1.249536805 #> 530  0.496885618 #> 531 -0.309076753 #> 571 -0.913394120 #> 572  1.225612645 #> 573 -1.153224135 #> 589 -1.441137171 #> 590  1.638578200 #> 591 -1.923365406 #> 598 -0.453984276 #> 599 -0.097612023 #> 600 -0.337073721 #>  #> $fit #> $fit$par #> [1]  0.3746027 -0.2732453 -1.0648873  1.2779566 #>  #> $fit$value #> [1] -111.5306 #>  #> $fit$counts #> function gradient  #>       45       16  #>  #> $fit$convergence #> [1] 0 #>  #> $fit$message #> NULL #>  #>  #> $est #>     log(α)     log(η)     log(ϕ)          X  #>  0.3746027 -0.2732453 -1.0648873  1.2779566  #>  #> $score #>             log(α)      log(η)      log(ϕ)           X #>  [1,]  0.013953486  0.20289224 -0.11965534  0.37373130 #>  [2,] -0.497884032  2.52001507 -0.46772296 -1.42384130 #>  [3,] -0.069007646  0.27713823 -0.23097814  0.66557460 #>  [4,]  0.137859989 -0.14641165  0.11013129  0.08105783 #>  [5,] -0.093520015  0.33405218 -0.30480286 -0.05516350 #>  [6,] -0.146292463 -1.03889812 -0.30025772  0.56123686 #>  [7,] -0.290499183  2.16086109 -0.47596378 -0.38636190 #>  [8,]  0.011099791  0.79398549 -0.21691275  0.29200148 #>  [9,] -0.535543940  1.18487038 -0.42506246  0.44181409 #> [10,]  0.472335268 -1.88851477  1.03243696  0.46707458 #> [11,]  0.095094051  0.20316653 -0.06270845  0.11113571 #> [12,]  0.608980969 -2.91764013  1.55679132 -0.03715771 #> [13,]  0.347705224 -1.22974595  0.54900599  0.16346613 #> [14,] -0.367475159  0.77234663 -0.45571856 -1.15674320 #> [15,]  0.425117806 -1.67705538  0.05311604  0.62175200 #> [16,] -0.398640488  1.06649391 -0.57673102  0.65703311 #> [17,]  0.511627932 -0.79298325  0.18290053  0.62054930 #> [18,]  0.053590571 -0.51220966 -0.03847510  1.12121813 #> [19,] -0.079267003  1.46932616 -0.33441943  0.13383099 #> [20,] -0.486755926  1.62765783 -0.44031649  0.01347658 #> [21,] -0.137988012  0.44503587 -0.27985220  0.45230331 #> [22,]  0.497364950 -4.91200778  2.74404636 -3.73290563 #> [23,] -0.495455064  1.69041058 -0.59856851 -2.12530681 #> [24,]  0.644289075 -1.65400943  0.56131700  0.08090285 #> [25,]  0.062392090  0.16838772 -0.04211771  0.04566587 #> [26,] -0.282676261  1.14737959 -0.90297310 -1.35697090 #> [27,]  0.387083643 -1.36082538  0.33269226  0.72348819 #> [28,] -0.139397789  0.18403258 -0.34557360  0.97437992 #> [29,] -0.077711350 -0.26019634 -0.16042625  0.90063440 #> [30,]  0.111303122  0.43070182 -0.18646515  0.65027345 #> [31,]  0.297062945 -0.01523354 -0.05276062  0.60089034 #> [32,]  0.258589968 -0.99229482  0.45701684 -0.06569007 #> [33,] -1.050040656  2.42648364 -0.34545708 -0.40334180 #> [34,] -0.155333955  0.63706623 -0.49576788 -0.24783674 #> [35,]  0.180210992 -0.77083636  0.34050852  0.53482735 #> [36,]  0.191151851  0.01906961  0.08256861 -0.21955355 #> [37,] -0.003373936  0.40771967 -0.14288170 -0.07758479 #>  #> $vcov #>             log(α)     log(η)      log(ϕ)          X #> log(α)  0.38182395 0.09552250 -0.02123094 0.01369009 #> log(η)  0.09552250 0.08284338  0.11211763 0.04021393 #> log(ϕ) -0.02123094 0.11211763  0.32410342 0.07514008 #> X       0.01369009 0.04021393  0.07514008 0.07210706 #>  #> $call #> parfrailty(formula = Surv(L, T, D) ~ X, data = dd, clusterid = \"id\") #>  #> attr(,\"class\") #> [1] \"parfrailty\""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots regression standardization fit — plot.std_glm","title":"Plots regression standardization fit — plot.std_glm","text":"plot method class \"std_glm\".","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots regression standardization fit — plot.std_glm","text":"","code":"# S3 method for class 'std_glm' plot(   x,   plot_ci = TRUE,   ci_type = \"plain\",   ci_level = 0.95,   transform = NULL,   contrast = NULL,   reference = NULL,   summary_fun = \"summary_std_glm\",   ... )"},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots regression standardization fit — plot.std_glm","text":"x object class \"std_glm\". plot_ci TRUE, add confidence intervals plot. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. ci_level Coverage probability confidence intervals. transform set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. left unspecified, \\(\\psi(x)=\\theta(x)\\). contrast set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL, doubly robust estimator standardized estimator used. reference contrast specified, desired reference level. summary_fun internal use . change. ... Unused.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots regression standardization fit — plot.std_glm","text":"None. Creates plot side effect","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plots regression standardization fit — plot.std_glm","text":"","code":"# see standardize_glm"},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.std_surv.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots regression standardization fit — plot.std_surv","title":"Plots regression standardization fit — plot.std_surv","text":"plot method class \"std_surv\".","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.std_surv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots regression standardization fit — plot.std_surv","text":"","code":"# S3 method for class 'std_surv' plot(   x,   plot_ci = TRUE,   ci_type = \"plain\",   ci_level = 0.95,   transform = NULL,   contrast = NULL,   reference = NULL,   legendpos = \"bottomleft\",   summary_fun = \"summary_std_coxph\",   ... )"},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.std_surv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots regression standardization fit — plot.std_surv","text":"x object class \"std_surv\". plot_ci TRUE, add confidence intervals plot. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. ci_level Coverage probability confidence intervals. transform set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. left unspecified, \\(\\psi(x)=\\theta(x)\\). contrast set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL, doubly robust estimator standardized estimator used. reference contrast specified, desired reference level. legendpos position legend; see legend. summary_fun internal use . change. ... Unused.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/plot.std_surv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots regression standardization fit — plot.std_surv","text":"None. Creates plot side effect","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Prints summary of regression standardization fit — print.std_surv","title":"Prints summary of regression standardization fit — print.std_surv","text":"Prints summary regression standardization fit","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prints summary of regression standardization fit — print.std_surv","text":"","code":"# S3 method for class 'std_surv' print(x, ...)  # S3 method for class 'std_glm' print(x, ...)  # S3 method for class 'std_custom' print(x, ...)"},{"path":"https://sachsmc.github.io/stdReg2/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prints summary of regression standardization fit — print.std_surv","text":"x object class \"std_glm\", \"std_surv\" \"std_custom\". ... unused","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prints summary of regression standardization fit — print.std_surv","text":"object printed, invisibly.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.summary.parfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for parametric frailty fits — print.summary.parfrailty","title":"Print method for parametric frailty fits — print.summary.parfrailty","text":"Print method parametric frailty fits","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.summary.parfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for parametric frailty fits — print.summary.parfrailty","text":"","code":"# S3 method for class 'summary.parfrailty' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)"},{"path":"https://sachsmc.github.io/stdReg2/reference/print.summary.parfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for parametric frailty fits — print.summary.parfrailty","text":"x object class \"parfrailty\" digits Number digits print ... used","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/print.summary.parfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for parametric frailty fits — print.summary.parfrailty","text":"object printed, invisibly","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics tidy","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/sandwich.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the sandwich variance components from a model fit — sandwich","title":"Compute the sandwich variance components from a model fit — sandwich","text":"Compute sandwich variance components model fit","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/sandwich.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the sandwich variance components from a model fit — sandwich","text":"","code":"sandwich(fit, data, weights, t, fit.detail)"},{"path":"https://sachsmc.github.io/stdReg2/reference/sandwich.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the sandwich variance components from a model fit — sandwich","text":"fit fitted model object class glm, coxph, ah, survfit data data used fit model weights Optional weights t Optional fixed time point survival objects fit.detail Cox models, result running coxph.detail model fit","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/sandwich.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the sandwich variance components from a model fit — sandwich","text":"list consisting Fisher information matrix () Score equations (U)","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":null,"dir":"Reference","previous_headings":"","what":"Get standardized estimates using the g-formula with a custom model — standardize","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Get standardized estimates using g-formula custom model","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"","code":"standardize(   fitter,   arguments,   predict_fun,   data,   values,   B = NULL,   ci_level = 0.95,   contrasts = NULL,   reference = NULL,   seed = NULL,   times = NULL,   transforms = NULL,   progressbar = TRUE )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"fitter function call fit data. arguments arguments used fitter function list. predict_fun function used predict means/probabilities new data set response level. survival data, matrix column time, row data. data data. values named list data.frame specifying variables values marginal means outcome estimated. B Number nonparametric bootstrap resamples. Default NULL (bootstrap). ci_level Coverage probability confidence intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. seed seed use nonparametric bootstrap. times use survival data. Set NULL otherwise. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\). progressbar Logical, TRUE print bootstrapping progress console","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"object class std_custom. Obtain numeric results using tidy.std_custom. list following components: res_contrast unnamed list one element requested contrasts. element list elements: B number bootstrap replicates estimates Estimated counterfactual means standard errors exposure level fit_outcome estimated regression model outcome estimates_boot list estimates, one bootstrap resample exposure_names character vector exposure variable names times vector times calculation done, relevant est_table Data.frame estimates contrast inference transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_level Confidence interval level res named list elements: B number bootstrap replicates estimates Estimated counterfactual means standard errors exposure level fit_outcome estimated regression model outcome estimates_boot list estimates, one bootstrap resample exposure_names character vector exposure variable names times vector times calculation done, relevant","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. standardize uses model estimate standardized mean \\(\\theta(x)=E\\{E(Y|X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\(Z\\). survival data, \\(Y=(T > t)\\), vector different time points times (\\(t\\)) can given, \\(T\\) uncensored survival time. Note nonparametric bootstrap may provide valid inference outcome model data-adaptive, e.g., based machine learning algorithms. situations alternative inference methods may required.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams & Wilkins. Sjölander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjölander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get standardized estimates using the g-formula with a custom model — standardize","text":"","code":"set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) prob_predict.glm <- function(...) predict.glm(..., type = \"response\")  x <- standardize(   fitter = \"glm\",   arguments = list(     formula = Y ~ X * Z,     family = \"binomial\"   ),   predict_fun = prob_predict.glm,   data = dd,   values = list(X = seq(-1, 1, 0.1)),   B = 100,   reference = 0,   contrasts = \"difference\" ) #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% x #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #>       X Estimate lower.0.95 upper.0.95 #> 1  -1.0    0.689      0.537      0.882 #> 2  -0.9    0.671      0.526      0.865 #> 3  -0.8    0.653      0.521      0.845 #> 4  -0.7    0.635      0.515      0.822 #> 5  -0.6    0.617      0.508      0.797 #> 6  -0.5    0.600      0.493      0.770 #> 7  -0.4    0.583      0.476      0.745 #> 8  -0.3    0.566      0.466      0.720 #> 9  -0.2    0.550      0.450      0.699 #> 10 -0.1    0.534      0.429      0.679 #> 11  0.0    0.519      0.409      0.658 #> 12  0.1    0.504      0.389      0.638 #> 13  0.2    0.490      0.369      0.621 #> 14  0.3    0.476      0.348      0.608 #> 15  0.4    0.462      0.322      0.595 #> 16  0.5    0.449      0.298      0.587 #> 17  0.6    0.437      0.275      0.583 #> 18  0.7    0.425      0.254      0.577 #> 19  0.8    0.413      0.235      0.571 #> 20  0.9    0.401      0.222      0.561 #> 21  1.0    0.391      0.207      0.551 #>  #> Reference level:  = 0  #> Contrast:  difference  #>       X Estimate lower.0.95 upper.0.95 #> 1  -1.0   0.1697    0.04037    0.30879 #> 2  -0.9   0.1516    0.03562    0.28505 #> 3  -0.8   0.1335    0.03104    0.25348 #> 4  -0.7   0.1156    0.02663    0.22134 #> 5  -0.6   0.0980    0.02238    0.19201 #> 6  -0.5   0.0807    0.01829    0.16211 #> 7  -0.4   0.0637    0.01435    0.13054 #> 8  -0.3   0.0472    0.01056    0.09810 #> 9  -0.2   0.0310    0.00691    0.06525 #> 10 -0.1   0.0153    0.00339    0.03243 #> 11  0.0   0.0000    0.00000    0.00000 #> 12  0.1  -0.0148   -0.03177   -0.00326 #> 13  0.2  -0.0292   -0.06286   -0.00641 #> 14  0.3  -0.0432   -0.09365   -0.00944 #> 15  0.4  -0.0566   -0.12370   -0.01237 #> 16  0.5  -0.0697   -0.15192   -0.01519 #> 17  0.6  -0.0823   -0.17663   -0.01791 #> 18  0.7  -0.0945   -0.19954   -0.02054 #> 19  0.8  -0.1062   -0.22074   -0.02308 #> 20  0.9  -0.1176   -0.24033   -0.02553 #> 21  1.0  -0.1285   -0.25891   -0.02790 #>   require(survival) prob_predict.coxph <- function(object, newdata, times) {   fit.detail <- suppressWarnings(basehaz(object))   cum.haz <- fit.detail$hazard[sapply(times, function(x) max(which(fit.detail$time <= x)))]   predX <- predict(object = object, newdata = newdata, type = \"risk\")   res <- matrix(NA, ncol = length(times), nrow = length(predX))   for (ti in seq_len(length(times))) {     res[, ti] <- exp(-predX * cum.haz[ti])   }   res } set.seed(68) n <- 500 Z <- rnorm(n) X <- rnorm(n, mean = Z) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, X, U, D) x <- standardize( fitter = \"coxph\",   arguments = list(     formula = Surv(U, D) ~ X + Z + X * Z,     method = \"breslow\",     x = TRUE,     y = TRUE   ),   predict_fun = prob_predict.coxph,   data = dd,   times = 1:5,   values = list(X = c(-1, 0, 1)),   B = 100,   reference = 0,   contrasts = \"difference\" ) #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% x #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #> Time:  1  #>    X  estimate     lower     upper #> 1 -1 0.6691328 0.6078557 0.7171407 #> 2  0 0.3632410 0.3011138 0.4060458 #> 3  1 0.2466151 0.1878811 0.2902875 #>  #> Time:  2  #>    X  estimate     lower     upper #> 1 -1 0.4676722 0.3847559 0.5519054 #> 2  0 0.2135942 0.1659279 0.2611208 #> 3  1 0.1656470 0.1168408 0.2079257 #>  #> Time:  3  #>    X  estimate      lower     upper #> 1 -1 0.3505275 0.26905828 0.4459009 #> 2  0 0.1520739 0.11902194 0.2044295 #> 3  1 0.1315275 0.09380255 0.1761028 #>  #> Time:  4  #>    X  estimate      lower     upper #> 1 -1 0.2546911 0.12386946 0.3508293 #> 2  0 0.1101689 0.06701637 0.1505347 #> 3  1 0.1069771 0.06241746 0.1430167 #>  #> Time:  5  #>    X   estimate      lower     upper #> 1 -1 0.16309819 0.07709911 0.2613375 #> 2  0 0.07480425 0.03816440 0.1127717 #> 3  1 0.08456621 0.04640860 0.1207358 #>  #>  #> Reference level:  = 0  #> Contrast:  difference  #> Time:  1  #>    X   estimate      lower       upper #> 1 -1  0.3058918  0.2515877  0.35267464 #> 2  0  0.0000000  0.0000000  0.00000000 #> 3  1 -0.1166259 -0.1534746 -0.08540127 #>  #> Time:  2  #>    X    estimate       lower       upper #> 1 -1  0.25407801  0.18502799  0.33076959 #> 2  0  0.00000000  0.00000000  0.00000000 #> 3  1 -0.04794719 -0.07480939 -0.02350433 #>  #> Time:  3  #>    X    estimate       lower       upper #> 1 -1  0.19845364  0.12283854 0.284032243 #> 2  0  0.00000000  0.00000000 0.000000000 #> 3  1 -0.02054638 -0.04719901 0.001297897 #>  #> Time:  4  #>    X     estimate       lower      upper #> 1 -1  0.144522281  0.04976681 0.22983546 #> 2  0  0.000000000  0.00000000 0.00000000 #> 3  1 -0.003191768 -0.02345454 0.01911037 #>  #> Time:  5  #>    X    estimate        lower      upper #> 1 -1 0.088293936  0.018981386 0.17857030 #> 2  0 0.000000000  0.000000000 0.00000000 #> 3  1 0.009761965 -0.009490497 0.02585953 #>  #>"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in Cox proportional hazards models — standardize_coxph","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"standardize_coxph performs regression standardization Cox proportional hazards models specified values exposure sample covariate distribution. Let \\(T\\), \\(X\\), \\(Z\\) survival outcome, exposure, vector covariates, respectively. standardize_coxph fits Cox proportional hazards model Breslow estimator baseline hazard order estimate standardized survival function \\(\\theta(t,x)=E\\{S(t|X=x,Z)\\}\\) measure = \"survival\" standardized restricted mean survival time t \\(\\theta(t, x) = E\\{\\int_0^t S(u|X = x, Z) du\\}\\) measure = \"rmean\", \\(t\\) specific value \\(T\\), \\(x\\) specific value \\(X\\), expectation marginal distribution \\(Z\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"","code":"standardize_coxph(   formula,   data,   values,   times,   measure = c(\"survival\", \"rmean\"),   clusterid,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   reference = NULL,   transforms = NULL )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"formula formula used fit model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. times vector containing specific values \\(T\\) estimate standardized survival function. measure Either \"survival\" estimate survival function times \"rmean\" restricted mean survival largest times. clusterid optional string containing name cluster identification variable data clustered. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"object class std_surv. Obtain numeric results using tidy.std_surv. list following components: res_contrast unnamed list one element requested contrasts. element list elements: call function call input list components used estimation measure Either \"survival\" \"rmean\" est Estimated counterfactual means standard errors exposure level vcov Estimated covariance matrix counterfactual means time est_table Data.frame estimates contrast inference times vector times used calculation transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_type Confidence interval type ci_level Confidence interval level res named list elements: call function call input list components used estimation measure Either \"survival\" \"rmean\" est Estimated counterfactual means standard errors exposure level vcov Estimated covariance matrix counterfactual means time","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"standardize_coxph fits Cox proportional hazards model $$\\lambda(t|X,Z)=\\lambda_0(t)\\exp\\{h(X,Z;\\beta)\\}.$$ Breslow's estimator cumulative baseline hazard \\(\\Lambda_0(t)=\\int_0^t\\lambda_0(u)du\\) used together partial likelihood estimate \\(\\beta\\) obtain estimates survival function \\(S(t|X=x,Z)\\) measure = \"survival\": $$\\hat{S}(t|X=x,Z)=\\exp[-\\hat{\\Lambda}_0(t)\\exp\\{h(X=x,Z;\\hat{\\beta})\\}].$$ \\(t\\) t argument \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\)) produce estimates $$\\hat{\\theta}(t,x)=\\sum_{=1}^n \\hat{S}(t|X=x,Z_i)/n,$$ \\(Z_i\\) value \\(Z\\) subject \\(\\), \\(=1,...,n\\).  variance \\(\\hat{\\theta}(t,x)\\) obtained sandwich formula. measure = \"rmean\", \\(\\Lambda_0(t)=\\int_0^t\\lambda_0(u)du\\) used together partial likelihood estimate \\(\\beta\\) obtain estimates restricted mean survival time t: \\(\\int_0^t S(u|X=x,Z) du\\) element times. estimation inference done using method described Chen Tsiatis 2001. Currently, can estimate difference RMST single binary exposure. Two separate Cox models fit level exposure, expected coded 0/1.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Standardized survival functions sometimes referred (direct) adjusted survival functions literature. standardize_coxph/standardize_parfrailty currently handle time-varying exposures covariates. standardize_coxph/standardize_parfrailty internally loops values t argument. Therefore, function usually considerably faster length(t) small. variance calculation performed standardize_coxph condition observed covariates \\(\\bar{Z}=(Z_1,...,Z_n)\\). see matters, note $$var\\{\\hat{\\theta}(t,x)\\}=E[var\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) Cox proportional hazards model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(t,x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\). variance calculation Gail Byar (1986) ignores term, thus effectively conditions \\(\\bar{Z}\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Chang .M., Gelman G., Pagano M. (1982). Corrected group prognostic curves summary statistics. Journal Chronic Diseases 35, 669-674. Gail M.H. Byar D.P. (1986). Variance calculations direct adjusted survival curves, applications testing treatment effect. Biometrical Journal 28(5), 587-599. Makuch R.W. (1982). Adjusted survival curve estimation using covariates. Journal Chronic Diseases 35, 437-443. Sjölander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjölander . (2018). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858. Chen, P. Y., Tsiatis, . . (2001). Causal inference difference restricted mean lifetime two groups. Biometrics, 57(4), 1030-1038.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"Arvid Sjölander, Adam Brand, Michael Sachs","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_coxph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in Cox proportional hazards models — standardize_coxph","text":"","code":"require(survival) set.seed(7) n <- 300 Z <- rnorm(n) Zbin <- rbinom(n, 1, .3) X <- rnorm(n, mean = Z) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time fact <- factor(sample(letters[1:3], n, replace = TRUE)) U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, Zbin, X, U, D, fact) fit.std.surv <- standardize_coxph(   formula = Surv(U, D) ~ X + Z + X * Z,   data = dd,   values = list(X = seq(-1, 1, 0.5)),   times = 1:5 ) print(fit.std.surv) #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:  X  #> Survival functions evaluated at t = 1  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.750    0.0347      0.682      0.818 #> X.1 -0.5    0.589    0.0457      0.500      0.679 #> X.2  0.0    0.401    0.0387      0.325      0.477 #> X.3  0.5    0.295    0.0351      0.226      0.364 #> X.4  1.0    0.245    0.0343      0.178      0.312 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:  X  #> Survival functions evaluated at t = 2  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.529    0.0538     0.4238      0.635 #> X.1 -0.5    0.324    0.0498     0.2264      0.422 #> X.2  0.0    0.203    0.0349     0.1344      0.271 #> X.3  0.5    0.164    0.0320     0.1015      0.227 #> X.4  1.0    0.151    0.0311     0.0897      0.211 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:  X  #> Survival functions evaluated at t = 3  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.378    0.0631     0.2540      0.501 #> X.1 -0.5    0.188    0.0432     0.1029      0.272 #> X.2  0.0    0.123    0.0307     0.0626      0.183 #> X.3  0.5    0.111    0.0287     0.0549      0.167 #> X.4  1.0    0.110    0.0282     0.0552      0.166 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:  X  #> Survival functions evaluated at t = 4  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0   0.2959    0.0662     0.1661      0.426 #> X.1 -0.5   0.1287    0.0392     0.0519      0.205 #> X.2  0.0   0.0902    0.0288     0.0337      0.147 #> X.3  0.5   0.0884    0.0273     0.0348      0.142 #> X.4  1.0   0.0925    0.0272     0.0393      0.146 #>  #>  #> Formula: Surv(U, D) ~ X + Z + X * Z #> Exposure:  X  #> Survival functions evaluated at t = 5  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0   0.2255    0.0657     0.0967      0.354 #> X.1 -0.5   0.0857    0.0328     0.0214      0.150 #> X.2  0.0   0.0663    0.0251     0.0170      0.116 #> X.3  0.5   0.0707    0.0247     0.0223      0.119 #> X.4  1.0   0.0781    0.0251     0.0288      0.127 #>  plot(fit.std.surv)  tidy(fit.std.surv) #>         X   Estimate  Std.Error lower.0.95 upper.0.95 time contrast transform #> X    -1.0 0.74957538 0.03473035 0.68150514  0.8176456    1     none  identity #> X.1  -0.5 0.58923837 0.04569250 0.49968272  0.6787940    1     none  identity #> X.2   0.0 0.40080421 0.03870365 0.32494645  0.4766620    1     none  identity #> X.3   0.5 0.29503631 0.03511907 0.22620419  0.3638684    1     none  identity #> X.4   1.0 0.24509832 0.03430180 0.17786802  0.3123286    1     none  identity #> X1   -1.0 0.52930785 0.05380874 0.42384466  0.6347710    2     none  identity #> X.11 -0.5 0.32403918 0.04979327 0.22644617  0.4216322    2     none  identity #> X.21  0.0 0.20279513 0.03490407 0.13438441  0.2712058    2     none  identity #> X.31  0.5 0.16421470 0.03202055 0.10145557  0.2269738    2     none  identity #> X.41  1.0 0.15059296 0.03105317 0.08972986  0.2114561    2     none  identity #> X2   -1.0 0.37756823 0.06305952 0.25397385  0.5011626    3     none  identity #> X.12 -0.5 0.18758405 0.04321095 0.10289215  0.2722760    3     none  identity #> X.22  0.0 0.12278095 0.03071049 0.06258949  0.1829724    3     none  identity #> X.32  0.5 0.11114345 0.02869313 0.05490595  0.1673810    3     none  identity #> X.42  1.0 0.11048888 0.02821866 0.05518133  0.1657964    3     none  identity #> X3   -1.0 0.29592579 0.06624722 0.16608364  0.4257679    4     none  identity #> X.13 -0.5 0.12866612 0.03918314 0.05186858  0.2054637    4     none  identity #> X.23  0.0 0.09021194 0.02881483 0.03373592  0.1466880    4     none  identity #> X.33  0.5 0.08837564 0.02733102 0.03480782  0.1419435    4     none  identity #> X.43  1.0 0.09253934 0.02716367 0.03929953  0.1457792    4     none  identity #> X4   -1.0 0.22546407 0.06571110 0.09667268  0.3542555    5     none  identity #> X.14 -0.5 0.08572099 0.03280939 0.02141577  0.1500262    5     none  identity #> X.24  0.0 0.06626788 0.02513536 0.01700349  0.1155323    5     none  identity #> X.34  0.5 0.07069841 0.02469322 0.02230060  0.1190962    5     none  identity #> X.44  1.0 0.07810587 0.02513514 0.02884189  0.1273698    5     none  identity #>       measure #> X    survival #> X.1  survival #> X.2  survival #> X.3  survival #> X.4  survival #> X1   survival #> X.11 survival #> X.21 survival #> X.31 survival #> X.41 survival #> X2   survival #> X.12 survival #> X.22 survival #> X.32 survival #> X.42 survival #> X3   survival #> X.13 survival #> X.23 survival #> X.33 survival #> X.43 survival #> X4   survival #> X.14 survival #> X.24 survival #> X.34 survival #> X.44 survival  fit.std <- standardize_coxph(   formula = Surv(U, D) ~ X + Zbin + X * Zbin + fact,   data = dd,   values = list(Zbin = 0:1),   times = 1.5,   measure = \"rmean\",   contrast = \"difference\",   reference = 0 ) print(fit.std) #>  #> Formula: Surv(U, D) ~ X + fact #>  Fit separately by exposure  #> Exposure:  Zbin  #> Restricted mean survival evaluated at t = 1.5  #>  #>   Zbin Estimate Std.Error lower.0.95 upper 0.95 #> 1    0    0.716    0.0447      0.628      0.804 #> 2    1    0.667    0.0542      0.561      0.773 #>  #>  #> Formula: Surv(U, D) ~ X + fact #>  Fit separately by exposure  #> Exposure:  Zbin  #> Reference level:  = 0  #> Contrast:  difference  #> Restricted mean survival evaluated at t = 1.5  #>  #>   Zbin Estimate Std.Error lower.0.95 upper 0.95 #> 1    0   0.0000    0.0000      0.000     0.0000 #> 2    1  -0.0489    0.0608     -0.168     0.0703 #>  tidy(fit.std) #>   Zbin    Estimate  Std.Error lower.0.95 upper.0.95 time   contrast transform #> 1    0  0.71593627 0.04471627  0.6282940 0.80357854  1.5       none  identity #> 2    1  0.66701532 0.05419799  0.5607892 0.77324143  1.5       none  identity #> 3    0  0.00000000 0.00000000  0.0000000 0.00000000  1.5 difference  identity #> 4    1 -0.04892094 0.06084537 -0.1681757 0.07033379  1.5 difference  identity #>   measure #> 1   rmean #> 2   rmean #> 3   rmean #> 4   rmean"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in conditional generalized estimating equations — standardize_gee","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"standardize_gee performs regression standardization linear log-linear fixed effects models, specified values exposure, sample covariate distribution. Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. assumed data clustered cluster indicator \\(\\). standardize_gee uses fitted fixed effects model, cluster-specific intercept \\(a_i\\) (see details), estimate standardized mean \\(\\theta(x)=E\\{E(Y|,X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\((a_i,Z)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"","code":"standardize_gee(   formula,   link = \"identity\",   data,   values,   clusterid,   case_control = FALSE,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   reference = NULL,   transforms = NULL )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"formula formula used \"gee\" drgee package. link link function used \"gee\". data data. values named list data.frame specifying variables values marginal means outcome estimated. clusterid optional string containing name cluster identification variable data clustered. case_control Whether data comes case-control study. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"object class std_glm. Obtain numeric results data frame tidy.std_glm function. list following components: res_contrast unnamed list one element requested contrasts. element list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names est_table Data.frame estimates contrast inference transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_type Confidence interval type ci_level Confidence interval level res named list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"standardize_gee assumes fixed effects model $$\\eta\\{E(Y|,X,Z)\\}=a_i+h(X,Z;\\beta)$$ fitted. link function \\(\\eta\\) assumed identity link log link. conditional generalized estimating equation (CGEE) estimate \\(\\beta\\) used obtain estimates cluster-specific means: $$\\hat{}_i=\\sum_{j=1}^{n_i}r_{ij}/n_i,$$ $$r_{ij}=Y_{ij}-h(X_{ij},Z_{ij};\\hat{\\beta})$$ \\(\\eta\\) identity link, $$r_{ij}=Y_{ij}\\exp\\{-h(X_{ij},Z_{ij};\\hat{\\beta})\\}$$ \\(\\eta\\) log link, \\((X_{ij},Z_{ij})\\) value \\((X,Z)\\) subject \\(j\\) cluster \\(\\), \\(j=1,...,n_i\\), \\(=1,...,n\\). CGEE estimate \\(\\beta\\) estimate \\(a_i\\) used estimate mean \\(E(Y|,X=x,Z)\\): $$\\hat{E}(Y|,X=x,Z)=\\eta^{-1}\\{\\hat{}_i+h(X=x,Z;\\hat{\\beta})\\}.$$ \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\) estimated values \\(a_i\\)) produce estimates $$\\hat{\\theta}(x)=\\sum_{=1}^n \\sum_{j=1}^{n_i} \\hat{E}(Y|,X=x,Z_i)/N,$$ \\(N=\\sum_{=1}^n n_i\\). variance \\(\\hat{\\theta}(x)\\) obtained sandwich formula.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"variance calculation performed standardize_gee condition observed covariates \\(\\bar{Z}=(Z_{11},...,Z_{nn_i})\\). see matters, note $$var\\{\\hat{\\theta}(x)\\}=E[var\\{\\hat{\\theta}(x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) generalized linear model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"Goetgeluk S. Vansteelandt S. (2008). Conditional generalized estimating equations analysis clustered longitudinal data. Biometrics 64(3), 772-780. Martin R.S. (2017). Estimation average marginal effects multiplicative unobserved effects panel models. Economics Letters 160, 16-19. Sjölander . (2019). Estimation marginal causal effects presence confounding cluster. Biostatistics doi: 10.1093/biostatistics/kxz054","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"Arvid Sjölander.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_gee.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in conditional generalized estimating equations — standardize_gee","text":"","code":"require(drgee) #> Loading required package: drgee  set.seed(4) n <- 300 ni <- 2 id <- rep(1:n, each = ni) ai <- rep(rnorm(n), each = ni) Z <- rnorm(n * ni) X <- rnorm(n * ni, mean = ai + Z) Y <- rnorm(n * ni, mean = ai + X + Z + 0.1 * X^2) dd <- data.frame(id, Z, X, Y) fit.std <- standardize_gee(   formula = Y ~ X + Z + I(X^2),   link = \"identity\",   data = dd,   values = list(X = seq(-3, 3, 0.5)),   clusterid = \"id\" ) print(fit.std) #> Outcome formula: Y ~ X + Z + I(X^2) #> <environment: 0x557036382810> #> Outcome family:  #> Outcome link function:  #> Exposure:  X  #>  #> Tables:  #>       X Estimate Std.Error lower.0.95 upper.0.95 #> 1  -3.0  -2.3117    0.1895     -2.683     -1.940 #> 2  -2.5  -2.0248    0.1531     -2.325     -1.725 #> 3  -2.0  -1.6962    0.1246     -1.940     -1.452 #> 4  -1.5  -1.3258    0.1045     -1.531     -1.121 #> 5  -1.0  -0.9137    0.0930     -1.096     -0.731 #> 6  -0.5  -0.4598    0.0894     -0.635     -0.285 #> 7   0.0   0.0358    0.0920     -0.144      0.216 #> 8   0.5   0.5732    0.0991      0.379      0.767 #> 9   1.0   1.1523    0.1101      0.936      1.368 #> 10  1.5   1.7731    0.1249      1.528      2.018 #> 11  2.0   2.4357    0.1442      2.153      2.718 #> 12  2.5   3.1401    0.1686      2.810      3.471 #> 13  3.0   3.8862    0.1989      3.496      4.276 #>  plot(fit.std)"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Get regression standardized estimates from a glm — standardize_glm","title":"Get regression standardized estimates from a glm — standardize_glm","text":"Get regression standardized estimates glm","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get regression standardized estimates from a glm — standardize_glm","text":"","code":"standardize_glm(   formula,   data,   values,   clusterid,   matched_density_cases,   matched_density_controls,   matching_variable,   p_population,   case_control = FALSE,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   reference = NULL,   transforms = NULL )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get regression standardized estimates from a glm — standardize_glm","text":"formula formula used fit model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. clusterid optional string containing name cluster identification variable data clustered. matched_density_cases function matching variable. probability (density) matched variable among cases. matched_density_controls function matching variable. probability (density) matched variable among controls. matching_variable matching variable extracted data set. p_population Specifies incidence population case_control=TRUE. case_control Whether data comes case-control study. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get regression standardized estimates from a glm — standardize_glm","text":"object class std_glm. Obtain numeric results data frame tidy.std_glm function. list following components: res_contrast unnamed list one element requested contrasts. element list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names est_table Data.frame estimates contrast inference transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_type Confidence interval type ci_level Confidence interval level res named list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get regression standardized estimates from a glm — standardize_glm","text":"standardize_glm performs regression standardization generalized linear models, specified values exposure, sample covariate distribution. Let \\(Y\\), \\(X\\), \\(Z\\) outcome, exposure, vector covariates, respectively. standardize_glm uses fitted generalized linear model estimate standardized mean \\(\\theta(x)=E\\{E(Y|X=x,Z)\\}\\), \\(x\\) specific value \\(X\\), outer expectation marginal distribution \\(Z\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get regression standardized estimates from a glm — standardize_glm","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams & Wilkins. Sjölander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjölander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get regression standardized estimates from a glm — standardize_glm","text":"","code":"# basic example # needs to correctly specify the outcome model and no unmeasered confounders # (+ standard causal assunmptions) set.seed(6) n <- 100 Z <- rnorm(n) X <- cut(rnorm(n, mean = Z), breaks = c(-Inf, 0, Inf), labels = c(\"low\", \"high\")) Y <- rbinom(n, 1, prob = (1 + exp(as.numeric(X) + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm(   formula = Y ~ X * Z,   family = \"binomial\",   data = dd,   values = list(X = c(\"low\", \"high\")),   contrasts = c(\"difference\", \"ratio\"),   reference = \"low\" ) x #> Outcome formula: Y ~ X * Z #> <environment: 0x557036270958> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X  #>  #> Tables:  #>      X Estimate Std.Error lower.0.95 upper.0.95 #> 1  low    0.286    0.0556    0.17676      0.395 #> 2 high    0.198    0.1007    0.00035      0.395 #>  #> Reference level:  X = low  #> Contrast:  difference  #>      X Estimate Std.Error lower.0.95 upper.0.95 #> 1  low    0.000     0.000      0.000      0.000 #> 2 high   -0.088     0.113     -0.309      0.133 #>  #> Reference level:  X = low  #> Contrast:  ratio  #>      X Estimate Std.Error lower.0.95 upper.0.95 #> 1  low    1.000     0.000      1.000       1.00 #> 2 high    0.692     0.372     -0.037       1.42 #>  # different transformations of causal effects  # example from Sjölander (2016) with case-control data # here the matching variable needs to be passed as an argument singapore <- AF::singapore #> Registered S3 methods overwritten by 'stdReg': #>   method                   from    #>   summary.parfrailty       stdReg2 #>   print.summary.parfrailty stdReg2 Mi <- singapore$Age m <- mean(Mi) s <- sd(Mi) d <- 5 standardize_glm(   formula = Oesophagealcancer ~ (Everhotbev + Age + Dial + Samsu + Cigs)^2,   family = binomial, data = singapore,   values = list(Everhotbev = 0:1), clusterid = \"Set\",   case_control = TRUE,   matched_density_cases = function(x) dnorm(x, m, s),   matched_density_controls = function(x) dnorm(x, m - d, s),   matching_variable = Mi,   p_population = 19.3 / 100000 ) #> Warning: case_control = TRUE may not give reasonable results for the variance with clustering #> Outcome formula: Oesophagealcancer ~ (Everhotbev + Age + Dial + Samsu + Cigs)^2 #> <environment: 0x557036270958> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  Everhotbev  #>  #> Tables:  #>   Everhotbev Estimate Std.Error lower.0.95 upper.0.95 #> 1          0 0.000128  1.69e-05   9.48e-05   0.000161 #> 2          1 0.000570  2.19e-04   1.41e-04   0.000999 #>   # multiple exposures set.seed(7) n <- 100 Z <- rnorm(n) X1 <- rnorm(n, mean = Z) X2 <- rnorm(n) Y <- rbinom(n, 1, prob = (1 + exp(X1 + X2 + Z))^(-1)) dd <- data.frame(Z, X1, X2, Y) x <- standardize_glm(   formula = Y ~ X1 + X2 + Z,   family = \"binomial\",   data = dd, values = list(X1 = 0:1, X2 = 0:1),   contrasts = c(\"difference\", \"ratio\"),   reference = c(X1 = 0, X2 = 0) ) x #> Outcome formula: Y ~ X1 + X2 + Z #> <environment: 0x557036270958> #> Outcome family: quasibinomial  #> Outcome link function: logit  #> Exposure:  X1, X2  #>  #> Tables:  #>   X1 X2 Estimate Std.Error lower.0.95 upper.0.95 #> 1  0  0    0.419    0.0576     0.3058      0.532 #> 2  1  0    0.252    0.0740     0.1074      0.397 #> 3  0  1    0.273    0.0690     0.1382      0.409 #> 4  1  1    0.146    0.0631     0.0221      0.269 #>  #> Reference level:  X1 = 0; X2 = 0  #> Contrast:  difference  #>   X1 X2 Estimate Std.Error lower.0.95 upper.0.95 #> 1  0  0    0.000    0.0000      0.000     0.0000 #> 2  1  0   -0.166    0.0590     -0.282    -0.0509 #> 3  0  1   -0.145    0.0458     -0.235    -0.0557 #> 4  1  1   -0.273    0.0581     -0.387    -0.1593 #>  #> Reference level:  X1 = 0; X2 = 0  #> Contrast:  ratio  #>   X1 X2 Estimate Std.Error lower.0.95 upper.0.95 #> 1  0  0    1.000     0.000      1.000      1.000 #> 2  1  0    0.603     0.141      0.327      0.878 #> 3  0  1    0.653     0.114      0.430      0.876 #> 4  1  1    0.348     0.131      0.091      0.605 #>  tidy(x) #>    X1 X2   Estimate  Std.Error  lower.0.95  upper.0.95   contrast transform #> 1   0  0  0.4187976 0.05763610  0.30583295  0.53176230       none  identity #> 2   1  0  0.2523918 0.07395458  0.10744346  0.39734008       none  identity #> 3   0  1  0.2733894 0.06899362  0.13816443  0.40861445       none  identity #> 4   1  1  0.1456809 0.06306346  0.02207877  0.26928300       none  identity #> 5   0  0  0.0000000 0.00000000  0.00000000  0.00000000 difference  identity #> 6   1  0 -0.1664059 0.05895745 -0.28196033 -0.05085138 difference  identity #> 7   0  1 -0.1454082 0.04579033 -0.23515558 -0.05566080 difference  identity #> 8   1  1 -0.2731167 0.05806770 -0.38692735 -0.15930614 difference  identity #> 9   0  0  1.0000000 0.00000000  1.00000000  1.00000000      ratio  identity #> 10  1  0  0.6026581 0.14070801  0.32687543  0.87844071      ratio  identity #> 11  0  1  0.6527961 0.11372894  0.42989144  0.87570068      ratio  identity #> 12  1  1  0.3478551 0.13106574  0.09097094  0.60473922      ratio  identity  # continuous exposure set.seed(2) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rnorm(n, mean = X + Z + 0.1 * X^2) dd <- data.frame(Z, X, Y) x <- standardize_glm(   formula = Y ~ X * Z,   family = \"gaussian\",   data = dd,   values = list(X = seq(-1, 1, 0.1)) )  # plot standardized mean as a function of x plot(x)  # plot standardized mean - standardized mean at x = 0 as a function of x plot(x, contrast = \"difference\", reference = 0)"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"Get regression standardized doubly-robust estimates glm","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"","code":"standardize_glm_dr(   formula_outcome,   formula_exposure,   data,   values,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family_outcome = \"gaussian\",   family_exposure = \"binomial\",   reference = NULL,   transforms = NULL )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"formula_outcome formula used fit glm model outcome. formula_exposure formula used fit glm model exposure. NULL, doubly robust estimator standardized estimator used. data data. values named list data.frame specifying variables values marginal means outcome estimated. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family_outcome family argument used fit glm model outcome. family_exposure family argument used fit glm model exposure. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"object class std_glm. Obtain numeric results data frame tidy.std_glm function. list following components: res_contrast unnamed list one element requested contrasts. element list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names est_table Data.frame estimates contrast inference transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_type Confidence interval type ci_level Confidence interval level res named list elements: estimates Estimated counterfactual means standard errors exposure level covariance Estimated covariance matrix counterfactual means fit_outcome estimated regression model outcome fit_exposure estimated exposure model exposure_names character vector exposure variable names","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"standardize_glm_dr performs regression standardization generalized linear models, see e.g., documentation standardize_glm_dr. Specifically, version uses doubly robust estimator standardization, meaning inference valid either outcome regression exposure model correctly specified unmeasured confounding.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"Gabriel E.E., Sachs, M.C., Martinussen T., Waernbaum ., Goetghebeur E., Vansteelandt S., Sjölander . (2024), Inverse probability treatment weighting generalized linear outcome models doubly robust estimation. Statistics Medicine, 43(3):534–547.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_glm_dr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get regression standardized doubly-robust estimates from a glm — standardize_glm_dr","text":"","code":"# doubly robust estimator # needs to correctly specify either the outcome model or the exposure model # for confounding # NOTE: only works with binary exposures data <- AF::clslowbwt x <- standardize_glm_dr(   formula_outcome = bwt ~ smoker * (race + age + lwt) + I(age^2) + I(lwt^2),   formula_exposure = smoker ~ race * age * lwt + I(age^2) + I(lwt^2),   family_outcome = \"gaussian\",   family_exposure = \"binomial\",   data = data,   values = list(smoker = c(0, 1)), contrasts = \"difference\", reference = 0 )  set.seed(6) n <- 100 Z <- rnorm(n) X <- rbinom(n, 1, prob = (1 + exp(Z))^(-1)) Y <- rbinom(n, 1, prob = (1 + exp(as.numeric(X) + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm_dr(   formula_outcome = Y ~ X * Z, formula_exposure = X ~ Z,   family_outcome = \"binomial\",   data = dd,   values = list(X = 0:1), reference = 0,   contrasts = c(\"difference\"), transforms = c(\"odds\") )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":null,"dir":"Reference","previous_headings":"","what":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"Get standardized estimates using g-formula separate models exposure level data","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"","code":"standardize_level(   fitter_list,   arguments,   predict_fun_list,   data,   values,   B = NULL,   ci_level = 0.95,   contrasts = NULL,   reference = NULL,   seed = NULL,   times = NULL,   transforms = NULL,   progressbar = TRUE )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"fitter_list function call fit data (list). arguments arguments used fitter function list. predict_fun_list function used predict means/probabilities new data set response level. survival data, matrix column time, row data (list). data data. values named list data.frame specifying variables values marginal means outcome estimated. B Number nonparametric bootstrap resamples. Default NULL (bootstrap). ci_level Coverage probability confidence intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. seed seed use nonparametric bootstrap. times use survival data. Set NULL otherwise. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\). progressbar Logical, TRUE print bootstrapping progress console","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"object class std_custom. Obtain numeric results using tidy.std_custom. list following components: res_contrast unnamed list one element requested contrasts. element list elements: B number bootstrap replicates estimates Estimated counterfactual means standard errors exposure level fit_outcome estimated regression model outcome estimates_boot list estimates, one bootstrap resample exposure_names character vector exposure variable names times vector times calculation done, relevant est_table Data.frame estimates contrast inference transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_level Confidence interval level res named list elements: B number bootstrap replicates estimates Estimated counterfactual means standard errors exposure level fit_outcome estimated regression model outcome estimates_boot list estimates, one bootstrap resample exposure_names character vector exposure variable names times vector times calculation done, relevant","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"See standardize. difference different models can fitted value x values.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"Rothman K.J., Greenland S., Lash T.L. (2008). Modern Epidemiology, 3rd edition. Lippincott, Williams & Wilkins. Sjölander . (2016). Regression standardization R-package stdReg. European Journal Epidemiology 31(6), 563-574. Sjölander . (2016). Estimation causal effect measures R-package stdReg. European Journal Epidemiology 33(9), 847-858.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_level.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get standardized estimates using the g-formula with and separate models for each exposure level in the data — standardize_level","text":"","code":"require(survival) prob_predict.coxph <- function(object, newdata, times) {   fit.detail <- suppressWarnings(basehaz(object))   cum.haz <- fit.detail$hazard[sapply(times, function(x) max(which(fit.detail$time <= x)))]   predX <- predict(object = object, newdata = newdata, type = \"risk\")   res <- matrix(NA, ncol = length(times), nrow = length(predX))   for (ti in seq_len(length(times))) {     res[, ti] <- exp(-predX * cum.haz[ti])   }   res }  set.seed(68) n <- 500 Z <- rnorm(n) X <- rbinom(n, 1, prob = 0.5) T <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(T, C) # time at risk D <- as.numeric(T < C) # event indicator dd <- data.frame(Z, X, U, D) x <- standardize_level(   fitter_list = list(\"coxph\", \"coxph\"),   arguments = list(     list(       formula = Surv(U, D) ~ X + Z + X * Z,       method = \"breslow\",       x = TRUE,       y = TRUE     ),     list(       formula = Surv(U, D) ~ X,       method = \"breslow\",       x = TRUE,       y = TRUE     )   ),   predict_fun_list = list(prob_predict.coxph, prob_predict.coxph),   data = dd,   times = seq(1, 5, 0.1),   values = list(X = c(0, 1)),   B = 100,   reference = 0,   contrasts = \"difference\" ) #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% print(x) #> Number of bootstraps:  100  #> Confidence intervals are based on percentile bootstrap confidence intervals  #>  #> Exposure:  X  #> Tables:  #>   #> Time:  1  #>   X  estimate     lower     upper #> 1 0 0.3426359 0.2685875 0.4126004 #> 2 1 0.3306066 0.2719930 0.3968803 #>  #> Time:  1.1  #>   X  estimate     lower     upper #> 1 0 0.3193937 0.2478319 0.3800825 #> 2 1 0.3140225 0.2554192 0.3785019 #>  #> Time:  1.2  #>   X  estimate     lower     upper #> 1 0 0.2910216 0.2229411 0.3537408 #> 2 1 0.2929892 0.2427978 0.3563471 #>  #> Time:  1.3  #>   X  estimate     lower     upper #> 1 0 0.2784148 0.2174233 0.3452300 #> 2 1 0.2844500 0.2347156 0.3465131 #>  #> Time:  1.4  #>   X  estimate     lower     upper #> 1 0 0.2722029 0.2119961 0.3390574 #> 2 1 0.2801398 0.2333808 0.3414379 #>  #> Time:  1.5  #>   X  estimate     lower     upper #> 1 0 0.2537637 0.1902978 0.3208143 #> 2 1 0.2670585 0.2136579 0.3222557 #>  #> Time:  1.6  #>   X  estimate     lower     upper #> 1 0 0.2473441 0.1836796 0.3202737 #> 2 1 0.2625318 0.2112083 0.3187268 #>  #> Time:  1.7  #>   X  estimate     lower     upper #> 1 0 0.2284202 0.1650471 0.2935825 #> 2 1 0.2487069 0.1926763 0.3090800 #>  #> Time:  1.8  #>   X  estimate     lower     upper #> 1 0 0.2284202 0.1650471 0.2935825 #> 2 1 0.2487069 0.1926763 0.3090800 #>  #> Time:  1.9  #>   X  estimate     lower     upper #> 1 0 0.2155678 0.1545570 0.2795913 #> 2 1 0.2385031 0.1879035 0.2981300 #>  #> Time:  2  #>   X  estimate     lower     upper #> 1 0 0.2083187 0.1540708 0.2697798 #> 2 1 0.2328579 0.1835953 0.2894313 #>  #> Time:  2.1  #>   X  estimate     lower     upper #> 1 0 0.2010004 0.1422739 0.2664994 #> 2 1 0.2271435 0.1820213 0.2827236 #>  #> Time:  2.2  #>   X  estimate     lower     upper #> 1 0 0.2010004 0.1422739 0.2664994 #> 2 1 0.2271435 0.1820213 0.2827236 #>  #> Time:  2.3  #>   X  estimate     lower     upper #> 1 0 0.1931929 0.1335466 0.2582027 #> 2 1 0.2210044 0.1767496 0.2766781 #>  #> Time:  2.4  #>   X  estimate     lower     upper #> 1 0 0.1931929 0.1335466 0.2582027 #> 2 1 0.2210044 0.1767496 0.2766781 #>  #> Time:  2.5  #>   X  estimate     lower     upper #> 1 0 0.1648695 0.1077254 0.2294345 #> 2 1 0.1993885 0.1465611 0.2613009 #>  #> Time:  2.6  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.7  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.8  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  2.9  #>   X  estimate     lower     upper #> 1 0 0.1557581 0.1009572 0.2232631 #> 2 1 0.1920041 0.1396859 0.2496035 #>  #> Time:  3  #>   X  estimate      lower     upper #> 1 0 0.1455376 0.09859477 0.2099133 #> 2 1 0.1838344 0.13550757 0.2463232 #>  #> Time:  3.1  #>   X  estimate      lower     upper #> 1 0 0.1344841 0.08736479 0.1989949 #> 2 1 0.1752849 0.13259212 0.2324468 #>  #> Time:  3.2  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.3  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.6  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.7  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.8  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  3.9  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.1  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.2  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.3  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.4  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.6  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.7  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.8  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  4.9  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #> Time:  5  #>   X  estimate      lower     upper #> 1 0 0.1239428 0.08002961 0.1812128 #> 2 1 0.1662973 0.12094475 0.2177145 #>  #>  #> Reference level:  = 0  #> Contrast:  difference  #> Time:  1  #>   X    estimate       lower    upper #> 1 0  0.00000000  0.00000000 0.000000 #> 2 1 -0.01202936 -0.08167224 0.063079 #>  #> Time:  1.1  #>   X     estimate       lower      upper #> 1 0  0.000000000  0.00000000 0.00000000 #> 2 1 -0.005371227 -0.07540159 0.07050428 #>  #> Time:  1.2  #>   X    estimate       lower      upper #> 1 0 0.000000000  0.00000000 0.00000000 #> 2 1 0.001967654 -0.06990685 0.07668053 #>  #> Time:  1.3  #>   X    estimate      lower     upper #> 1 0 0.000000000  0.0000000 0.0000000 #> 2 1 0.006035151 -0.0652183 0.0817649 #>  #> Time:  1.4  #>   X    estimate       lower      upper #> 1 0 0.000000000  0.00000000 0.00000000 #> 2 1 0.007936927 -0.06470686 0.08250201 #>  #> Time:  1.5  #>   X   estimate       lower    upper #> 1 0 0.00000000  0.00000000 0.000000 #> 2 1 0.01329481 -0.05696887 0.086919 #>  #> Time:  1.6  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.01518772 -0.05696887 0.08771999 #>  #> Time:  1.7  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.02028668 -0.04978553 0.09089055 #>  #> Time:  1.8  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.02028668 -0.04978553 0.09089055 #>  #> Time:  1.9  #>   X   estimate       lower     upper #> 1 0 0.00000000  0.00000000 0.0000000 #> 2 1 0.02293534 -0.04767694 0.0918496 #>  #> Time:  2  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.02453919 -0.04613972 0.09298423 #>  #> Time:  2.1  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.02614307 -0.04472088 0.09315902 #>  #> Time:  2.2  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.02614307 -0.04472088 0.09315902 #>  #> Time:  2.3  #>   X  estimate       lower      upper #> 1 0 0.0000000  0.00000000 0.00000000 #> 2 1 0.0278115 -0.04234635 0.09415061 #>  #> Time:  2.4  #>   X  estimate       lower      upper #> 1 0 0.0000000  0.00000000 0.00000000 #> 2 1 0.0278115 -0.04234635 0.09415061 #>  #> Time:  2.5  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03451898 -0.03573643 0.09685651 #>  #> Time:  2.6  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.7  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.8  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  2.9  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03624594 -0.03573643 0.09685959 #>  #> Time:  3  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.03829678 -0.03307009 0.09632176 #>  #> Time:  3.1  #>   X  estimate       lower      upper #> 1 0 0.0000000  0.00000000 0.00000000 #> 2 1 0.0408008 -0.02680682 0.09743773 #>  #> Time:  3.2  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.3  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.4  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.5  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.6  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.7  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.8  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  3.9  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.1  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.2  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.3  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.4  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.5  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.6  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.7  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.8  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  4.9  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #> Time:  5  #>   X   estimate       lower      upper #> 1 0 0.00000000  0.00000000 0.00000000 #> 2 1 0.04235446 -0.02413252 0.09619845 #>  #>"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":null,"dir":"Reference","previous_headings":"","what":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"standardize_parfrailty performs regression standardization shared frailty gamma-Weibull models, specified values exposure, sample covariate distribution. Let \\(T\\), \\(X\\), \\(Z\\) survival outcome, exposure, vector covariates, respectively. standardize_parfrailty fits parametric frailty model estimate standardized survival function \\(\\theta(t,x)=E\\{S(t|X=x,Z)\\}\\), \\(t\\) specific value \\(T\\), \\(x\\) specific value \\(X\\), expectation marginal distribution \\(Z\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"","code":"standardize_parfrailty(   formula,   data,   values,   times,   clusterid,   ci_level = 0.95,   ci_type = \"plain\",   contrasts = NULL,   family = \"gaussian\",   reference = NULL,   transforms = NULL )"},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"formula formula used fit model outcome. data data. values named list data.frame specifying variables values marginal means outcome estimated. times vector containing specific values \\(T\\) estimate standardized survival function. clusterid optional string containing name cluster identification variable data clustered. ci_level Coverage probability confidence intervals. ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. contrasts vector contrasts following format: set \"difference\" \"ratio\", \\(\\psi(x)-\\psi(x_0)\\) \\(\\psi(x) / \\psi(x_0)\\) constructed, \\(x_0\\) reference level specified reference argument. NULL references specified. family family argument used fit glm model outcome. reference vector reference levels following format: contrasts NULL, desired reference level(s). must vector list length contrasts, named, assumed order specified contrasts. transforms vector transforms following format: set \"log\", \"logit\", \"odds\", standardized mean \\(\\theta(x)\\) transformed \\(\\psi(x)=\\log\\{\\theta(x)\\}\\), \\(\\psi(x)=\\log[\\theta(x)/\\{1-\\theta(x)\\}]\\), \\(\\psi(x)=\\theta(x)/\\{1-\\theta(x)\\}\\), respectively. vector NULL, \\(\\psi(x)=\\theta(x)\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"object class std_surv. Obtain numeric results using tidy.std_surv. list following components: res_contrast unnamed list one element requested contrasts. element list elements: call function call input list components used estimation measure Either \"survival\" \"rmean\" est Estimated counterfactual means standard errors exposure level vcov Estimated covariance matrix counterfactual means time est_table Data.frame estimates contrast inference times vector times used calculation transform transform argument used contrast contrast requested contrast type reference reference level exposure ci_type Confidence interval type ci_level Confidence interval level res named list elements: call function call input list components used estimation measure Either \"survival\" \"rmean\" est Estimated counterfactual means standard errors exposure level vcov Estimated covariance matrix counterfactual means time","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"standardize_parfrailty fits shared frailty gamma-Weibull model $$\\lambda(t_{ij}|X_{ij},Z_{ij})=\\lambda(t_{ij};\\alpha,\\eta)U_iexp\\{h(X_{ij},Z_{ij};\\beta)\\}$$ , parameterization described help section parfrailty. Integrating gamma frailty gives survival function $$S(t|X,Z)=[1+\\phi\\Lambda_0(t;\\alpha,\\eta)\\exp\\{h(X,Z;\\beta)\\}]^{-1/\\phi},$$ \\(\\Lambda_0(t;\\alpha,\\eta)\\) cumulative baseline hazard $$(t/\\alpha)^{\\eta}.$$ ML estimates \\((\\alpha,\\eta,\\phi,\\beta)\\) used obtain estimates survival function \\(S(t|X=x,Z)\\): $$\\hat{S}(t|X=x,Z)=[1+\\hat{\\phi}\\Lambda_0(t;\\hat{\\alpha},\\hat{\\eta})\\exp\\{h(X,Z;\\hat{\\beta})\\}]^{-1/\\hat{\\phi}}.$$ \\(t\\) t argument \\(x\\) x argument, estimates averaged across subjects (.e. observed values \\(Z\\)) produce estimates $$\\hat{\\theta}(t,x)=\\sum_{=1}^n \\hat{S}(t|X=x,Z_i)/n.$$ variance \\(\\hat{\\theta}(t,x)\\) obtained sandwich formula.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Standardized survival functions sometimes referred (direct) adjusted survival functions literature. standardize_coxph/standardize_parfrailty currently handle time-varying exposures covariates. standardize_coxph/standardize_parfrailty internally loops values t argument. Therefore, function usually considerably faster length(t) small. variance calculation performed standardize_coxph condition observed covariates \\(\\bar{Z}=(Z_1,...,Z_n)\\). see matters, note $$var\\{\\hat{\\theta}(t,x)\\}=E[var\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]+var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}].$$ usual parameter \\(\\beta\\) Cox proportional hazards model depend \\(\\bar{Z}\\). Thus, \\(E(\\hat{\\beta}|\\bar{Z})\\) independent \\(\\bar{Z}\\) well (since \\(E(\\hat{\\beta}|\\bar{Z})=\\beta\\)), term \\(var[E\\{\\hat{\\beta}|\\bar{Z}\\}]\\) corresponding variance decomposition \\(var(\\hat{\\beta})\\) becomes equal 0. However, \\(\\theta(t,x)\\) depends \\(\\bar{Z}\\) average sample distribution \\(Z\\), thus term \\(var[E\\{\\hat{\\theta}(t,x)|\\bar{Z}\\}]\\) 0, unless one conditions \\(\\bar{Z}\\). variance calculation Gail Byar (1986) ignores term, thus effectively conditions \\(\\bar{Z}\\).","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Chang .M., Gelman G., Pagano M. (1982). Corrected group prognostic curves summary statistics. Journal Chronic Diseases 35, 669-674. Dahlqwist E., Pawitan Y., Sjölander . (2019). Regression standardization attributable fraction estimation -within frailty models clustered survival data. Statistical Methods Medical Research 28(2), 462-485. Gail M.H. Byar D.P. (1986). Variance calculations direct adjusted survival curves, applications testing treatment effect. Biometrical Journal 28(5), 587-599. Makuch R.W. (1982). Adjusted survival curve estimation using covariates. Journal Chronic Diseases 35, 437-443.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"Arvid Sjölander","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/standardize_parfrailty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Regression standardization in shared frailty gamma-Weibull models — standardize_parfrailty","text":"","code":"require(survival)  # simulate data set.seed(6) n <- 300 m <- 3 alpha <- 1.5 eta <- 1 phi <- 0.5 beta <- 1 id <- rep(1:n, each = m) U <- rep(rgamma(n, shape = 1 / phi, scale = phi), each = m) X <- rnorm(n * m) # reparameterize scale as in rweibull function weibull.scale <- alpha / (U * exp(beta * X))^(1 / eta) T <- rweibull(n * m, shape = eta, scale = weibull.scale)  # right censoring C <- runif(n * m, 0, 10) D <- as.numeric(T < C) T <- pmin(T, C)  # strong left-truncation L <- runif(n * m, 0, 2) incl <- T > L incl <- ave(x = incl, id, FUN = sum) == m dd <- data.frame(L, T, D, X, id) dd <- dd[incl, ]  fit.std <- standardize_parfrailty(   formula = Surv(L, T, D) ~ X,   data = dd,   values = list(X = seq(-1, 1, 0.5)),   times = 1:5,   clusterid = \"id\" ) print(fit.std) #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 1  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.799    0.0565      0.688      0.909 #> X.1 -0.5    0.719    0.0701      0.582      0.857 #> X.2  0.0    0.619    0.0858      0.451      0.787 #> X.3  0.5    0.503    0.1011      0.304      0.701 #> X.4  1.0    0.378    0.1109      0.161      0.595 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 2  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.667    0.0672      0.535      0.799 #> X.1 -0.5    0.557    0.0745      0.411      0.703 #> X.2  0.0    0.435    0.0805      0.277      0.592 #> X.3  0.5    0.311    0.0822      0.150      0.473 #> X.4  1.0    0.202    0.0762      0.053      0.352 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 3  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0    0.568    0.0693     0.4324      0.704 #> X.1 -0.5    0.446    0.0700     0.3092      0.584 #> X.2  0.0    0.323    0.0686     0.1883      0.457 #> X.3  0.5    0.212    0.0632     0.0878      0.336 #> X.4  1.0    0.125    0.0526     0.0218      0.228 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 4  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0   0.4909    0.0682    0.35727      0.625 #> X.1 -0.5   0.3663    0.0637    0.24151      0.491 #> X.2  0.0   0.2491    0.0576    0.13620      0.362 #> X.3  0.5   0.1527    0.0492    0.05629      0.249 #> X.4  1.0   0.0841    0.0380    0.00958      0.159 #>  #>  #> Formula: Surv(L, T, D) ~ X #> Exposure:   #> Survival functions evaluated at t = 5  #>  #>        X Estimate Std.Error lower.0.95 upper 0.95 #> X   -1.0   0.4289    0.0658     0.2998      0.558 #> X.1 -0.5   0.3061    0.0574     0.1936      0.419 #> X.2  0.0   0.1978    0.0486     0.1026      0.293 #> X.3  0.5   0.1150    0.0391     0.0382      0.192 #> X.4  1.0   0.0601    0.0287     0.0039      0.116 #>  plot(fit.std)"},{"path":"https://sachsmc.github.io/stdReg2/reference/stdReg2-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","title":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","text":"Contains modern tools causal inference using regression standardization. Four general classes models implemented; generalized linear models, conditional generalized estimating equation models, Cox proportional hazards models, shared frailty gamma-Weibull models. Methodological details described Sjölander, . (2016) doi:10.1007/s10654-016-0157-3 . Also includes functionality doubly robust estimation generalized linear models special cases, ability implement custom models.","code":""},{"path":[]},{"path":"https://sachsmc.github.io/stdReg2/reference/stdReg2-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stdReg2: Regression Standardization for Causal Inference — stdReg2-package","text":"Maintainer: Michael C Sachs sachsmc@gmail.com Authors: Arvid Sjölander Erin E Gabriel Johan Sebastian Ohlendorff Adam Brand","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarizes parfrailty fit — summary.parfrailty","title":"Summarizes parfrailty fit — summary.parfrailty","text":"summary method class \"parfrailty\".","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarizes parfrailty fit — summary.parfrailty","text":"","code":"# S3 method for class 'parfrailty' summary(   object,   ci_type = \"plain\",   ci_level = 0.95,   digits = max(3L, getOption(\"digits\") - 3L),   ... )"},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarizes parfrailty fit — summary.parfrailty","text":"object object class \"parfrailty\". ci_type string, indicating type confidence intervals. Either \"plain\", gives untransformed intervals, \"log\", gives log-transformed intervals. ci_level desired coverage probability confidence intervals, decimal form. digits number significant digits use printing.. ... used.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarizes parfrailty fit — summary.parfrailty","text":"object class \"summary.parfrailty\", list contains relevant summary statistics fitted model","code":""},{"path":[]},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Summarizes parfrailty fit — summary.parfrailty","text":"Arvid Sjölander Elisabeth Dahlqwist.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarizes parfrailty fit — summary.parfrailty","text":"","code":"## See documentation for parfrailty"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_custom.html","id":null,"dir":"Reference","previous_headings":"","what":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","title":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","text":"Tidy summarizes information components standardized regression fit.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_custom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","text":"","code":"# S3 method for class 'std_custom' tidy(x, ...)"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_custom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","text":"x object class std_custom ... currently used","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_custom.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","text":"data.frame","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_custom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Provide tidy output from a std_custom object for use in downstream computations — tidy.std_custom","text":"","code":"set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) prob_predict.glm <- function(...) predict.glm(..., type = \"response\")  x <- standardize(   fitter = \"glm\",   arguments = list(     formula = Y ~ X * Z,     family = \"binomial\"   ),   predict_fun = prob_predict.glm,   data = dd,   values = list(X = seq(-1, 1, 0.1)),   B = 100,   reference = 0,   contrasts = \"difference\" ) #>    |                                                           |                                                  |   0%   |                                                           |=                                                 |   1%   |                                                           |=                                                 |   2%   |                                                           |==                                                |   3%   |                                                           |==                                                |   4%   |                                                           |===                                               |   5%   |                                                           |===                                               |   6%   |                                                           |====                                              |   7%   |                                                           |====                                              |   8%   |                                                           |=====                                             |   9%   |                                                           |=====                                             |  10%   |                                                           |======                                            |  11%   |                                                           |======                                            |  12%   |                                                           |=======                                           |  13%   |                                                           |=======                                           |  14%   |                                                           |========                                          |  15%   |                                                           |========                                          |  16%   |                                                           |=========                                         |  17%   |                                                           |=========                                         |  18%   |                                                           |==========                                        |  19%   |                                                           |==========                                        |  20%   |                                                           |===========                                       |  21%   |                                                           |===========                                       |  22%   |                                                           |============                                      |  23%   |                                                           |============                                      |  24%   |                                                           |=============                                     |  25%   |                                                           |=============                                     |  26%   |                                                           |==============                                    |  27%   |                                                           |==============                                    |  28%   |                                                           |===============                                   |  29%   |                                                           |===============                                   |  30%   |                                                           |================                                  |  31%   |                                                           |================                                  |  32%   |                                                           |=================                                 |  33%   |                                                           |=================                                 |  34%   |                                                           |==================                                |  35%   |                                                           |==================                                |  36%   |                                                           |===================                               |  37%   |                                                           |===================                               |  38%   |                                                           |====================                              |  39%   |                                                           |====================                              |  40%   |                                                           |=====================                             |  41%   |                                                           |=====================                             |  42%   |                                                           |======================                            |  43%   |                                                           |======================                            |  44%   |                                                           |=======================                           |  45%   |                                                           |=======================                           |  46%   |                                                           |========================                          |  47%   |                                                           |========================                          |  48%   |                                                           |=========================                         |  49%   |                                                           |=========================                         |  51%   |                                                           |==========================                        |  52%   |                                                           |==========================                        |  53%   |                                                           |===========================                       |  54%   |                                                           |===========================                       |  55%   |                                                           |============================                      |  56%   |                                                           |============================                      |  57%   |                                                           |=============================                     |  58%   |                                                           |=============================                     |  59%   |                                                           |==============================                    |  60%   |                                                           |==============================                    |  61%   |                                                           |===============================                   |  62%   |                                                           |===============================                   |  63%   |                                                           |================================                  |  64%   |                                                           |================================                  |  65%   |                                                           |=================================                 |  66%   |                                                           |=================================                 |  67%   |                                                           |==================================                |  68%   |                                                           |==================================                |  69%   |                                                           |===================================               |  70%   |                                                           |===================================               |  71%   |                                                           |====================================              |  72%   |                                                           |====================================              |  73%   |                                                           |=====================================             |  74%   |                                                           |=====================================             |  75%   |                                                           |======================================            |  76%   |                                                           |======================================            |  77%   |                                                           |=======================================           |  78%   |                                                           |=======================================           |  79%   |                                                           |========================================          |  80%   |                                                           |========================================          |  81%   |                                                           |=========================================         |  82%   |                                                           |=========================================         |  83%   |                                                           |==========================================        |  84%   |                                                           |==========================================        |  85%   |                                                           |===========================================       |  86%   |                                                           |===========================================       |  87%   |                                                           |============================================      |  88%   |                                                           |============================================      |  89%   |                                                           |=============================================     |  90%   |                                                           |=============================================     |  91%   |                                                           |==============================================    |  92%   |                                                           |==============================================    |  93%   |                                                           |===============================================   |  94%   |                                                           |===============================================   |  95%   |                                                           |================================================  |  96%   |                                                           |================================================  |  97%   |                                                           |================================================= |  98%   |                                                           |================================================= |  99%   |                                                           |==================================================| 100% tidy(x) #>       X    Estimate   lower.0.95   upper.0.95   contrast transform #> 1  -1.0  0.68877411  0.536707403  0.882255075       none  identity #> 2  -0.9  0.67062009  0.525813648  0.864879358       none  identity #> 3  -0.8  0.65256341  0.520721573  0.844794061       none  identity #> 4  -0.7  0.63468885  0.514816667  0.822014143       none  identity #> 5  -0.6  0.61706694  0.508080952  0.796787359       none  identity #> 6  -0.5  0.59975468  0.492641840  0.769600133       none  identity #> 7  -0.4  0.58279684  0.476460813  0.744746574       none  identity #> 8  -0.3  0.56622750  0.466008766  0.719976812       none  identity #> 9  -0.2  0.55007164  0.450432696  0.699444328       none  identity #> 10 -0.1  0.53434668  0.429345254  0.678849732       none  identity #> 11  0.0  0.51906387  0.408810531  0.658324522       none  identity #> 12  0.1  0.50422954  0.388670917  0.638001823       none  identity #> 13  0.2  0.48984604  0.369157271  0.621476434       none  identity #> 14  0.3  0.47591268  0.347567318  0.607917623       none  identity #> 15  0.4  0.46242632  0.321901089  0.594855155       none  identity #> 16  0.5  0.44938199  0.297725989  0.587484524       none  identity #> 17  0.6  0.43677323  0.275239193  0.582974320       none  identity #> 18  0.7  0.42459251  0.253692137  0.576538798       none  identity #> 19  0.8  0.41283139  0.234773966  0.571133996       none  identity #> 20  0.9  0.40148079  0.221703266  0.560876519       none  identity #> 21  1.0  0.39053110  0.207361355  0.551041083       none  identity #> 22 -1.0  0.16971023  0.040373417  0.308788466 difference  identity #> 23 -0.9  0.15155622  0.035622795  0.285045672 difference  identity #> 24 -0.8  0.13349953  0.031043865  0.253475213 difference  identity #> 25 -0.7  0.11562498  0.026632363  0.221343098 difference  identity #> 26 -0.6  0.09800307  0.022383437  0.192009746 difference  identity #> 27 -0.5  0.08069081  0.018291855  0.162111431 difference  identity #> 28 -0.4  0.06373297  0.014352152  0.130535298 difference  identity #> 29 -0.3  0.04716363  0.010558738  0.098102367 difference  identity #> 30 -0.2  0.03100776  0.006905981  0.065247743 difference  identity #> 31 -0.1  0.01528280  0.003388258  0.032425935 difference  identity #> 32  0.0  0.00000000  0.000000000  0.000000000 difference  identity #> 33  0.1 -0.01483434 -0.031773088 -0.003264277 difference  identity #> 34  0.2 -0.02921783 -0.062855815 -0.006409950 difference  identity #> 35  0.3 -0.04315120 -0.093646417 -0.009442266 difference  identity #> 36  0.4 -0.05663755 -0.123698295 -0.012366334 difference  identity #> 37  0.5 -0.06968189 -0.151915636 -0.015187109 difference  identity #> 38  0.6 -0.08229064 -0.176633031 -0.017909383 difference  identity #> 39  0.7 -0.09447137 -0.199542682 -0.020537779 difference  identity #> 40  0.8 -0.10623249 -0.220740791 -0.023076747 difference  identity #> 41  0.9 -0.11758309 -0.240331840 -0.025530558 difference  identity #> 42  1.0 -0.12853277 -0.258910112 -0.027903299 difference  identity"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","title":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","text":"Tidy summarizes information components standardized regression fit.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","text":"","code":"# S3 method for class 'std_glm' tidy(x, ...)"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","text":"x object class std_glm ... currently used","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","text":"data.frame","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Provide tidy output from a std_glm object for use in downstream computations — tidy.std_glm","text":"","code":"set.seed(6) n <- 100 Z <- rnorm(n) X <- rnorm(n, mean = Z) Y <- rbinom(n, 1, prob = (1 + exp(X + Z))^(-1)) dd <- data.frame(Z, X, Y) x <- standardize_glm(   formula = Y ~ X * Z,   family = \"binomial\",   data = dd,   values = list(X = 0:1),   contrasts = c(\"difference\", \"ratio\"),   reference = 0 ) tidy(x) #>   X   Estimate  Std.Error lower.0.95   upper.0.95   contrast transform #> 1 0  0.5190639 0.06149960  0.3985269  0.639600881       none  identity #> 2 1  0.3905311 0.08816362  0.2177336  0.563328623       none  identity #> 3 0  0.0000000 0.00000000  0.0000000  0.000000000 difference  identity #> 4 1 -0.1285328 0.06377604 -0.2535315 -0.003534039 difference  identity #> 5 0  1.0000000 0.00000000  1.0000000  1.000000000      ratio  identity #> 6 1  0.7523758 0.12604216  0.5053377  0.999413910      ratio  identity"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_surv.html","id":null,"dir":"Reference","previous_headings":"","what":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","title":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","text":"Tidy summarizes information components standardized model fit.","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_surv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","text":"","code":"# S3 method for class 'std_surv' tidy(x, ...)"},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_surv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","text":"x object class std_surv ... currently used","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_surv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","text":"data.frame","code":""},{"path":"https://sachsmc.github.io/stdReg2/reference/tidy.std_surv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Provide tidy output from a std_surv object for use in downstream computations — tidy.std_surv","text":"","code":"require(survival) set.seed(8) n <- 300 Z <- rnorm(n) X <- rnorm(n, mean = Z) time <- rexp(n, rate = exp(X + Z + X * Z)) # survival time C <- rexp(n, rate = exp(X + Z + X * Z)) # censoring time U <- pmin(time, C) # time at risk D <- as.numeric(time < C) # event indicator dd <- data.frame(Z, X, U, D) x <- standardize_coxph(   formula = Surv(U, D) ~ X + Z + X * Z,   data = dd, values = list(X = seq(-1, 1, 0.5)), times = c(2,3,4) )  tidy(x) #>         X   Estimate  Std.Error lower.0.95 upper.0.95 time contrast transform #> X    -1.0 0.37425848 0.05636602 0.26378312  0.4847338    2     none  identity #> X.1  -0.5 0.18100390 0.04207569 0.09853707  0.2634707    2     none  identity #> X.2   0.0 0.11371977 0.02911956 0.05664648  0.1707931    2     none  identity #> X.3   0.5 0.10190382 0.02572408 0.05148554  0.1523221    2     none  identity #> X.4   1.0 0.10122665 0.02449415 0.05321900  0.1492343    2     none  identity #> X1   -1.0 0.29008665 0.05502003 0.18224936  0.3979239    3     none  identity #> X.11 -0.5 0.11993079 0.03651545 0.04836184  0.1914998    3     none  identity #> X.21  0.0 0.08120193 0.02656267 0.02914006  0.1332638    3     none  identity #> X.31  0.5 0.07969740 0.02411627 0.03243039  0.1269644    3     none  identity #> X.41  1.0 0.08403060 0.02343156 0.03810559  0.1299556    3     none  identity #> X2   -1.0 0.29008665 0.05502003 0.18224936  0.3979239    4     none  identity #> X.12 -0.5 0.11993079 0.03651545 0.04836184  0.1914998    4     none  identity #> X.22  0.0 0.08120193 0.02656267 0.02914006  0.1332638    4     none  identity #> X.32  0.5 0.07969740 0.02411627 0.03243039  0.1269644    4     none  identity #> X.42  1.0 0.08403060 0.02343156 0.03810559  0.1299556    4     none  identity #>       measure #> X    survival #> X.1  survival #> X.2  survival #> X.3  survival #> X.4  survival #> X1   survival #> X.11 survival #> X.21 survival #> X.31 survival #> X.41 survival #> X2   survival #> X.12 survival #> X.22 survival #> X.32 survival #> X.42 survival"},{"path":"https://sachsmc.github.io/stdReg2/news/index.html","id":"stdreg2-104","dir":"Changelog","previous_headings":"","what":"stdReg2 1.0.4","title":"stdReg2 1.0.4","text":"minor tweak documentation","code":""},{"path":"https://sachsmc.github.io/stdReg2/news/index.html","id":"stdreg2-103","dir":"Changelog","previous_headings":"","what":"stdReg2 1.0.3","title":"stdReg2 1.0.3","text":"CRAN release: 2025-02-28 bugfix – standard error calculation standardized_coxph survival probabilities factors dummy variables. Thanks @dominicmagirr report bugfix – add tidy method std_custom Improved documentation return objects","code":""},{"path":"https://sachsmc.github.io/stdReg2/news/index.html","id":"stdreg2-102","dir":"Changelog","previous_headings":"","what":"stdReg2 1.0.2","title":"stdReg2 1.0.2","text":"bugfixes – warning model.matrix added support factors glm methods","code":""},{"path":"https://sachsmc.github.io/stdReg2/news/index.html","id":"stdreg2-101","dir":"Changelog","previous_headings":"","what":"stdReg2 1.0.1","title":"stdReg2 1.0.1","text":"CRAN release: 2024-09-13 Documentation improvements add progressbar argument standardize standardize_level","code":""},{"path":"https://sachsmc.github.io/stdReg2/news/index.html","id":"stdreg2-100","dir":"Changelog","previous_headings":"","what":"stdReg2 1.0.0","title":"stdReg2 1.0.0","text":"Initial CRAN submission.","code":""}]
